{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import operator\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from src.data.progress_bar import progress_bar\n",
    "from src.features.features_utils import convert_categoricals_to_numerical\n",
    "from src.features.features_utils import convert_target_to_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in both sets of training and validation features and targets as well as the sample weights we created for covariate shift adaptation. We make sure to convert the categorical fields to a numerical form that is suitable for building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/processed/train-features.csv')\n",
    "X_train = convert_categoricals_to_numerical(train_features)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = pd.read_csv('../models/train-features-sample-weights.csv')\n",
    "sample_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_topics = pd.read_csv('../data/processed/train-features-topics.csv')\n",
    "X_train_topics = convert_categoricals_to_numerical(train_features_topics)\n",
    "X_train_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights_topics = pd.read_csv('../models/train-features-topics-sample-weights.csv')\n",
    "sample_weights_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('../data/processed/train-target.csv', index_col='full_name', squeeze=True)\n",
    "y_train = convert_target_to_numerical(train_target)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv('../data/processed/validation-features.csv')\n",
    "X_validation = convert_categoricals_to_numerical(validation_features)\n",
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features_topics = pd.read_csv('../data/processed/validation-features-topics.csv')\n",
    "X_validation_topics = convert_categoricals_to_numerical(validation_features_topics)\n",
    "X_validation_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target = pd.read_csv('../data/processed/validation-target.csv', index_col='full_name',\n",
    "                                squeeze=True)\n",
    "y_validation = convert_target_to_numerical(validation_target)\n",
    "y_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection\n",
    "\n",
    "The hyperparameters of the models that we will be fitting are critical to their predictive performance. We will use an exhaustive grid search to select them in a principled manner. The optimal hyperparameter values will be chosen according to the set of values that maximize the Matthews Correlation Coefficient (MCC) on the validation set. The function below will be used to accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_linear_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=LogisticRegression(),\n",
    "    param_grid=ParameterGrid({'C': [10**x for x in list(range(-10, 10))]}),\n",
    "    score_func=matthews_corrcoef, greater_score_is_better=True, solver='lbfgs',\n",
    "    sample_weight=None, max_iter=1000, random_state=None, progress_bar=None):\n",
    "    \n",
    "    if progress_bar:\n",
    "        progress_bar.start()\n",
    "\n",
    "    train_scores = {}\n",
    "    validation_scores = {}\n",
    "    classifiers = {}\n",
    "    num_iters = 0\n",
    "    for params in param_grid:\n",
    "        \n",
    "        num_iters += 1\n",
    "        if progress_bar:\n",
    "            progress_bar.update(num_iters)\n",
    "        \n",
    "        # fit the model to training set\n",
    "        if isinstance(clf, LogisticRegression):\n",
    "            classifier = LogisticRegression(\n",
    "                penalty=params['penalty'], C=params['C'], solver=solver, random_state=random_state,\n",
    "                class_weight=params['class_weight'], max_iter=max_iter)\n",
    "            classifier.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        classifiers[str(params)] = classifier\n",
    "\n",
    "        # predict on validation set and evaluate scores\n",
    "        y_train_predict = classifier.predict(X_train)\n",
    "        y_validation_predict = classifier.predict(X_validation)\n",
    "        with warnings.catch_warnings():  # ignore runtime warnings caused by zero MCC\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            train_scores[str(params)] = score_func(y_true=y_train, y_pred=y_train_predict)\n",
    "            validation_scores[str(params)] = score_func(y_true=y_validation,\n",
    "                                                        y_pred=y_validation_predict)\n",
    "            \n",
    "    if progress_bar:\n",
    "        progress_bar.finish()\n",
    "    \n",
    "    # find the best scoring model\n",
    "    sorted_validation_scores = sorted(\n",
    "        validation_scores.items(), key=operator.itemgetter(1), reverse=greater_score_is_better)\n",
    "    best_params = ast.literal_eval(sorted_validation_scores[0][0])\n",
    "    best_score = sorted_validation_scores[0][1]\n",
    "    best_classifier = classifiers[str(best_params)]\n",
    "    \n",
    "    # return results\n",
    "    results = {'best_classifier': best_classifier, 'best_params': best_params, 'best_score': best_score,\n",
    "               'train_scores': train_scores, 'validation_scores': validation_scores} \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_best_classifier(results, title=None):\n",
    "    if title:\n",
    "        print(title)\n",
    "    print('Best params: ', results['best_params'])\n",
    "    print('Training score: ', round(results['train_scores'][str(results['best_params'])], 3))\n",
    "    print('Validation score: ', round(results['best_score'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to select the best parameters for the two feature sets with and without the sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The hyperparameters to be selected for the logistic regression model are:\n",
    "- The `penalty` which is used to specify whether the $L1$ or $L2$ norms are used in the regularization. The latter favors sparse solutions and naturally performs feature selection. \n",
    "- `C`, the inverse of regularization strength. Smaller values specify stronger regularization.\n",
    "- `class_weight`, the weights associated with the classes. It penalizes mistakes in samples of a class with its associated class_weight. So a higher value indicates more emphasis is put on a class.\n",
    "\n",
    "Let's perform the grid search now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "Cs = [10**x for x in list(range(1, 10))]\n",
    "class_weight = ([{0: weight, 1: 1.0 - weight} for weight in np.linspace(0.0, 1.0, 21)] +\n",
    "                 [{0: 1.0, 1: 1.0}] + ['balanced'])\n",
    "param_grid = ParameterGrid({'penalty': penalty, 'C': Cs, 'class_weight': class_weight})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = progress_bar(len(param_grid), banner_text_begin='Running: ', banner_text_end=' param sets')\n",
    "logit_results = evaluate_linear_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=LogisticRegression(), param_grid=param_grid,\n",
    "    solver='liblinear', random_state=0, progress_bar=bar)\n",
    "print_best_classifier(logit_results, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_weights = evaluate_linear_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=LogisticRegression(), param_grid=param_grid,\n",
    "    solver='liblinear', sample_weight=sample_weights['weight'], random_state=1, progress_bar=bar)\n",
    "print_best_classifier(logit_results_weights, 'Logistic Regression + sample weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_topics = evaluate_linear_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=LogisticRegression(),\n",
    "    param_grid=param_grid, solver='liblinear', random_state=2, progress_bar=bar)\n",
    "print_best_classifier(logit_results_topics, 'Logistic Regression (topics)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_topics_weights = evaluate_linear_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=LogisticRegression(),\n",
    "    param_grid=param_grid, solver='liblinear',  sample_weight=sample_weights_topics['weight'],\n",
    "    random_state=3, progress_bar=bar)\n",
    "print_best_classifier(logit_results_topics_weights, 'Logistic Regression (topics) + sample weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important.\n",
    "- Unsurprisingly, $L1$ regularization is chosen for the original features and $L2$ regularization for the topics features.\n",
    "- Models fitted with the original features are overfitting. The overfitting in the case of the first model is extreme.\n",
    "- Applying strong regularization does not improve performance for the original features.\n",
    "- Interestingly, for the topics features, the validation MCC's are higher than the training MCCs. This suggests that these models are underfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
