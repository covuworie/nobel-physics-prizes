{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import operator\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from src.data.progress_bar import progress_bar\n",
    "from src.features.features_utils import convert_categoricals_to_numerical\n",
    "from src.features.features_utils import convert_target_to_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in both sets of training and validation features and targets as well as the sample weights we created for covariate shift adaptation. We make sure to convert the categorical fields to a numerical form that is suitable for building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/processed/train-features.csv')\n",
    "X_train = convert_categoricals_to_numerical(train_features)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = pd.read_csv('../models/train-features-sample-weights.csv')\n",
    "sample_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_topics = pd.read_csv('../data/processed/train-features-topics.csv')\n",
    "X_train_topics = convert_categoricals_to_numerical(train_features_topics)\n",
    "X_train_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights_topics = pd.read_csv('../models/train-features-topics-sample-weights.csv')\n",
    "sample_weights_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('../data/processed/train-target.csv', index_col='full_name', squeeze=True)\n",
    "y_train = convert_target_to_numerical(train_target)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv('../data/processed/validation-features.csv')\n",
    "X_validation = convert_categoricals_to_numerical(validation_features)\n",
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features_topics = pd.read_csv('../data/processed/validation-features-topics.csv')\n",
    "X_validation_topics = convert_categoricals_to_numerical(validation_features_topics)\n",
    "X_validation_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target = pd.read_csv('../data/processed/validation-target.csv', index_col='full_name',\n",
    "                                squeeze=True)\n",
    "y_validation = convert_target_to_numerical(validation_target)\n",
    "y_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection\n",
    "\n",
    "The hyperparameters of the models that we will be fitting are critical to their predictive performance. We will use an exhaustive grid search to select them in a principled manner. The optimal hyperparameter values will be chosen according to the set of values that maximize the Matthews Correlation Coefficient (MCC) on the validation set. The function below will be used to accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=LogisticRegression(),\n",
    "    param_grid=ParameterGrid(dict(C=np.logspace(-5, 15, 21, base=2.0))),\n",
    "    score_func=matthews_corrcoef, greater_score_is_better=True, solver='lbfgs',\n",
    "    sample_weight=None, max_iter=1000, random_state=None, n_jobs=None, progress_bar=None):\n",
    "    \n",
    "    if progress_bar:\n",
    "        progress_bar.start()\n",
    "\n",
    "    train_scores = {}\n",
    "    validation_scores = {}\n",
    "    classifiers = {}\n",
    "    num_iters = 0\n",
    "    for params in param_grid:\n",
    "        \n",
    "        num_iters += 1\n",
    "        if progress_bar:\n",
    "            progress_bar.update(num_iters)\n",
    "        \n",
    "        # fit the model to training set\n",
    "        if isinstance(clf, LogisticRegression):\n",
    "            classifier = LogisticRegression(\n",
    "                penalty=params.get('penalty', 'l2'), C=params.get('C', 1.0), solver=solver,\n",
    "                random_state=random_state, class_weight=params.get('class_weight'), max_iter=max_iter)\n",
    "        elif isinstance(clf, SVC):\n",
    "            classifier = SVC(\n",
    "                C=params.get('C', 1.0), kernel=params.get('kernel', 'rbf'),\n",
    "                gamma=params.get('gamma', 'auto_deprecated'), random_state=random_state,\n",
    "                class_weight=params.get('class_weight'), max_iter=max_iter)\n",
    "        elif isinstance(clf, RandomForestClassifier):\n",
    "            classifier = RandomForestClassifier(\n",
    "                n_estimators=params.get('n_estimators', 'warn'),\n",
    "                max_features=params.get('max_features', 'auto'),\n",
    "                min_samples_leaf=params.get('min_samples_leaf', 1), random_state=random_state,\n",
    "                class_weight=params.get('class_weight'), n_jobs=n_jobs)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        classifier.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        classifiers[str(params)] = classifier\n",
    "\n",
    "        # predict on validation set and evaluate scores\n",
    "        y_train_predict = classifier.predict(X_train)\n",
    "        y_validation_predict = classifier.predict(X_validation)\n",
    "        with warnings.catch_warnings():  # ignore runtime warnings caused by zero MCC\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            train_scores[str(params)] = score_func(y_true=y_train, y_pred=y_train_predict)\n",
    "            validation_scores[str(params)] = score_func(y_true=y_validation,\n",
    "                                                        y_pred=y_validation_predict)\n",
    "            \n",
    "    if progress_bar:\n",
    "        progress_bar.finish()\n",
    "    \n",
    "    # find the best scoring model\n",
    "    sorted_validation_scores = sorted(\n",
    "        validation_scores.items(), key=operator.itemgetter(1), reverse=greater_score_is_better)\n",
    "    best_params = ast.literal_eval(sorted_validation_scores[0][0])\n",
    "    best_score = sorted_validation_scores[0][1]\n",
    "    best_classifier = classifiers[str(best_params)]\n",
    "    \n",
    "    # return results\n",
    "    results = {'best_classifier': best_classifier, 'best_params': best_params,\n",
    "               'best_score': best_score, 'train_scores': train_scores,\n",
    "               'validation_scores': validation_scores} \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_best_classifier(results, title=None):\n",
    "    if title:\n",
    "        print(title)\n",
    "    print('Best params: ', results['best_params'])\n",
    "    print('Training score: ', round(results['train_scores'][str(results['best_params'])], 3))\n",
    "    print('Validation score: ', round(results['best_score'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to select the best parameters for the two feature sets with and without the sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR)\n",
    "\n",
    "The hyperparameters to be selected for the logistic regression model are:\n",
    "- The `penalty` which is used to specify whether the $L1$ or $L2$ norms are used in the regularization. The latter favors sparse solutions and naturally performs feature selection. \n",
    "- `C`, the inverse of regularization strength. Smaller values specify stronger regularization.\n",
    "- `class_weight`, the weights associated with the classes. It penalizes mistakes in samples of a class with its associated class_weight. So a higher value indicates more emphasis is put on a class.\n",
    "\n",
    "Let's perform the grid search now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "Cs= np.logspace(-5, 15, 21, base=2.0)\n",
    "class_weight = ([{0: weight, 1: 1.0 - weight} for weight in np.linspace(0.0, 1.0, 21)] +\n",
    "                 [{0: 1.0, 1: 1.0}] + ['balanced'])\n",
    "param_grid = ParameterGrid(dict(penalty=penalty, C=Cs, class_weight=class_weight))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "solver = 'liblinear'\n",
    "bar = progress_bar(len(param_grid), banner_text_begin='Running: ', banner_text_end=' param sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, solver=solver,\n",
    "    random_state=0, progress_bar=bar)\n",
    "print_best_classifier(logit_results, 'LR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_weights = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, solver=solver,\n",
    "    sample_weight=sample_weights['weight'], random_state=1, progress_bar=bar)\n",
    "print_best_classifier(logit_results_weights, 'LR + sample weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_topics = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    solver=solver, random_state=2, progress_bar=bar)\n",
    "print_best_classifier(logit_results_topics, 'LR (topics)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_topics_weights = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    solver=solver, sample_weight=sample_weights_topics['weight'], random_state=3, progress_bar=bar)\n",
    "print_best_classifier(logit_results_topics_weights, 'LR (topics) + sample weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important.\n",
    "- Unsurprisingly, $L1$ regularization is chosen for the original features and $L2$ regularization for the topics features.\n",
    "- Models fitted with the original features are overfitting and those with the topics features are underfitting (the validation MCC's are higher than the training MCCs).\n",
    "- Applying strong regularization does not improve performance for the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "The hyperparameters to be selected for the support vector machine model are:\n",
    "- The regularization parameter `C` of the error term. This parameter trades off correct classification of training examples against maximization of the separating hyperplane's margin. For larger values of `C`, a smaller margin will be accepted if the separating hyperplane is better at classifying training points correctly. Lower values of `C` encourage a larger margin at the cost of misclassifying more training points.\n",
    "- `class_weight`, as defined above for logistic regression.\n",
    "\n",
    "Note that the `kernel` parameter, which is used to specify the kernel type to be used in the algorithm, will be fixed as the *linear* kernel. The reason for not considering the *RBF* or *poly* kernels is interpretability of the model. For the *RBF* and *poly* kernels, the [separating hyperplane and the weights that define it exist in a transformed space](https://stackoverflow.com/questions/21260691/how-to-obtain-features-weights) that is not directly related to the input feature space. OK let's perform the grid search now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-5, 10, 16, base=2.0)\n",
    "param_grid = ParameterGrid(dict(kernel=['linear'], C=Cs, class_weight=class_weight))\n",
    "\n",
    "clf=SVC()\n",
    "max_iter = -1\n",
    "bar = progress_bar(len(param_grid), banner_text_begin='Running: ', banner_text_end=' param sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, random_state=4,\n",
    "    progress_bar=bar, max_iter=max_iter)\n",
    "print_best_classifier(svm_results, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_weights = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid,\n",
    "    sample_weight=sample_weights['weight'], random_state=5, progress_bar=bar, max_iter=max_iter)\n",
    "print_best_classifier(svm_results_weights, 'SVM + sample weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_topics = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    random_state=6, progress_bar=bar, max_iter=max_iter)\n",
    "print_best_classifier(svm_results_topics, 'SVM (topics)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_topics_weights = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    sample_weight=sample_weights_topics['weight'], random_state=7, progress_bar=bar, max_iter=max_iter)\n",
    "print_best_classifier(svm_results_topics_weights, 'SVM (topics) + sample weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Once again, since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important. Interestingly, the balanced class weight is chosen for the topics features.\n",
    "- Again we see overfitting and underfitting of the models. However, for the classifiers fitted with the sample weights, the effect is far less pronounced than for the logistic regression models.\n",
    "- The sample-weighted classifiers seem to favor larger values of C (smaller margin) whereas the unweighted ones facvor smaller values of C (larger margin). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)\n",
    "\n",
    "There are a multitude of hyperparameters that can be selected for a random forest model. The hyperparameters control the randomness of the forest. The aim is to set them such that reasonable predictive power of individual trees is achieved without excessive correlation between the trees (bias-variance trade-off). Based on the advice of Probst et al. in [Hyperparameters and Tuning Strategies for Random Forest](https://arxiv.org/pdf/1804.03515.pdf), we will restrict ourselves to the following most influential hyperparameters:\n",
    "\n",
    "- The number of features `max_features` to consider when looking for the best split. Lower values lead to more diverse and less correlated trees, which results in more stable aggregation of predictions. However, lower values also lead to the individual trees being weaker predictors of the target variable.\n",
    "- The minimum number of samples required to be at a leaf node `min_samples_leaf`. Lower values lead to\n",
    "trees of larger depth, which means that more splits are performed until the terminal nodes. More splits means the model is more complex and this can potentially lead to overfitting.\n",
    "- `class_weight`, as defined above for logistic regression.\n",
    "\n",
    "Note that the number of trees in the forest `n_estimators` will be fixed at 500. Following the recommendations of Probst et al., \"the number of trees should be set high: the higher the number of trees, the better the results in terms of performance and precision of variable importances. However, the improvement obtained by adding trees diminishes as more and more trees are added.\" 500 trees in the forest seems like a reasonable compromise based on the stability of the solution and computational resources available.  \n",
    "\n",
    "Let's perform the grid search now. Note that this takes a considerable amount of time due to the number of trees in the forest and the large hyperparameter space being searched. To reduce the computation time, we will perform a *random search* over one-quarter of the values in the hyperparameter space, by drawing randomly from a uniform distribution. Bergstra and Bengio show in [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) that for neural networks random search is more efficient in searching good hyperparameter specifications than grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [500]\n",
    "max_features = [0.01] + list(np.linspace(0.05, 0.95, 7)) + ['sqrt', None]\n",
    "min_samples_leaf = np.logspace(0, 6, 7, base=2.0).astype('int64')\n",
    "\n",
    "class_weight = ([{0: weight, 1: 1.0 - weight} for weight in np.linspace(0.0, 1.0, 11)] +\n",
    "                 [{0: 1.0, 1: 1.0}] + ['balanced', 'balanced_subsample'])\n",
    "param_grid = dict(n_estimators=n_estimators, max_features=max_features,\n",
    "                  min_samples_leaf=min_samples_leaf, class_weight=class_weight)\n",
    "param_grid_sampler = ParameterSampler(param_grid, n_iter=int(0.25 * len(list(ParameterGrid(param_grid)))),\n",
    "                                      random_state=8)\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "n_jobs = -1\n",
    "bar = progress_bar(len(param_grid_sampler), banner_text_begin='Running: ', banner_text_end=' param sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid_sampler, random_state=9,\n",
    "    progress_bar=bar, n_jobs=n_jobs)\n",
    "print_best_classifier(rf_results, 'RF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results_weights = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid_sampler,\n",
    "    random_state=10, sample_weight=sample_weights['weight'], progress_bar=bar, n_jobs=n_jobs)\n",
    "print_best_classifier(rf_results_weights, 'RF + sample weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results_topics = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid_sampler,\n",
    "    random_state=11, progress_bar=bar, n_jobs=n_jobs)\n",
    "print_best_classifier(rf_results_topics, 'RF (topics)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results_topics_weights = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid_sampler,\n",
    "    sample_weight=sample_weights_topics['weight'], random_state=12, progress_bar=bar, n_jobs=n_jobs)\n",
    "print_best_classifier(rf_results_topics_weights, 'RF (topics) + sample weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Once again, since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important. In fact all four models select values for the weights which weights the laureate class much higher than the non-laureate class.\n",
    "- The original features classifiers seem to favor lower values of `min_samples_leaf` (deeper trees) and are overfitting considerably.\n",
    "- The `max_features` is low for the majority of the classifiers, which means the individual trees are diverse, but weak predictors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
