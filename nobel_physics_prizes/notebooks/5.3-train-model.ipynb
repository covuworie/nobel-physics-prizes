{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import operator\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "from src.data.progress_bar import progress_bar\n",
    "from src.features.features_utils import convert_categoricals_to_numerical\n",
    "from src.features.features_utils import convert_target_to_numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in both sets of training and validation features and targets as well as the sample weights we created for covariate shift adaptation. We make sure to convert the categorical fields to a numerical form that is suitable for building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/processed/train-features.csv')\n",
    "X_train = convert_categoricals_to_numerical(train_features)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = pd.read_csv('../models/train-features-sample-weights.csv')\n",
    "sample_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_topics = pd.read_csv('../data/processed/train-features-topics.csv')\n",
    "X_train_topics = convert_categoricals_to_numerical(train_features_topics)\n",
    "X_train_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights_topics = pd.read_csv('../models/train-features-topics-sample-weights.csv')\n",
    "sample_weights_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('../data/processed/train-target.csv', index_col='full_name', squeeze=True)\n",
    "y_train = convert_target_to_numerical(train_target)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv('../data/processed/validation-features.csv')\n",
    "X_validation = convert_categoricals_to_numerical(validation_features)\n",
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features_topics = pd.read_csv('../data/processed/validation-features-topics.csv')\n",
    "X_validation_topics = convert_categoricals_to_numerical(validation_features_topics)\n",
    "X_validation_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target = pd.read_csv('../data/processed/validation-target.csv', index_col='full_name',\n",
    "                                squeeze=True)\n",
    "y_validation = convert_target_to_numerical(validation_target)\n",
    "y_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection\n",
    "\n",
    "The hyperparameters of the models that we will be fitting are critical to their predictive performance. We will use an exhaustive grid search to select them in a principled manner. The optimal hyperparameter values will be chosen according to the set of values that maximize the Matthews Correlation Coefficient (MCC) on the validation set. The function below will be used to accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=LogisticRegression(),\n",
    "    param_grid=ParameterGrid(dict(C=np.logspace(-5, 15, 21, base=2.0))),\n",
    "    score_func=matthews_corrcoef, greater_score_is_better=True, solver='lbfgs',\n",
    "    sample_weight=None, max_iter=1000, random_state=None, progress_bar=None):\n",
    "    \n",
    "    if progress_bar:\n",
    "        progress_bar.start()\n",
    "\n",
    "    train_scores = {}\n",
    "    validation_scores = {}\n",
    "    classifiers = {}\n",
    "    num_iters = 0\n",
    "    for params in param_grid:\n",
    "        \n",
    "        num_iters += 1\n",
    "        if progress_bar:\n",
    "            progress_bar.update(num_iters)\n",
    "        \n",
    "        # fit the model to training set\n",
    "        if isinstance(clf, LogisticRegression):\n",
    "            classifier = LogisticRegression(\n",
    "                penalty=params.get('penalty', 'l2'), C=params.get('C', 1.0), solver=solver,\n",
    "                random_state=random_state, class_weight=params.get('class_weight'), max_iter=max_iter)\n",
    "        elif isinstance(clf, SVC):\n",
    "            classifier = SVC(\n",
    "                C=params.get('C', 1.0), kernel=params.get('kernel', 'rbf'),\n",
    "                gamma=params.get('gamma', 'auto_deprecated'), random_state=random_state,\n",
    "                class_weight=params.get('class_weight'), max_iter=max_iter)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        classifier.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        classifiers[str(params)] = classifier\n",
    "\n",
    "        # predict on validation set and evaluate scores\n",
    "        y_train_predict = classifier.predict(X_train)\n",
    "        y_validation_predict = classifier.predict(X_validation)\n",
    "        with warnings.catch_warnings():  # ignore runtime warnings caused by zero MCC\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            train_scores[str(params)] = score_func(y_true=y_train, y_pred=y_train_predict)\n",
    "            validation_scores[str(params)] = score_func(y_true=y_validation,\n",
    "                                                        y_pred=y_validation_predict)\n",
    "            \n",
    "    if progress_bar:\n",
    "        progress_bar.finish()\n",
    "    \n",
    "    # find the best scoring model\n",
    "    sorted_validation_scores = sorted(\n",
    "        validation_scores.items(), key=operator.itemgetter(1), reverse=greater_score_is_better)\n",
    "    best_params = ast.literal_eval(sorted_validation_scores[0][0])\n",
    "    best_score = sorted_validation_scores[0][1]\n",
    "    best_classifier = classifiers[str(best_params)]\n",
    "    \n",
    "    # return results\n",
    "    results = {'best_classifier': best_classifier, 'best_params': best_params, 'best_score': best_score,\n",
    "               'train_scores': train_scores, 'validation_scores': validation_scores} \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_best_classifier(results, title=None):\n",
    "    if title:\n",
    "        print(title)\n",
    "    print('Best params: ', results['best_params'])\n",
    "    print('Training score: ', round(results['train_scores'][str(results['best_params'])], 3))\n",
    "    print('Validation score: ', round(results['best_score'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to select the best parameters for the two feature sets with and without the sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "\n",
    "The hyperparameters to be selected for the logistic regression model are:\n",
    "- The `penalty` which is used to specify whether the $L1$ or $L2$ norms are used in the regularization. The latter favors sparse solutions and naturally performs feature selection. \n",
    "- `C`, the inverse of regularization strength. Smaller values specify stronger regularization.\n",
    "- `class_weight`, the weights associated with the classes. It penalizes mistakes in samples of a class with its associated class_weight. So a higher value indicates more emphasis is put on a class.\n",
    "\n",
    "Let's perform the grid search now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "Cs= np.logspace(-5, 15, 21, base=2.0)\n",
    "class_weight = ([{0: weight, 1: 1.0 - weight} for weight in np.linspace(0.0, 1.0, 21)] +\n",
    "                 [{0: 1.0, 1: 1.0}] + ['balanced'])\n",
    "param_grid = ParameterGrid(dict(penalty=penalty, C=Cs, class_weight=class_weight))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "solver = 'liblinear'\n",
    "bar = progress_bar(len(param_grid), banner_text_begin='Running: ', banner_text_end=' param sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, solver=solver,\n",
    "    random_state=0, progress_bar=bar)\n",
    "print_best_classifier(logit_results, 'Logistic Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_weights = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, solver=solver,\n",
    "    sample_weight=sample_weights['weight'], random_state=1, progress_bar=bar)\n",
    "print_best_classifier(logit_results_weights, 'Logistic Regression + sample weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_topics = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    solver=solver, random_state=2, progress_bar=bar)\n",
    "print_best_classifier(logit_results_topics, 'Logistic Regression (topics)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_topics_weights = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    solver=solver, sample_weight=sample_weights_topics['weight'], random_state=3, progress_bar=bar)\n",
    "print_best_classifier(logit_results_topics_weights, 'Logistic Regression (topics) + sample weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important.\n",
    "- Unsurprisingly, $L1$ regularization is chosen for the original features and $L2$ regularization for the topics features.\n",
    "- Models fitted with the original features are overfitting and those with the topics features are underfitting (the validation MCC's are higher than the training MCCs).\n",
    "- Applying strong regularization does not improve performance for the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "The hyperparameters to be selected for the support vector machine model are:\n",
    "- The regularization parameter `C` of the error term. This parameter trades off correct classification of training examples against maximization of the separating hyperplane's margin. For larger values of `C`, a smaller margin will be accepted if the separating hyperplane is better at classifying training points correctly. Lower values of `C` encourage a larger margin at the cost of misclassifying more training points.\n",
    "- `class_weight`, as defined above for logistic regression.\n",
    "\n",
    "Note that the `kernel` parameter, which is used to specify the kernel type to be used in the algorithm, will be fixed as the *linear* kernel. The reason for not considering the *RBF* or *poly* kernels is interpretability of the model. For the *RBF* and *poly* kernels, the [separating hyperplane and the weights that define it exist in a transformed space](https://stackoverflow.com/questions/21260691/how-to-obtain-features-weights) that is not directly related to the input feature space. OK let's perform the grid search now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-5, 10, 16, base=2.0)\n",
    "param_grid = ParameterGrid(dict(kernel=['linear'], C=Cs, class_weight=class_weight))\n",
    "\n",
    "clf=SVC()\n",
    "max_iter = -1\n",
    "bar = progress_bar(len(param_grid), banner_text_begin='Running: ', banner_text_end=' param sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, random_state=4,\n",
    "    progress_bar=bar, max_iter=max_iter)\n",
    "print_best_classifier(svm_results, 'SVM')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_weights = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid,\n",
    "    sample_weight=sample_weights['weight'], random_state=5, progress_bar=bar, max_iter=max_iter)\n",
    "print_best_classifier(svm_results_weights, 'SVM + sample weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_topics = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    random_state=6, progress_bar=bar, max_iter=max_iter)\n",
    "print_best_classifier(svm_results_topics, 'SVM (topics)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_topics_weights = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    sample_weight=sample_weights_topics['weight'], random_state=7, progress_bar=bar, max_iter=max_iter)\n",
    "print_best_classifier(svm_results_topics_weights, 'SVM (topics) + weights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Once again, since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important. Interestingly, the balanced class weight is chosen for the topics features.\n",
    "- Again we see overfitting and underfitting of the models. However, for the classifiers fitted with the sample weights, the effect is far less pronounced than for the logistic regression models.\n",
    "- The sample-weighted classifiers seem to favor larger values of C (smaller margin) whereas the unweighted ones facvor smaller values of C (larger margin). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
