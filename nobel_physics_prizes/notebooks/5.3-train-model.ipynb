{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model\n",
    "\n",
    "It is finally time to use the features we have created to train some classifiers to predict *Physics Nobel Laureates*. We will be training the following classifiers:\n",
    "- [Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "- [Support Vector Machine](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "- [Random Forest](https://en.wikipedia.org/wiki/Random_forest)\n",
    "\n",
    "These classifiers are chosen due to their simplicity, appropriateness for the input features and target data types, ease of interpretability and their well-established performance on a range of classification tasks.\n",
    "\n",
    "We will be selecting hyperparameters and evaluating the performance of the models on the validation set. This will be done in a principled manner using a couple of performance measures. The first of these, the [Matthews Correlation Coefficient (MCC)](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient), you have already seen when we created the [baseline model](5.0-baseline-model.ipynb). We will discuss the second measure in detail, and explain why it is needed, when we get to that point. OK let's get going!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import operator\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from src.data.progress_bar import progress_bar\n",
    "from src.features.features_utils import convert_categoricals_to_numerical\n",
    "from src.features.features_utils import convert_target_to_numerical\n",
    "from src.models.metrics_utils import confusion_matrix_to_dataframe\n",
    "from src.models.metrics_utils import mcc_auc_score\n",
    "from src.models.metrics_utils import mcc_curve\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in both sets of training and validation features and targets as well as the sample weights we created for covariate shift adaptation. We make sure to convert the categorical fields to a numerical form that is suitable for building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/processed/train-features.csv')\n",
    "X_train = convert_categoricals_to_numerical(train_features)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = pd.read_csv('../models/train-features-sample-weights.csv')\n",
    "sample_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_topics = pd.read_csv('../data/processed/train-features-topics.csv')\n",
    "X_train_topics = convert_categoricals_to_numerical(train_features_topics)\n",
    "X_train_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights_topics = pd.read_csv('../models/train-features-topics-sample-weights.csv')\n",
    "sample_weights_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('../data/processed/train-target.csv', index_col='full_name', squeeze=True)\n",
    "y_train = convert_target_to_numerical(train_target)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv('../data/processed/validation-features.csv')\n",
    "X_validation = convert_categoricals_to_numerical(validation_features)\n",
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features_topics = pd.read_csv('../data/processed/validation-features-topics.csv')\n",
    "X_validation_topics = convert_categoricals_to_numerical(validation_features_topics)\n",
    "X_validation_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target = pd.read_csv('../data/processed/validation-target.csv', index_col='full_name',\n",
    "                                squeeze=True)\n",
    "y_validation = convert_target_to_numerical(validation_target)\n",
    "y_validation.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Selection\n",
    "\n",
    "The hyperparameters of the models that we will be fitting are critical to their predictive performance. We will use an exhaustive grid search to select them in a principled manner. The optimal hyperparameter values will be chosen according to the set of values that maximize the Matthews Correlation Coefficient (MCC) on the validation set. The function below will be used to accomplish this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=LogisticRegression(),\n",
    "    param_grid=ParameterGrid(dict(C=np.logspace(-5, 15, 21, base=2.0))), score_func=matthews_corrcoef,\n",
    "    greater_score_is_better=True, solver='lbfgs', sample_weight=None, max_iter=1000,\n",
    "    random_state=None, n_jobs=None, name='classifier', progress_bar=None):\n",
    "    \n",
    "    \"\"\"Evaluate an `sklearn` classifier over a parameter grid using a scoring function. \n",
    "    \n",
    "    Args:\n",
    "        X_train ({array-like, sparse matrix}, shape (n_samples, n_features)): Training features matrix, \n",
    "            where n_samples is the number of samples and n_features is the number of features.\n",
    "        y_train (array-like, shape (n_samples,)): Training data target vector.\n",
    "        X_validation ({array-like, sparse matrix}, shape (n_samples, n_features)): Validation features\n",
    "        matrix, where n_samples is the number of samples and n_features is the number of features.\n",
    "        y_validation (array-like, shape (n_samples,)): Validation data target vector.\n",
    "        clf (sklearn.base.BaseEstimator, optional): Defaults to LogisticRegression(). Classifier.\n",
    "        param_grid (sklearn.model_selection.ParameterGrid, optional): Defaults to \n",
    "            ParameterGrid(dict(C=np.logspace(-5, 15, 21, base=2.0))). Grid of parameters with a discrete\n",
    "            number of values for each.\n",
    "        score_func (callable, optional): Defaults to matthews_corrcoef. Score function (or loss function)\n",
    "            with signature score_func(y, y_pred, **kwargs).\n",
    "        greater_score_is_better (bool, optional): Defaults to True. Whether score_func is a score function\n",
    "            (default), meaning high is good, or a loss function, meaning low is good. In the latter case,\n",
    "            the scorer object will sign-flip the outcome of the score_func.\n",
    "        solver (str, optional): Defaults to 'lbfgs'. Algorithm to use in the optimization problem.\n",
    "        sample_weight (array-like, shape (n_samples,), optional): Defaults to None. Array of weights\n",
    "            that are assigned to individual samples. If not provided, then each sample is given unit\n",
    "            weight.\n",
    "        max_iter (int, optional): Defaults to 1000. Maximum number of iterations taken for the solvers\n",
    "            to converge.\n",
    "        random_state (int, RandomState instance or None, optional): Defaults to None. The seed of the\n",
    "            pseudo random number generator to use when shuffling the data.\n",
    "        n_jobs (int or None, optional): Defaults to None. Number of CPU cores used when parallelizing.\n",
    "            None means 1 unless in a joblib.parallel_backend context. -1 means using all processors.\n",
    "        name (str, optional): Defaults to 'classifier'. Name of classifier.\n",
    "        progress_bar (progressbar.ProgressBar, optional): Defaults to None. Progress bar.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Results containing the name, best classifier estimator, best parameters, best score,\n",
    "            training scores, and validation scores.\n",
    "    \n",
    "    Raises:\n",
    "        NotImplementedError: If classifier is not LogisticRegression, SVC or RandomForestClassifier.\n",
    "\n",
    "        \"\"\"\n",
    "    \n",
    "    if progress_bar:\n",
    "        progress_bar.start()\n",
    "\n",
    "    train_scores = {}\n",
    "    validation_scores = {}\n",
    "    classifiers = {}\n",
    "    num_iters = 0\n",
    "    for params in param_grid:\n",
    "        \n",
    "        num_iters += 1\n",
    "        if progress_bar:\n",
    "            progress_bar.update(num_iters)\n",
    "        \n",
    "        # fit the model to training set\n",
    "        if isinstance(clf, LogisticRegression):\n",
    "            classifier = LogisticRegression(\n",
    "                penalty=params.get('penalty', 'l2'), C=params.get('C', 1.0), solver=solver,\n",
    "                random_state=random_state, class_weight=params.get('class_weight'), max_iter=max_iter)\n",
    "        elif isinstance(clf, SVC):\n",
    "            classifier = SVC(\n",
    "                C=params.get('C', 1.0), kernel=params.get('kernel', 'rbf'),\n",
    "                gamma=params.get('gamma', 'auto_deprecated'), random_state=random_state,\n",
    "                class_weight=params.get('class_weight'), max_iter=max_iter)\n",
    "        elif isinstance(clf, RandomForestClassifier):\n",
    "            classifier = RandomForestClassifier(\n",
    "                n_estimators=params.get('n_estimators', 'warn'),\n",
    "                max_features=params.get('max_features', 'auto'),\n",
    "                min_samples_leaf=params.get('min_samples_leaf', 1), random_state=random_state,\n",
    "                class_weight=params.get('class_weight'), n_jobs=n_jobs)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        classifier.fit(X_train, y_train, sample_weight=sample_weight)\n",
    "        classifiers[str(params)] = classifier\n",
    "\n",
    "        # predict on validation set and evaluate scores\n",
    "        y_train_predict = classifier.predict(X_train)\n",
    "        y_validation_predict = classifier.predict(X_validation)\n",
    "        with warnings.catch_warnings():  # ignore runtime warnings caused by zero MCC\n",
    "            warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "            train_scores[str(params)] = score_func(y_true=y_train, y_pred=y_train_predict)\n",
    "            validation_scores[str(params)] = score_func(y_true=y_validation,\n",
    "                                                        y_pred=y_validation_predict)\n",
    "            \n",
    "    if progress_bar:\n",
    "        progress_bar.finish()\n",
    "    \n",
    "    # find the best scoring model\n",
    "    sorted_validation_scores = sorted(\n",
    "        validation_scores.items(), key=operator.itemgetter(1), reverse=greater_score_is_better)\n",
    "    best_params = ast.literal_eval(sorted_validation_scores[0][0])\n",
    "    best_score = sorted_validation_scores[0][1]\n",
    "    best_classifier = classifiers[str(best_params)]\n",
    "    \n",
    "    # return results\n",
    "    results = dict(name=name, best_classifier=best_classifier, best_params=best_params,\n",
    "                   best_score=best_score, train_scores=train_scores, validation_scores=validation_scores) \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_best_classifier(results):\n",
    "    \"\"\"Print the best classifier.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Results of classifier evaluation.\n",
    "    \"\"\"\n",
    "\n",
    "    print(results['name'])\n",
    "    print('Best params: ', results['best_params'])\n",
    "    print('Training score: ', round(results['train_scores'][str(results['best_params'])], 3))\n",
    "    print('Validation score: ', round(results['best_score'], 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's now time to select the best parameters for the two feature sets with and without the sample weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (LR)\n",
    "\n",
    "The hyperparameters to be selected for the logistic regression model are:\n",
    "- The `penalty` which is used to specify whether the $L1$ or $L2$ norms are used in the regularization. The latter favors sparse solutions and naturally performs feature selection. \n",
    "- `C`, the inverse of regularization strength. Smaller values specify stronger regularization.\n",
    "- `class_weight`, the weights associated with the classes. It penalizes mistakes in samples of a class with its associated class_weight. So a higher value indicates more emphasis is put on a class.\n",
    "\n",
    "Let's perform the grid search now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2']\n",
    "Cs= np.logspace(-5, 15, 21, base=2.0)\n",
    "class_weight = ([{0: weight, 1: 1.0 - weight} for weight in np.linspace(0.0, 1.0, 21)] +\n",
    "                 [{0: 1.0, 1: 1.0}] + ['balanced'])\n",
    "param_grid = ParameterGrid(dict(penalty=penalty, C=Cs, class_weight=class_weight))\n",
    "\n",
    "clf = LogisticRegression()\n",
    "solver = 'liblinear'\n",
    "bar = progress_bar(len(param_grid), banner_text_begin='Running: ', banner_text_end=' param sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, solver=solver,\n",
    "    random_state=0, name='LR', progress_bar=bar)\n",
    "print_best_classifier(logit_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_weights = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, solver=solver,\n",
    "    sample_weight=sample_weights['weight'], random_state=1, name='LR + sample weights', progress_bar=bar)\n",
    "print_best_classifier(logit_results_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_topics = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    solver=solver, random_state=2, name='LR (topics)', progress_bar=bar)\n",
    "print_best_classifier(logit_results_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_results_topics_weights = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    solver=solver, sample_weight=sample_weights_topics['weight'], random_state=3,\n",
    "    name='LR (topics) + sample weights', progress_bar=bar)\n",
    "print_best_classifier(logit_results_topics_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important.\n",
    "- Unsurprisingly, $L1$ regularization is chosen for the original features and $L2$ regularization for the topics features.\n",
    "- Models fitted with the original features are overfitting and those with the topics features are underfitting (the validation MCC's are higher than the training MCCs).\n",
    "- Applying strong regularization does not improve performance for the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVM)\n",
    "\n",
    "The hyperparameters to be selected for the support vector machine model are:\n",
    "- The regularization parameter `C` of the error term. This parameter trades off correct classification of training examples against maximization of the separating hyperplane's margin. For larger values of `C`, a smaller margin will be accepted if the separating hyperplane is better at classifying training points correctly. Lower values of `C` encourage a larger margin at the cost of misclassifying more training points.\n",
    "- `class_weight`, as defined above for logistic regression.\n",
    "\n",
    "Note that the `kernel` parameter, which is used to specify the kernel type to be used in the algorithm, will be fixed as the *linear* kernel. The reason for not considering the *RBF* or *poly* kernels is interpretability of the model. For the *RBF* and *poly* kernels, the [separating hyperplane and the weights that define it exist in a transformed space](https://stackoverflow.com/questions/21260691/how-to-obtain-features-weights) that is not directly related to the input feature space. OK let's perform the grid search now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cs = np.logspace(-5, 10, 16, base=2.0)\n",
    "param_grid = ParameterGrid(dict(kernel=['linear'], C=Cs, class_weight=class_weight))\n",
    "\n",
    "clf=SVC()\n",
    "max_iter = -1\n",
    "bar = progress_bar(len(param_grid), banner_text_begin='Running: ', banner_text_end=' param sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid, random_state=4,\n",
    "    max_iter=max_iter, name='SVM', progress_bar=bar)\n",
    "print_best_classifier(svm_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_weights = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid,\n",
    "    sample_weight=sample_weights['weight'], random_state=5, max_iter=max_iter,\n",
    "    name='SVM + sample weights', progress_bar=bar)\n",
    "print_best_classifier(svm_results_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_topics = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    random_state=6, max_iter=max_iter, name='SVM (topics)', progress_bar=bar)\n",
    "print_best_classifier(svm_results_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_results_topics_weights = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid,\n",
    "    sample_weight=sample_weights_topics['weight'], random_state=7, max_iter=max_iter,\n",
    "    name='SVM (topics) + sample weights', progress_bar=bar)\n",
    "print_best_classifier(svm_results_topics_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Once again, since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important. Interestingly, the balanced class weight is chosen for the topics features.\n",
    "- Again we see overfitting and underfitting of the models. However, for the classifiers fitted with the sample weights, the effect is far less pronounced than for the logistic regression models.\n",
    "- The sample-weighted classifiers seem to favor larger values of C (smaller margin) whereas the unweighted ones facvor smaller values of C (larger margin). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest (RF)\n",
    "\n",
    "There are a multitude of hyperparameters that can be selected for a random forest model. The hyperparameters control the randomness of the forest. The aim is to set them such that reasonable predictive power of individual trees is achieved without excessive correlation between the trees (bias-variance trade-off). Based on the advice of Probst et al. in [Hyperparameters and Tuning Strategies for Random Forest](https://arxiv.org/pdf/1804.03515.pdf), we will restrict ourselves to the following most influential hyperparameters:\n",
    "\n",
    "- The number of features `max_features` to consider when looking for the best split. Lower values lead to more diverse and less correlated trees, which results in more stable aggregation of predictions. However, lower values also lead to the individual trees being weaker predictors of the target variable.\n",
    "- The minimum number of samples required to be at a leaf node `min_samples_leaf`. Lower values lead to\n",
    "trees of larger depth, which means that more splits are performed until the terminal nodes. More splits means the model is more complex and this can potentially lead to overfitting.\n",
    "- `class_weight`, as defined above for logistic regression.\n",
    "\n",
    "Note that the number of trees in the forest `n_estimators` will be fixed at 500. Following the recommendations of Probst et al., \"the number of trees should be set high: the higher the number of trees, the better the results in terms of performance and precision of variable importances. However, the improvement obtained by adding trees diminishes as more and more trees are added.\" 500 trees in the forest seems like a reasonable compromise based on the stability of the solution and computational resources available.  \n",
    "\n",
    "Let's perform the grid search now. Note that this takes a considerable amount of time due to the number of trees in the forest and the large hyperparameter space being searched. To reduce the computation time, we will perform a *random search* over one-quarter of the values in the hyperparameter space, by drawing randomly from a uniform distribution. Bergstra and Bengio show in [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) that for neural networks random search is more efficient in searching good hyperparameter specifications than grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [500]\n",
    "max_features = [0.01] + list(np.linspace(0.05, 0.95, 7)) + ['sqrt', None]\n",
    "min_samples_leaf = np.logspace(0, 6, 7, base=2.0).astype('int64')\n",
    "\n",
    "class_weight = ([{0: weight, 1: 1.0 - weight} for weight in np.linspace(0.0, 1.0, 11)] +\n",
    "                 [{0: 1.0, 1: 1.0}] + ['balanced', 'balanced_subsample'])\n",
    "param_grid = dict(n_estimators=n_estimators, max_features=max_features,\n",
    "                  min_samples_leaf=min_samples_leaf, class_weight=class_weight)\n",
    "param_grid_sampler = ParameterSampler(param_grid, n_iter=int(0.25 * len(list(ParameterGrid(param_grid)))),\n",
    "                                      random_state=8)\n",
    "\n",
    "clf=RandomForestClassifier()\n",
    "n_jobs = -1\n",
    "bar = progress_bar(len(param_grid_sampler), banner_text_begin='Running: ', banner_text_end=' param sets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid_sampler, random_state=9,\n",
    "    n_jobs=n_jobs, name='RF', progress_bar=bar)\n",
    "print_best_classifier(rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results_weights = evaluate_classifier(\n",
    "    X_train, y_train, X_validation, y_validation, clf=clf, param_grid=param_grid_sampler,\n",
    "    random_state=10, sample_weight=sample_weights['weight'], n_jobs=n_jobs, name='RF + sample weights',\n",
    "    progress_bar=bar)\n",
    "print_best_classifier(rf_results_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results_topics = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid_sampler,\n",
    "    random_state=11, n_jobs=n_jobs, name='RF (topics)', progress_bar=bar)\n",
    "print_best_classifier(rf_results_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results_topics_weights = evaluate_classifier(\n",
    "    X_train_topics, y_train, X_validation_topics, y_validation, clf=clf, param_grid=param_grid_sampler,\n",
    "    sample_weight=sample_weights_topics['weight'], random_state=12, n_jobs=n_jobs,\n",
    "    name='RF (topics) + sample weights', progress_bar=bar)\n",
    "print_best_classifier(rf_results_topics_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the results:\n",
    "- Once again, since none of the models selected uniform class weights, we can see that the choice of this hyperparamter is very important. In fact all four models select values for the weights which weights the laureate class much higher than the non-laureate class.\n",
    "- The original features classifiers seem to favor lower values of `min_samples_leaf` (deeper trees) and are overfitting considerably.\n",
    "- The `max_features` is low for the majority of the classifiers, which means the individual trees are diverse, but weak predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection \n",
    "\n",
    "Despite the Matthews Correlation Coefficient (MCC) being a balanced measure, which is computed using the entire confusion matrix, it is not a suitable metric for choosing between classifiers. The reason is that its value depends on the *threshold*, $T$, that is used for separating the positive from the negative class. As such, it is quite possible that two classifiers can be made to perform identically by simply adjusting one of their thresholds. To avoid such artifacts a nonparametric performance measure such as a [Receiver Operator Characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic) (ROC) curve is generally applied.\n",
    "\n",
    "The ROC curve is constructed by using different values of the threshold, $T$, \n",
    "to plot the [false positive rate](https://en.wikipedia.org/wiki/False_positive_rate) (FPR) (or alternatively the [true negative rate](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)) (TNR = 1 - FPR) on the x-axis against the [true positive rate](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) (TPR) on the y-axis. The [area under the ROC curve](https://en.wikipedia.org/wiki/Receiver_operating_characteristic#Area_under_the_curve) is typically used as a measure of the predictive performance of a classifier. An alternative, is to construct the [precision-recall](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) (PR) curve by using different values of the threshold, $T$, to plot the [recall](https://en.wikipedia.org/wiki/Precision_and_recall) on the x-axis against the [precision](https://en.wikipedia.org/wiki/Precision_and_recall) on the y-axis. In this case, the [average precision](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html) is typically used as a measure of the predictive performance of a classifier. For [imbalanced datasets the PR curve is more informative](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4349800/).\n",
    "\n",
    "I'd prefer not to resort to new measures and would like to continue working directly with the MCC. I'll follow the methodology of Zhou and Jakobsson in [Predicting Protein-Protein Interaction by the MirrortreeMethod: Possibilities and Limitations](https://www.researchgate.net/publication/259354929_Predicting_Protein-Protein_Interaction_by_the_Mirrortree_Method_Possibilities_and_Limitations) by constructing the MCC curve. This curve plots different values of the threshold, $T$, on the x-axis against the [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) (MCC) on the y-axis. The area under the MCC curve seems like an intuitive measure of the predictive performance of a classifier and is defined by:\n",
    "\n",
    "\\begin{equation}\n",
    "AUC_{MCC} = \\int_{-\\infty}^{\\infty} MCC(T) dT\n",
    "\\end{equation}\n",
    "\n",
    "However, the problem with this measure is that it is not easy to discern good performance versus bad performance. So let's normalize by the area under the MCC curve assuming $MCC \\equiv 1$ for *all* thresholds, $T$. Thus we define the **normalized area under the MCC curve** as: \n",
    "\n",
    "\\begin{equation}\n",
    "NAUC_{MCC} = \\frac{AUC_{MCC}}{AUC_{MCC \\equiv 1}}\n",
    "\\end{equation}\n",
    "\n",
    "As a result, the interpretation of the $NAUC_{MCC}$ is analogous to that of the MCC. It has an upper limit of +1 indicating a perfect prediction, a lower limit of -1 indicating total disagreement between prediction and observation and a mid value of 0 representing a random prediction. To the best of my knowledge this is the first time the normalized area under the MCC curve has been defined and used in a study.\n",
    "\n",
    "An interesting case is the case of a probabilistic classfier where $T \\in [0, 1]$. By definition, for such a classifier, $AUC_{MCC \\equiv 1} = 1$ and $NAUC_{MCC} = AUC_{MCC} = \\int_{0}^{1} MCC(T) dT$. So we see that the AUC is already normalized. For all other classifiers we should perform the normalization by integrating over the finite range of thresholds.\n",
    "\n",
    "Now let's proceed and plot the MCC curves for each of the classifiers, evaluate their normalized AUC and see how they performance compared to our [baseline classifier](5.0-baseline-model.ipynb). Note that for the support vector machine (SVM) classifiers, I have normalized the thresholds $T \\in [0, 1]$ so that their performance can be visualized on the same scale as the logistic regression (LR) and random forest (RF) classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mccs(\n",
    "    clfs, X, linestyle='solid', colors = ['black', 'blue', 'green', 'red', 'orange', 'purple'],\n",
    "    normalize_thresholds=True, ax=None):\n",
    "    \n",
    "    \"\"\"Plot the Matthew Correlation Coefficients (MCC) as a function of the classification threshold for\n",
    "        a list of classifiers. \n",
    "\n",
    "    \n",
    "    Args:\n",
    "        clfs (list): Classifiers.\n",
    "        X ({array-like, sparse matrix}, shape (n_samples, n_features)): Features matrix, where n_samples\n",
    "            is the number of samples and n_features is the number of features.\n",
    "        linestyle (str, optional): Defaults to 'solid'. Linestyle to plot.\n",
    "        colors (list, optional): Defaults to ['black', 'blue', 'green', 'red', 'orange', 'purple']. Colors\n",
    "            to plot the lines in. They will be colored according to this order.\n",
    "        normalize_thresholds (bool, optional): Defaults to True. Normalized the thresholds to lie in [0, 1].\n",
    "        ax (matplotlib.axes.Axes, optional): Defaults to None. axes.\n",
    "    \n",
    "    Raises:\n",
    "        NotImplementedError: Raises if classifier does not have `predict_proba` or `decision_function`\n",
    "            attribute.\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.axes.Axes): axes.\n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    for color, clf in enumerate(clfs):\n",
    "        best_classifier = clf['best_classifier']\n",
    "        if hasattr(best_classifier, 'predict_proba'):\n",
    "            y_score = best_classifier.predict_proba(X)[:, 1]\n",
    "            probability = True\n",
    "        elif hasattr(best_classifier, 'decision_function'):\n",
    "            y_score = best_classifier.decision_function(X)\n",
    "            probability = False\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        mccs, _, _, thresholds = mcc_curve(y_true=y_validation, y_score=y_score,\n",
    "                                           probability=probability)\n",
    "        mcc_auc = mcc_auc_score(y_true=y_validation, y_score=y_score, sample_weight=None,\n",
    "                                probability=probability)\n",
    "        \n",
    "        if normalize_thresholds and (np.min(thresholds) < 0.0 or np.max(thresholds) > 1.0):\n",
    "            scaler = MinMaxScaler()\n",
    "            thresholds = scaler.fit_transform(thresholds.reshape(-1, 1))\n",
    "\n",
    "        ax.plot(thresholds, mccs, linestyle=linestyle, color=colors[color],\n",
    "                label=clf['name'] + ' - {} NAUC'.format(round(mcc_auc, 3)))\n",
    "\n",
    "    ax.set_xlabel('Classification Threshold')\n",
    "    ax.set_ylabel('Matthews Correlation Coefficient (MCC)')\n",
    "    ax.set_title('Matthews Correlation Coefficient (MCC) vs Classification Threshold')\n",
    "    ax.legend()\n",
    "\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [logit_results, logit_results_weights, svm_results, svm_results_weights, rf_results,\n",
    "               rf_results_weights]\n",
    "classifiers_topics = [logit_results_topics, logit_results_topics_weights, svm_results_topics,\n",
    "                      svm_results_topics_weights, rf_results_topics, rf_results_topics_weights]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = plot_mccs(classifiers, X_validation)\n",
    "ax = plot_mccs(classifiers_topics, X_validation_topics, linestyle='dashed', ax=ax)\n",
    "baseline_mcc = matthews_corrcoef(y_validation, X_validation.num_workplaces_at_least_2)\n",
    "ax.axhline(y=baseline_mcc, linestyle='-.',\n",
    "           label='Baseline' + ' - {} MCC'.format(round(baseline_mcc, 3)))\n",
    "ax.set_xlim(0.0, 1.0)\n",
    "ax.set_ylim(-0.2, 1.0)\n",
    "ax.set_xlabel('Normalized Classification Threshold')\n",
    "ax.set_title('Matthews Correlation Coefficient (MCC) vs Normalized Classification Threshold')\n",
    "ax.legend(ncol=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the chart:\n",
    "\n",
    "1. All of the models beat the naive baseline classifier over at least some range of classification thresholds. As such we can conclude that machine learning is appropriate for this task.\n",
    "2. All $NAUC_{MCC}$'s are significantly greater than zero. This lends further support to the first point. It looks like there is some signal in the data for predicting the target. However, the low values indicate that the majority of the classifiers are weak.\n",
    "3. The logistic regression classifiers peform better than the random forest classifier, which in turn, perform better than the support vector machine classifiers.\n",
    "4. Performance of the classifiers trained with the topics features are worse than those trained with the original features. It seems like some important information was lost during topic modeling.\n",
    "5. The effect of the sample weights varies with classifier type and feature type. Using sample weights improves the performance of the logistic regression and support vector machine classifiers when using the original features. The improvement in performance of the logistic regression model is really impressive! However, sample weights worsen the performance when the topics features are used or the classifier is a random forest.\n",
    "\n",
    "Clearly we should select the logistic regression with sample weights (LR + sample weights) classifier as it is the standout performer based on $NAUC_{MCC}$. It is also very interesting and a good characteristic that this is the only classifier which is relatively insensitive to the classification threshold. Whether or not it is a good classifier is open to interpretation as it depends on the context and purposes. The task of predicting Nobel Physics Laureates is a difficult one, so I think this is most likely a moderate to good classifier at best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Classification Threshold\n",
    "\n",
    "Our work is not quite done yet as we still have to choose an optimal classification threshold for the LR + sample weights classifier. Below we plot the classification threshold against the Matthews Correlation Coefficient (MCC), true positive rate (TPR) and true negative rate (TNR) to help us with this selection. Naturally, there is a tradeoff between the TPR and the TNR. Minimizing false positives (i.e. maximizing the TNR) is more important than minimizing false negatives (i.e. maximizing the TPR) when classifying the physicists as laureates and non-laureates. This is because we are trying to gain insight into any kinds of biases that may involved when a physicist is awarded the Nobel Prize. So any threshold to the right of the intersection of the TPR and TNR would satisfy this criteria. However, we must also ensure that we retrieve as many of the true positives as possible so that the conclusions we draw are based on an as large as possible sample of *actual* laureates. So intuitively, the threshold corresponding to the maximum value of the MCC, seems to be a good choice for the optimal classification threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mcc_tpr_tnr(clf, X, ax=None):\n",
    "    \"\"\"Plot the Matthew Correlation Coefficients (MCC), true positive rate (TPR) and true negative rate (TNR) \n",
    "        as a function of the classification threshold for a classifier. \n",
    "    \n",
    "    Args:\n",
    "        clf ([type]): [description]\n",
    "        X ({array-like, sparse matrix}, shape (n_samples, n_features)): Features matrix, where n_samples\n",
    "            is the number of samples and n_features is the number of features.\n",
    "        ax (matplotlib.axes.Axes, optional): Defaults to None. axes.\n",
    "    \n",
    "    Raises:\n",
    "        NotImplementedError: Raises if classifier does not have `predict_proba` or `decision_function`\n",
    "            attribute.\n",
    "    \n",
    "    Returns:\n",
    "        matplotlib.axes.Axes): axes.\n",
    "\n",
    "        float: Threshold corresponding to maximum MCC. \n",
    "    \"\"\"\n",
    "\n",
    "    if not ax:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    best_classifier = clf['best_classifier']\n",
    "    if hasattr(best_classifier, 'predict_proba'):\n",
    "        y_score = best_classifier.predict_proba(X)[:, 1]\n",
    "        probability = True\n",
    "    elif hasattr(best_classifier, 'decision_function'):\n",
    "        y_score = best_classifier.decision_function(X)\n",
    "        probability = False\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    mcc, tnr, tpr, thresholds = mcc_curve(y_true=y_validation, y_score=y_score, probability=probability)\n",
    "\n",
    "    ax.plot(thresholds, mcc, color='black', label='Matthews Correlation Coefficient (MCC)')\n",
    "    ax.plot(thresholds, tpr, color='blue', label='True Positive Rate (TPR)')\n",
    "    ax.plot(thresholds, tnr, color='green', label='True Negative Rate (TNR)')\n",
    "    \n",
    "    max_mcc_idx = np.argmax(mcc)\n",
    "    ax.axvline(thresholds[max_mcc_idx], color='red', linestyle='dashed', linewidth=1.0,\n",
    "               label='Max MCC = {} (threshold = {})'.format(round(np.max(mcc), 3),\n",
    "                                                            round(thresholds[max_mcc_idx], 3)))\n",
    "    ax.set_xlim(0.0, 1.0)\n",
    "    ax.set_ylim(0.0, 1.0)\n",
    "    \n",
    "    ax.set_xlabel('Classification Threshold')\n",
    "    ax.set_title('Matthews Correlation Coefficient (MCC) / True Positive Rate (TPR) \\n' \n",
    "                 '/ False Negative Rate (FNR) vs Classification Threshold')\n",
    "    ax.legend()\n",
    "\n",
    "    return ax, thresholds[max_mcc_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, optimal_threshold = plot_mcc_tpr_tnr(logit_results_weights, X_validation);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternatively, if you prefer, the confusion matrix and the classification report of this classifier tell a similar story to what I've described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier = logit_results_weights['best_classifier']\n",
    "y_pred = (best_classifier.predict_proba(X_validation)[:, 1] > optimal_threshold).astype('int64')\n",
    "display(confusion_matrix_to_dataframe(confusion_matrix(y_validation, y_pred)))\n",
    "print(classification_report(y_validation, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Best Classifier Parameters\n",
    "\n",
    "Now we have the best classifier, let's persist it's parameters and associated metadata, which will allow us to reconstruct the classifier at any point in the future. Note that I am not persisting the actual sklearn estimator using [joblib](https://pypi.org/project/joblib/) or [pickle](https://docs.python.org/3/library/pickle.html) as I'd like to avoid any [compatibility or security issues](https://stackabuse.com/scikit-learn-save-and-restore-models/#compatibilityissues). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier_params = pd.Series(\n",
    "    type(best_classifier), name=logit_results_weights['name'].replace(' ', '_'), index=['estimator'])\n",
    "best_classifier_params['params'] = best_classifier.get_params()\n",
    "best_classifier_params['threshold'] = optimal_threshold\n",
    "best_classifier_params['sample_weight'] = '../models/train-features-sample-weights.csv'\n",
    "best_classifier_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier_params.to_csv('../models/' + best_classifier_params.name + '.csv', header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check let's recreate the estimator and make sure that we get the same results as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier_params_check = pd.read_csv('../models/' + best_classifier_params.name + '.csv',\n",
    "                                           squeeze=True, index_col=0)\n",
    "best_classifier_params_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_classifier_check = LogisticRegression()\n",
    "best_classifier_check.set_params(**ast.literal_eval(best_classifier_params_check.params))\n",
    "best_classifier_check.fit(\n",
    "    X_train, y_train, sample_weight=pd.read_csv(best_classifier_params_check.sample_weight)['weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(np.array_equal(best_classifier_check.predict(X_validation), best_classifier.predict(X_validation)))\n",
    "np.testing.assert_allclose(best_classifier_check.predict_proba(X_validation),\n",
    "                           best_classifier.predict_proba(X_validation))\n",
    "y_pred_check = (\n",
    "    (best_classifier_check.predict_proba(\n",
    "        X_validation)[:, 1] > best_classifier_params.threshold).astype('int64'))\n",
    "assert(np.array_equal(y_pred_check, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, everything looks good."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
