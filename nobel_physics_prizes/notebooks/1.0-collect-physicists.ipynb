{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Physicists\n",
    "\n",
    "For this project, I need a list of physicists who are notable for their achievements. Wikipedia contains two such lists, one general [list of physicists](https://en.wikipedia.org/wiki/List_of_physicists) and another list of [theoretical physicists](https://en.wikipedia.org/wiki/List_of_theoretical_physicists). I will scrape these lists and unify them into a single list. It is important to recognize that some of these physicists have won the *Nobel Prize* and some have not and also that some are *dead* and some are *alive*. You should at least recognize a few of the more famous names in the list even if you do not recognize them all. The entire analysis of this project will be based on the data that is acquired on these physicists. OK time to get scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "An initialization step is needed to setup the environment:\n",
    "- The locale needs to be set for all categories to the user’s default setting (typically specified in the LANG environment variable) to enable correct sorting of physicists names with accents.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import string\n",
    "import time\n",
    "from urllib import parse\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import NavigableString\n",
    "import numpy as np\n",
    "import progressbar as pb\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_ALL, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping the Physicists\n",
    "\n",
    "I use a combination of *requests* and *beautifulsoup* to scrape the links from the Wikipedia pages. I filter the list of links down to only those containing physicist names. The important point to note is that I actually need to send HTTP requests to fetch the pages associated with these links as some of them are redirected to different URLs. The really tricky part is that the redirects are done via javascript so they are not detected by *requests*. As a result I have to parse the javascript to find the redirect link.\n",
    "\n",
    "Even after all of this, some of the redirected Wikipedia links are not in sync with the DBpedia links. This means that when I later try to fetch the data from DBpedia, the links resolve the the wrong resource. So I force these redirects manually here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKI_URL = 'https://en.wikipedia.org/wiki/'\n",
    "\n",
    "def get_notable_physicists(progress_bar=None):\n",
    "    \"\"\"Get a list of notable physicists.\n",
    "    Args:\n",
    "        progress_bar (progressbar.ProgressBar): Progress bar.\n",
    "\n",
    "    Returns:\n",
    "        list (str): List of names of notable physicists.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # get the theoretical physicists\n",
    "    theoretical_physicists = _get_linked_article_titles(\n",
    "        WIKI_URL + 'List_of_theoretical_physicists',\n",
    "        section_titles=[\n",
    "            'Ancient times',\n",
    "            'Middle_Ages',\n",
    "            '15th–16th century',\n",
    "            '16th–17th century',\n",
    "            '17th–18th century',\n",
    "            '18th–19th century',\n",
    "            '19th century',\n",
    "            '19th–20th century',\n",
    "            '20th century',\n",
    "            '20th–21st century'\n",
    "        ]\n",
    "    )\n",
    "    assert(len(theoretical_physicists) == 267)\n",
    "\n",
    "    # get the physicists\n",
    "    blacklist_links = [\n",
    "            'Newcastle University',  # university\n",
    "            # leads to the physicist at this foreign language link:\n",
    "            # https://tr.wikipedia.org/wiki/Victor_Twersky_(fizik%C3%A7i)\n",
    "            'Ernst equation',  # equation\n",
    "            'Matthew Sanders',  # not a physicist\n",
    "            'Ricardo Carezani',  # not found in DBpedia (misspelt there?)\n",
    "            'Twersky#Twersky'  # a group of people of this name\n",
    "        ]\n",
    "    physicists = _get_linked_article_titles(\n",
    "        WIKI_URL + 'List_of_physicists',\n",
    "        section_titles=list(string.ascii_uppercase),\n",
    "        blacklist_links=blacklist_links\n",
    "    )\n",
    "    assert(len(physicists) == 974)\n",
    "    assert(not set(blacklist_links).intersection(set(physicists)))\n",
    "\n",
    "    # merge the lists\n",
    "    notable_physicists = list(set(theoretical_physicists + physicists))\n",
    "\n",
    "    # get the redirect title (if any) from a HTTP request\n",
    "    forced_redirects = {\n",
    "        # DBpedia not in sync with Wikipedia for these names\n",
    "        # so force these redirects\n",
    "        'Ea Ea': 'Craige Schensted',\n",
    "        'Gian Carlo Wick': 'Gian-Carlo Wick',\n",
    "        'Hans Adolf Buchdahl': 'Hans Adolph Buchdahl',\n",
    "        'James Jeans': 'James Hopwood Jeans',\n",
    "        'Lawrence Bragg' : 'William Lawrence Bragg',\n",
    "        \"Shin'ichirō Tomonaga\": \"Sin'ichirō_Tomonaga\",\n",
    "        'Thales of Miletus': 'Thales'\n",
    "    }\n",
    "    notable_physicists = _get_redirected_titles(notable_physicists,\n",
    "                                                forced_redirects,\n",
    "                                                progress_bar)\n",
    "    assert(len(notable_physicists) == 1059)\n",
    "    assert(not set(forced_redirects.keys()).intersection(\n",
    "        set(notable_physicists)))\n",
    "    assert(set(forced_redirects.values()).intersection(\n",
    "        set(notable_physicists)))\n",
    "\n",
    "    # sort the list\n",
    "    notable_physicists.sort(key=locale.strxfrm)\n",
    "    return notable_physicists\n",
    "\n",
    "\n",
    "def _get_linked_article_titles(url, section_titles,\n",
    "                               blacklist_links=None):\n",
    "    # fetch the page\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text)\n",
    "\n",
    "    # loop to find links\n",
    "    article_titles = []\n",
    "    for section_title in section_titles:\n",
    "        span_id = section_title.replace(' ', '_')\n",
    "        section = soup.find('span', id=span_id)\n",
    "        ul = section.find_next('ul')\n",
    "        for link in ul.find_all('a', href=True):\n",
    "            if not link['href'].startswith('/wiki/'):\n",
    "                # skip external and dead (redlink=1) links\n",
    "                continue\n",
    "            article_title = (\n",
    "                link['href'].replace('/wiki/', '').replace('_', ' ')\n",
    "            )\n",
    "            if (not blacklist_links or \n",
    "                article_title not in blacklist_links):\n",
    "                article_titles.append(article_title)\n",
    "    return article_titles\n",
    "\n",
    "\n",
    "def _get_redirected_titles(titles, forced_redirects=None,\n",
    "                           progress_bar=None):\n",
    "    redirected_titles = set()\n",
    "\n",
    "    for i in range(len(titles)):\n",
    "        # fetch the page\n",
    "        url = WIKI_URL + str(titles[i]).replace(' ', '_')\n",
    "        time.sleep(1)  # delay to crawl responsibly\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # parse javascript for redirects\n",
    "        redirected = False\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            REDIRECT = '\"wgInternalRedirectTargetUrl\":'\n",
    "            soup = BeautifulSoup(response.text)\n",
    "            for script_tag in soup.find_all(name='script'):\n",
    "                script_code = script_tag.string\n",
    "                if (isinstance(script_code, NavigableString) and\n",
    "                        REDIRECT in script_code):\n",
    "                    start = script_code.find(REDIRECT)\n",
    "                    end = script_code.find('\"', start + len(REDIRECT) + 1)\n",
    "                    redirected_title = (\n",
    "                        script_code[start + len(REDIRECT) + 1:end]\n",
    "                        .replace('/wiki/', '').replace('_', ' ')\n",
    "                    )\n",
    "                    # some physicist names contain unicode characters\n",
    "                    # which have been quoted when in a url\n",
    "                    # unquote for sorting\n",
    "                    redirected_titles.add(parse.unquote(redirected_title))\n",
    "                    redirected = True\n",
    "        if not redirected:\n",
    "            redirected_titles.add(parse.unquote(titles[i]))\n",
    "        \n",
    "        if progress_bar:\n",
    "            progress_bar.update(i)\n",
    "            \n",
    "    # force redirects\n",
    "    redirected_titles = list(redirected_titles)\n",
    "    for key, value in forced_redirects.items():\n",
    "        redirected_titles[redirected_titles.index(key)] = value\n",
    "            \n",
    "    return redirected_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets = [\n",
    "    'Checking: ', pb.Counter(),\n",
    "    ' / ' + str(1084) + ' urls',\n",
    "    ' ', pb.Bar(marker='█'),\n",
    "    ' ', pb.Percentage(),\n",
    "    ' ', pb.Timer(),\n",
    "    ' ', pb.ETA()\n",
    "]\n",
    "bar = pb.ProgressBar(max_value=1084, widgets=widgets,\n",
    "                     redirect_stdout=True).start()\n",
    "\n",
    "notable_physicists = get_notable_physicists(bar)\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that there are no duplicate names and how many names we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(np.unique(notable_physicists)) == len(notable_physicists))\n",
    "len(notable_physicists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the list to a file for future use and check the list of names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/physicists.txt', mode='w', encoding='utf-8') as file:\n",
    "    file.writelines('\\n'.join(notable_physicists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pycat ../data/raw/physicists.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "A few clean up steps are needed:\n",
    "\n",
    "- Convert the notebook to a HTML file with all the output.\n",
    "- Convert the notebook to another notebook with the output removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=False --output-dir html_output --to html 1.0-collect-physicists.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to notebook 1.0-collect-physicists.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
