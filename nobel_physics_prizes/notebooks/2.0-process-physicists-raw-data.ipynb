{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Physicists Raw Data\n",
    "\n",
    "Now I aim to convert the *JSON lines* format [notable physicists raw data](../data/raw/notable_physicists.jsonl.gz) into an intermediate format *pandas dataframe* that is more convenient to work with. This intermediate format will be close to the final format of the data that I'll be working with for analysis.\n",
    "\n",
    "My goal is to parse the JSON data in order to extract the interesting fields of information such as *name*, *birth date*, *death date*, *citizenships*, *workplaces*, *awards*, *alma mater*, *academic advisors*, *doctoral students*, *DBpedia categories*, *description*, etc. I believe that this information may be useful in helping to predict whether or not a physicist is awarded a Nobel Prize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from datetime import datetime\n",
    "import gzip\n",
    "import locale\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "An few initialization steps are needed to setup the environment:\n",
    "- The locale needs to be set for all categories to the userâ€™s default setting (typically specified in the LANG environment variable) to enable correct sorting of words with accents.\n",
    "- [spacy](https://spacy.io/) is a python package that is used for *natural language processing*. It relies on statistical [language models](https://spacy.io/usage/models) which must be loaded before use. So here I load `en_core_web_sm` which is the default English language model in spacy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the JSON Lines Data\n",
    "\n",
    "First let's extract the JSON lines data and read it into a list for future parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzip_file = '../data/raw/notable_physicists.jsonl.gz'\n",
    "jsonl_file = gzip_file[:-3]\n",
    "with gzip.open(gzip_file, 'rb') as src, open(jsonl_file, 'wb') as dest:\n",
    "    shutil.copyfileobj(src, dest)\n",
    "\n",
    "json_lines = []\n",
    "with jsonlines.open(jsonl_file, mode='r') as reader:\n",
    "    for json_line in reader:\n",
    "        json_lines.append(json_line)\n",
    "os.remove(jsonl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Fields of Interest\n",
    "\n",
    "Here I define the keys and values that I wish to extract from the JSON lines data and also some that I explicitly wish to exclude. These keys and values are in the form of *semantic URLs* which allows anyone to visit the resource in a web browser and see their meaning. The URLs are in 5 namespaces:\n",
    "\n",
    "- [DBpedia Ontology](https://wiki.dbpedia.org/services-resources/ontology)\n",
    "- DBpedia Property\n",
    "- PURL\n",
    "- W3\n",
    "- FOAF\n",
    "\n",
    "As you can see, there are a lot of duplicate keys in the ontology and property namespaces (e.g. citizenship). Also, there are a lot of slightly different named keys that sound like they hold similar information (e.g. alma mater and education). Clearly, a set of rules will need to be defined about how to handle these duplicate keys and named variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPEDIA_JSON_KEYS = [\n",
    "    # DBpedia ontology\n",
    "    'http://dbpedia.org/ontology/abstract',\n",
    "    'http://dbpedia.org/ontology/academicAdvisor',\n",
    "    'http://dbpedia.org/ontology/almaMater',\n",
    "    'http://dbpedia.org/ontology/award',\n",
    "    'http://dbpedia.org/ontology/birthDate',\n",
    "    'http://dbpedia.org/ontology/birthName',\n",
    "    'http://dbpedia.org/ontology/birthPlace',\n",
    "    'http://dbpedia.org/ontology/child',\n",
    "    'http://dbpedia.org/ontology/citizenship',\n",
    "    'http://dbpedia.org/ontology/deathDate',\n",
    "    'http://dbpedia.org/ontology/deathPlace',\n",
    "    'http://dbpedia.org/ontology/doctoralAdvisor',\n",
    "    'http://dbpedia.org/ontology/doctoralStudent',\n",
    "    'http://dbpedia.org/ontology/education',\n",
    "    'http://dbpedia.org/ontology/field',\n",
    "    'http://dbpedia.org/ontology/influenced',\n",
    "    'http://dbpedia.org/ontology/knownFor',\n",
    "    'http://dbpedia.org/ontology/nationality',\n",
    "    'http://dbpedia.org/ontology/notableStudent',\n",
    "    'http://dbpedia.org/ontology/residence',\n",
    "    'http://dbpedia.org/ontology/spouse',\n",
    "    'http://dbpedia.org/ontology/thumbnail',\n",
    "    'http://dbpedia.org/ontology/wikiPageID',\n",
    "    'http://dbpedia.org/ontology/wikiPageRevisionID',\n",
    "\n",
    "    # DBpedia property\n",
    "    'http://dbpedia.org/property/academicAdvisor',\n",
    "    'http://dbpedia.org/property/almaMater',\n",
    "    'http://dbpedia.org/property/award',\n",
    "    'http://dbpedia.org/property/birthDate',\n",
    "    'http://dbpedia.org/property/birthName',\n",
    "    'http://dbpedia.org/property/birthPlace',\n",
    "    'http://dbpedia.org/property/child',\n",
    "    'http://dbpedia.org/property/children',\n",
    "    'http://dbpedia.org/property/citizenship',\n",
    "    'http://dbpedia.org/property/deathDate',\n",
    "    'http://dbpedia.org/property/deathPlace',\n",
    "    'http://dbpedia.org/property/doctoralAdvisor',\n",
    "    'http://dbpedia.org/property/doctoralStudent',\n",
    "    'http://dbpedia.org/property/education',\n",
    "    'http://dbpedia.org/property/field',\n",
    "    'http://dbpedia.org/property/influenced',\n",
    "    'http://dbpedia.org/property/knownFor',\n",
    "    'http://dbpedia.org/property/nationality',\n",
    "    'http://dbpedia.org/property/notableStudent',\n",
    "    'http://dbpedia.org/property/residence',\n",
    "    'http://dbpedia.org/property/signature',\n",
    "    'http://dbpedia.org/property/spouse',\n",
    "    'http://dbpedia.org/property/thesisTitle',\n",
    "    'http://dbpedia.org/property/thesisUrl',\n",
    "    'http://dbpedia.org/property/thesisYear',\n",
    "    'http://dbpedia.org/property/thumbnail',\n",
    "    'http://dbpedia.org/property/workInstitutions',\n",
    "    'http://dbpedia.org/property/workplaces',\n",
    "\n",
    "    # PURL\n",
    "    'http://purl.org/dc/terms/description',\n",
    "\n",
    "    # W3\n",
    "    'http://www.w3.org/2000/01/rdf-schema#comment',\n",
    "    'http://www.w3.org/ns/prov#wasDerivedFrom',\n",
    "\n",
    "    # FOAF\n",
    "    'http://xmlns.com/foaf/0.1/gender',\n",
    "    'http://xmlns.com/foaf/0.1/givenName',\n",
    "    'http://xmlns.com/foaf/0.1/isPrimaryTopicOf',\n",
    "    'http://xmlns.com/foaf/0.1/name',\n",
    "    'http://xmlns.com/foaf/0.1/surname'\n",
    "]\n",
    "\n",
    "DBPEDIA_JSON_VALUES = [\n",
    "    # DBpedia ontology\n",
    "    'http://dbpedia.org/ontology/influenced',  # target is influencedBy\n",
    "    'http://dbpedia.org/ontology/influencedBy',  # target is influenced\n",
    "\n",
    "    # DBpedia property\n",
    "    'http://dbpedia.org/property/theorized'\n",
    "]\n",
    "\n",
    "DBPEDIA_IGNORE_URLS = [\n",
    "    'http://dbpedia.org/resource/Bachelor_of_Arts',\n",
    "    'http://dbpedia.org/resource/Bachelor_of_Science',\n",
    "    'http://dbpedia.org/resource/Doctor_of_Philosophy',\n",
    "    'http://dbpedia.org/resource/Master_of_Arts',\n",
    "    'http://dbpedia.org/resource/Master_of_Philosophy',\n",
    "    'http://dbpedia.org/resource/Master_of_Science',\n",
    "    'http://dbpedia.org/resource/None',\n",
    "    'http://dbpedia.org/resource/PhD',\n",
    "    'http://dbpedia.org/resource/Ph.D',\n",
    "    'http://dbpedia.org/resource/Ph.D.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Physicists Dataframe\n",
    "\n",
    "Now I parse the JSON lines data to create dictionaries and use the dictionaries to create a dataframe. The following rules apply when creating the dictionaries:\n",
    "\n",
    "1. Values in the DBpedia Ontology namespace takes precedence over those in the DBpedia Property namespace since, as described in section 4.3 of [DBpedia datasets](https://wiki.dbpedia.org/services-resources/datasets/dbpedia-datasets), it contains the cleanest data. Property namespace values are only used when there are no corresponding ontology namespace values.\n",
    "\n",
    "2. Several keys holding similar data are merged together. *Education* is merged into *alma mater*, *work institutions* is merged into *workplaces* and *child* is merged into children. *InfluencedBy* and *Influenced* are also merged appropriately.\n",
    "\n",
    "3. Some of the fields (e.g. the *abstract*) is multilingual. In such cases, only the English is extracted.\n",
    "\n",
    "4. The data is messy and some cleanup is done on the fields involving *dates*, *nationality*, *citizenship*, *spouse* and *children*. However, a lot of noise remains in the data and will need to handled when generating features for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_physicist_data(json_line):\n",
    "    \"\"\"Create physicist data from json_line data.\n",
    "\n",
    "    Args:\n",
    "        json_line (dict): JSON dict.\n",
    "    Returns:\n",
    "        dict: Dictionary of physicist data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # flatten the json\n",
    "    flat_json = json_normalize(json_line)\n",
    "\n",
    "    # find the resource, source and fullName\n",
    "    resource = _find_resource(flat_json)\n",
    "    source = _get_source(resource)\n",
    "    full_name = _get_filename_from_url(resource).replace('_', ' ')\n",
    "\n",
    "    # construct the dictionary\n",
    "    dict_ = {'resource': resource, 'source': source, 'fullName': full_name,\n",
    "             **_process_json_keys(resource, flat_json),\n",
    "             **_process_json_values(resource, flat_json),\n",
    "             **_process_categories(flat_json)}\n",
    "\n",
    "    # merge keys\n",
    "    dict_ = _merge_keys(dict_)\n",
    "\n",
    "    # clean fields\n",
    "    dict_ = _clean_fields(dict_)\n",
    "\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _process_json_keys(resource, flat_json):\n",
    "    dict_ = {}\n",
    "\n",
    "    # loop over the keys\n",
    "    for json_key in DBPEDIA_JSON_KEYS:\n",
    "        flat_json_key = resource + '.' + json_key\n",
    "        filename = _get_filename_from_url(json_key)\n",
    "        dict_key = filename[filename.rfind('#') + 1:]  # take fragment\n",
    "        # sanitize for later merging in _merge_influences\n",
    "        if dict_key == 'influenced':\n",
    "            dict_key = 'influenced_'\n",
    "        if flat_json_key not in flat_json:\n",
    "            continue\n",
    "\n",
    "        # loop and get the values\n",
    "        for list_ in flat_json[flat_json_key].values:\n",
    "            val_list = []\n",
    "            for val in list_:\n",
    "                if not _val_is_english(val):\n",
    "                    continue\n",
    "\n",
    "                if isinstance(val['value'], int):\n",
    "                    value = val['value']\n",
    "                else:  # str\n",
    "                    value = val['value'].lstrip('* ')\n",
    "\n",
    "                if value in DBPEDIA_IGNORE_URLS:\n",
    "                    continue\n",
    "\n",
    "                if not _value_has_information_content(value):\n",
    "                    continue\n",
    "\n",
    "                if isinstance(value, str):\n",
    "                    if value.startswith('http://dbpedia.org/'):\n",
    "                        value = _get_filename_from_url(value).replace('_', ' ')\n",
    "                    elif value.startswith('http://wikidata.dbpedia.org/'):\n",
    "                        continue\n",
    "                val_list.append(value)\n",
    "\n",
    "            # split such a string into a list of values\n",
    "            if (len(val_list) == 1 and isinstance(val_list[0], str) and\n",
    "                    '\\n*' in val_list[0]):\n",
    "                val_list = (val_list[0].replace('\\n* ', '\\n*')\n",
    "                                       .replace(' \\n*', '\\n*').split('\\n*'))\n",
    "            if not val_list:\n",
    "                continue\n",
    "            elif len(val_list) == 1 and dict_key not in dict_:\n",
    "                dict_[dict_key] = val_list[0]\n",
    "            else:\n",
    "                if not all(isinstance(v, str) for v in val_list):\n",
    "                    # multiple values must all be of 'str' type\n",
    "                    # (some children and spouse have int values)\n",
    "                    val_list = [v for v in val_list if\n",
    "                                not isinstance(v, int)]\n",
    "                if dict_key not in dict_:\n",
    "                    val_list.sort(key=locale.strxfrm)\n",
    "                    dict_[dict_key] = '|'.join(val_list)\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _process_json_values(resource, flat_json):\n",
    "    dict_ = {}\n",
    "\n",
    "    # loop over the values\n",
    "    for json_value in DBPEDIA_JSON_VALUES:\n",
    "        # loop and get the keys\n",
    "        key_list = []\n",
    "        for json_key in flat_json:\n",
    "            sep_position = json_key.rfind('.http')\n",
    "            key, val = json_key[:sep_position], json_key[sep_position + 1:]\n",
    "            # only consider keys other than the resource\n",
    "            if json_value == val and not key == resource:\n",
    "                dict_key = _get_filename_from_url(key).replace('_', ' ')\n",
    "                key_list.append(dict_key)\n",
    "                dict_val = _get_filename_from_url(val).replace('_', ' ')\n",
    "        if not key_list:\n",
    "            continue\n",
    "        elif len(key_list) == 1:\n",
    "            dict_[dict_val] = key_list[0]\n",
    "        else:\n",
    "            key_list.sort(key=locale.strxfrm)\n",
    "            dict_[dict_val] = '|'.join(key_list)\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _process_categories(flat_json):\n",
    "    dict_ = {}\n",
    "\n",
    "    val_list = []\n",
    "    for list_ in flat_json.values:\n",
    "        for item in list_:\n",
    "            for val in item:\n",
    "                value = val['value']\n",
    "                if (not isinstance(value, str) or not value.startswith(\n",
    "                        'http://dbpedia.org/resource/Category:')):\n",
    "                    continue\n",
    "                value = (_get_filename_from_url(value)\n",
    "                         .replace('_', ' ')\n",
    "                         .replace('Category:', ''))\n",
    "                val_list.append(value)\n",
    "    if val_list:\n",
    "        val_list.sort(key=locale.strxfrm)\n",
    "        dict_['categories'] = '|'.join(val_list)\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _merge_keys(dict_):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    merged_dict = _merge_influences(merged_dict)\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'almaMater', 'education')\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'workplaces', 'workInstitutions')\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'children', 'child')\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _clean_fields(dict_):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    cleaned_dict = _clean_dates(cleaned_dict, 'birthDate')\n",
    "    cleaned_dict = _clean_dates(cleaned_dict, 'deathDate')\n",
    "    cleaned_dict = _clean_citizenship_nationality(cleaned_dict, 'citizenship')\n",
    "    cleaned_dict = _clean_citizenship_nationality(cleaned_dict, 'nationality')\n",
    "    cleaned_dict = _clean_spouse_children(cleaned_dict, 'spouse')\n",
    "    cleaned_dict = _clean_spouse_children(cleaned_dict, 'children')\n",
    "    cleaned_dict = _clean_almaMater(cleaned_dict)\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _find_resource(flat_json):\n",
    "    # resource is the most common value\n",
    "    vals = [val.split('.h')[0] for val in flat_json.columns.values]\n",
    "    return collections.Counter(vals).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def _get_source(resource):\n",
    "    parsed_url = urlparse(resource)\n",
    "    filename = _get_filename_from_url(resource)\n",
    "    return ('{uri.scheme}://{uri.netloc}/'.format(uri=parsed_url) +\n",
    "            'data/' + filename + '.json')\n",
    "\n",
    "\n",
    "def _get_filename_from_url(url):\n",
    "    return url[url.rfind('/') + 1:]\n",
    "\n",
    "\n",
    "def _val_is_english(val):\n",
    "    language = val.get('lang')\n",
    "    if not language or language == 'en':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _value_has_information_content(value):\n",
    "    if not isinstance(value, str):\n",
    "        return True\n",
    "    if (not value or\n",
    "            value == '*' or\n",
    "            value.startswith('* \\n') or\n",
    "            value.endswith('\\n*') or\n",
    "            value.startswith('--') or\n",
    "            '_family' in value or  # e.g. children contains this\n",
    "            value in spacy.lang.en.STOP_WORDS):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _merge_influences(dict_):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    influenced = merged_dict.get('influenced')\n",
    "    influenced_ = merged_dict.get('influenced_')\n",
    "\n",
    "    if influenced_:\n",
    "        if influencedBy:\n",
    "            merged_dict['influencedBy'] += '|' + influenced_\n",
    "        else:\n",
    "            merged_dict['influencedBy'] = influenced_\n",
    "        del merged_dict['influenced_']\n",
    "\n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    if influencedBy and influenced:\n",
    "        merged_dict['influencedBy'], merged_dict['influenced'] = \\\n",
    "            merged_dict['influenced'], merged_dict['influencedBy']\n",
    "    elif influencedBy and not influenced:\n",
    "        merged_dict['influenced'] = influencedBy\n",
    "        del merged_dict['influencedBy']\n",
    "    elif not influencedBy and influenced:\n",
    "        merged_dict['influencedBy'] = influenced\n",
    "        del merged_dict['influenced']\n",
    "    \n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    influenced = merged_dict.get('influenced')\n",
    "    if influencedBy:\n",
    "        merged_dict['influencedBy'] = '|'.join(sorted(\n",
    "            merged_dict['influencedBy'].split('|'), key=locale.strxfrm))\n",
    "    if influenced:\n",
    "        merged_dict['influenced'] = '|'.join(sorted(\n",
    "            merged_dict['influenced'].split('|'), key=locale.strxfrm))\n",
    "\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _merge_right_key_into_left_key(dict_, left_key, right_key):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    left = merged_dict.get(left_key)\n",
    "    right = merged_dict.get(right_key)\n",
    "\n",
    "    if not left and right:\n",
    "        merged_dict[left_key] = right\n",
    "    merged_dict.pop(right_key, None)\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _clean_citizenship_nationality(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get(key)\n",
    "    if (key_present and not\n",
    "        key_present == 'United Kingdom of Great Britain and Ireland'):\n",
    "        cleaned_dict[key] = (\n",
    "            ' '.join(key_present.split())\n",
    "            .replace(' / ', '|')\n",
    "            .replace(' , ', '|')\n",
    "            .replace(', ', '|')\n",
    "            .replace(' and then ', '|')\n",
    "            .replace(' and ', '|')\n",
    "            .replace('Citizenship in the ', '')\n",
    "            .replace('Citizen of ', '')\n",
    "            .replace(' citizen', '')\n",
    "            .replace(' law', '')\n",
    "            .replace('List of ', '')\n",
    "            .replace('Nationality law of the ', '')\n",
    "            .replace('Nationality of the ', '')\n",
    "            .replace(' nationality law', '')\n",
    "            .replace(' nationality', '')\n",
    "            .replace('People of ', '')\n",
    "            .replace(' people', '')\n",
    "            .replace(' subject', '')\n",
    "        )\n",
    "        cleaned_dict[key] = '|'.join(sorted(cleaned_dict[key].split('|'),\n",
    "                                            key=locale.strxfrm))\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_almaMater(dict_):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get('almaMater')\n",
    "    if key_present:\n",
    "        cleaned_dict['almaMater'] = (\n",
    "             key_present\n",
    "            .replace('|Theoretical Physics', '')\n",
    "            .replace('|Physics', '')\n",
    "        )\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_spouse_children(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get(key)\n",
    "    if key_present and isinstance(key_present, str):\n",
    "        cleaned_dict[key] = (\n",
    "            ' '.join(key_present.split())\n",
    "            .replace(', and ', '|')\n",
    "            .replace(' and ', '|')\n",
    "            .replace(', ', '|')\n",
    "            .replace(' ; 1 child', '')\n",
    "            .replace('|divorced', '')\n",
    "            .replace('divorced|', '')\n",
    "            .replace('sons ', '')\n",
    "        )\n",
    "        cleaned_dict[key] = '|'.join(sorted(cleaned_dict[key].split('|'),\n",
    "                                            key=locale.strxfrm))\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_dates(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    date = dict_.get(key)\n",
    "    if date:\n",
    "        # replace invalid months and days with first day and first month\n",
    "        # the dates only need to be approximately correct as years lived\n",
    "        # will be estimated from them\n",
    "        if isinstance(date, int):  # year only\n",
    "            first_date = str(date) + '-01-01'\n",
    "        else:\n",
    "            first_date = date.split('|')[0]\n",
    "            first_date = first_date.replace('-0-0', '-01-01')\n",
    "            if first_date.endswith('-0'):\n",
    "                first_date = first_date[:-2] + '-1'\n",
    "            if 'c. ' in date:  # deal with circa\n",
    "                first_date = date.replace('c. ', '') + '-01-01'\n",
    "            # ignore dates with a year below 1000 as datetime does not\n",
    "            # handle these\n",
    "            if (first_date.startswith('-') or\n",
    "                int(first_date.split('-')[0]) < 1000):\n",
    "                return cleaned_dict\n",
    "        cleaned_dict[key] = str(\n",
    "            datetime.strptime(first_date, '%Y-%m-%d').date())\n",
    "    return cleaned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_physicists_dataframe(json_lines):\n",
    "    \"\"\"Create physicists dataframe from json_lines data.\n",
    "\n",
    "    Args:\n",
    "        json_lines (list(dict)): JSON lines.\n",
    "    Returns:\n",
    "        pandas.Dataframe: Dataframe containing the physicists data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    data = [create_physicist_data(json_line) for json_line in json_lines]\n",
    "    return pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicists_dataframe = create_physicists_dataframe(json_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that we have the expected number of physicists and take a look at the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(physicists_dataframe) == 1050)\n",
    "with pd.option_context('display.max_rows', 1100):\n",
    "    display(physicists_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now I have the dataframe, I'd like to persist it for later analysis. So I'll write out the contents to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicists_dataframe.to_csv('../data/interim/notable_physicists.csv',\n",
    "                            index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "A few clean up steps are needed:\n",
    "\n",
    "- Convert the notebook to a HTML file with all the output.\n",
    "- Convert the notebook to another notebook with the output removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir html_output --to html 2.0-process-physicists-raw-data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to notebook 2.0-process-physicists-raw-data.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
