{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Physicists Raw Data\n",
    "\n",
    "Now I aim to convert the *JSON lines* format [notable physicists raw data](../data/raw/notable_physicists.jsonl.gz) into an intermediate format *pandas dataframe* that is more convenient to work with. This intermediate format will be close to the final format of the data that I'll be working with for analysis.\n",
    "\n",
    "My goal is to parse the JSON data in order to extract the interesting fields of information such as *name*, *birth date*, *death date*, *citizenships*, *workplaces*, *awards*, *alma mater*, *academic advisors*, *doctoral students*, *DBpedia categories*, *description*, etc. I believe that this information may be useful in helping to predict whether or not a physicist is awarded a Nobel Prize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "An few initialization steps are needed to setup the environment:\n",
    "- The top-level module directory of the repository needs to be added to the system path to enable the loading of python modules.\n",
    "- The locale needs to be set for all categories to the userâ€™s default setting (typically specified in the LANG environment variable) to enable correct sorting of words with accents.\n",
    "- [spacy](https://spacy.io/) is a python package that is used for *natural language processing*. It relies on statistical [language models](https://spacy.io/usage/models) which must be loaded before use. So here I load `en_core_web_sm` which is the default English language model in spacy.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import locale\n",
    "import spacy\n",
    "\n",
    "repo_dir = '../'\n",
    "if repo_dir not in sys.path:\n",
    "    sys.path.append(repo_dir)\n",
    "    \n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import time\n",
    "from datetime import datetime\n",
    "from urllib import parse\n",
    "\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "from src.data.url_utils import DBPEDIA_RESOURCE_URL\n",
    "from src.data.url_utils import get_filename_from_url\n",
    "from src.data.url_utils import get_pathname_from_url\n",
    "from src.data.url_utils import get_redirect_urls\n",
    "from src.data.url_utils import quote_url\n",
    "from src.data.url_utils import urls_progress_bar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the JSON Lines Data\n",
    "\n",
    "First let's extract the JSON lines data and read it into a list so that we can parse it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzip_file = '../data/raw/notable_physicists.jsonl.gz'\n",
    "jsonl_file = gzip_file[:-3]\n",
    "with gzip.open(gzip_file, 'rb') as src, open(jsonl_file, 'wb') as dest:\n",
    "    shutil.copyfileobj(src, dest)\n",
    "\n",
    "json_lines = []\n",
    "with jsonlines.open(jsonl_file, mode='r') as reader:\n",
    "    for json_line in reader:\n",
    "        json_lines.append(json_line)\n",
    "os.remove(jsonl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Fields of Interest\n",
    "\n",
    "Here I define the keys and values that I wish to extract from the JSON lines data and also some that I explicitly wish to exclude. These keys and values are in the form of *semantic URLs* which allows anyone to visit the resource in a web browser and see their meaning. The URLs are in 5 namespaces:\n",
    "\n",
    "- [DBpedia Ontology](https://wiki.dbpedia.org/services-resources/ontology)\n",
    "- DBpedia Property\n",
    "- PURL\n",
    "- W3\n",
    "- FOAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPEDIA_JSON_KEYS = [\n",
    "    # DBpedia ontology\n",
    "    'http://dbpedia.org/ontology/abstract',\n",
    "    'http://dbpedia.org/ontology/academicAdvisor',\n",
    "    'http://dbpedia.org/ontology/almaMater',\n",
    "    'http://dbpedia.org/ontology/award',\n",
    "    'http://dbpedia.org/ontology/birthDate',\n",
    "    'http://dbpedia.org/ontology/birthName',\n",
    "    'http://dbpedia.org/ontology/birthPlace',\n",
    "    'http://dbpedia.org/ontology/child',\n",
    "    'http://dbpedia.org/ontology/citizenship',\n",
    "    'http://dbpedia.org/ontology/deathDate',\n",
    "    'http://dbpedia.org/ontology/deathPlace',\n",
    "    'http://dbpedia.org/ontology/doctoralAdvisor',\n",
    "    'http://dbpedia.org/ontology/doctoralStudent',\n",
    "    'http://dbpedia.org/ontology/education',\n",
    "    'http://dbpedia.org/ontology/field',\n",
    "    'http://dbpedia.org/ontology/influenced',\n",
    "    'http://dbpedia.org/ontology/knownFor',\n",
    "    'http://dbpedia.org/ontology/nationality',\n",
    "    'http://dbpedia.org/ontology/notableStudent',\n",
    "    'http://dbpedia.org/ontology/parent',\n",
    "    'http://dbpedia.org/ontology/residence',\n",
    "    'http://dbpedia.org/ontology/spouse',\n",
    "    'http://dbpedia.org/ontology/thumbnail',\n",
    "    'http://dbpedia.org/ontology/wikiPageID',\n",
    "    'http://dbpedia.org/ontology/wikiPageRevisionID',\n",
    "\n",
    "    # DBpedia property\n",
    "    'http://dbpedia.org/property/academicAdvisor',\n",
    "    'http://dbpedia.org/property/almaMater',\n",
    "    'http://dbpedia.org/property/award',\n",
    "    'http://dbpedia.org/property/birthDate',\n",
    "    'http://dbpedia.org/property/birthName',\n",
    "    'http://dbpedia.org/property/birthPlace',\n",
    "    'http://dbpedia.org/property/child',\n",
    "    'http://dbpedia.org/property/children',\n",
    "    'http://dbpedia.org/property/citizenship',\n",
    "    'http://dbpedia.org/property/deathDate',\n",
    "    'http://dbpedia.org/property/deathPlace',\n",
    "    'http://dbpedia.org/property/doctoralAdvisor',\n",
    "    'http://dbpedia.org/property/doctoralStudent',\n",
    "    'http://dbpedia.org/property/education',\n",
    "    'http://dbpedia.org/property/field',\n",
    "    'http://dbpedia.org/property/influenced',\n",
    "    'http://dbpedia.org/property/knownFor',\n",
    "    'http://dbpedia.org/property/nationality',\n",
    "    'http://dbpedia.org/property/notableStudent',\n",
    "    'http://dbpedia.org/property/parent',\n",
    "    'http://dbpedia.org/property/residence',\n",
    "    'http://dbpedia.org/property/signature',\n",
    "    'http://dbpedia.org/property/spouse',\n",
    "    'http://dbpedia.org/property/thesisTitle',\n",
    "    'http://dbpedia.org/property/thesisUrl',\n",
    "    'http://dbpedia.org/property/thesisYear',\n",
    "    'http://dbpedia.org/property/thumbnail',\n",
    "    'http://dbpedia.org/property/workInstitutions',\n",
    "    'http://dbpedia.org/property/workplaces',\n",
    "\n",
    "    # PURL\n",
    "    'http://purl.org/dc/terms/description',\n",
    "\n",
    "    # W3\n",
    "    'http://www.w3.org/2000/01/rdf-schema#comment',\n",
    "    'http://www.w3.org/ns/prov#wasDerivedFrom',\n",
    "\n",
    "    # FOAF\n",
    "    'http://xmlns.com/foaf/0.1/gender',\n",
    "    'http://xmlns.com/foaf/0.1/givenName',\n",
    "    'http://xmlns.com/foaf/0.1/isPrimaryTopicOf',\n",
    "    'http://xmlns.com/foaf/0.1/name',\n",
    "    'http://xmlns.com/foaf/0.1/surname'\n",
    "]\n",
    "\n",
    "DBPEDIA_JSON_VALUES = [\n",
    "    # DBpedia ontology\n",
    "    'http://dbpedia.org/ontology/influenced',  # target is influencedBy\n",
    "    'http://dbpedia.org/ontology/influencedBy',  # target is influenced\n",
    "\n",
    "    # DBpedia property\n",
    "    'http://dbpedia.org/property/theorized'\n",
    "]\n",
    "\n",
    "DBPEDIA_IGNORE_URLS = [\n",
    "    'http://dbpedia.org/resource/Bachelor_of_Arts',\n",
    "    'http://dbpedia.org/resource/Bachelor_of_Science',\n",
    "    'http://dbpedia.org/resource/Doctor_of_Philosophy',\n",
    "    'http://dbpedia.org/resource/Master_of_Arts',\n",
    "    'http://dbpedia.org/resource/Master_of_Philosophy',\n",
    "    'http://dbpedia.org/resource/Master_of_Science',\n",
    "    'http://dbpedia.org/resource/None',\n",
    "    'http://dbpedia.org/resource/PhD',\n",
    "    'http://dbpedia.org/resource/Ph.D',\n",
    "    'http://dbpedia.org/resource/Ph.D.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a lot of duplicate keys in the ontology and property namespaces (e.g. citizenship). Also, there are a lot of slightly different named keys that sound like they hold similar information (e.g. alma mater and education). Clearly, a set of rules will need to be defined about how to handle these duplicate keys and named variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Physicists Dictionaries\n",
    "\n",
    "Now I parse the JSON lines data to create dictionaries. The following rules apply when creating the dictionaries:\n",
    "\n",
    "1. Values in the DBpedia Ontology namespace takes precedence over those in the DBpedia Property namespace since, as described in section 4.3 of [DBpedia datasets](https://wiki.dbpedia.org/services-resources/datasets/dbpedia-datasets), it contains the cleanest data. Property namespace values are only used when there are no corresponding ontology namespace values.\n",
    "\n",
    "2. Several keys holding similar data are merged together. *Education* is merged into *alma mater*, *work institutions* is merged into *workplaces* and *children* is merged into *child*. *InfluencedBy* and *Influenced* are also merged appropriately.\n",
    "\n",
    "3. Some of the fields (e.g. the *abstract*) is multilingual. In such cases, only the English is extracted.\n",
    "\n",
    "4. The data is messy and some cleanup is done on the fields involving *dates*, *nationality*, *citizenship*, *spouse* and *children*. However, a lot of noise remains in the data. The sources of the noise are: \n",
    "    - Fields containing semantic URLs that are redirected.\n",
    "    - Semi-structured text which contains no information content (e.g. *stopwords*).\n",
    "    - Semi-structured text containing valuable information that is not easy for a machine to understand. Either the text mixes concepts which should be in multiple fields or contains variant representations that essentially refer to the same \"thing\".\n",
    "    \n",
    "I will handle some of these issues now and some of them later prior to and when generating features for machine learning.\n",
    "\n",
    "OK let's now generate the dictionaries and take a look at the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_physicist_data(json_line):\n",
    "    \"\"\"Create physicist data from json_line data.\n",
    "\n",
    "    Args:\n",
    "        json_line (dict): JSON dict.\n",
    "    Returns:\n",
    "        dict: Dictionary of physicist data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # flatten the json\n",
    "    flat_json = json_normalize(json_line)\n",
    "\n",
    "    # find the resource, source and fullName\n",
    "    resource = _find_resource(flat_json)\n",
    "    source = _get_source(resource)\n",
    "    full_name = get_filename_from_url(resource).replace('_', ' ')\n",
    "\n",
    "    # construct the dictionary\n",
    "    dict_ = {'resource': resource, 'source': source, 'fullName': full_name,\n",
    "             **_process_json_keys(resource, flat_json),\n",
    "             **_process_json_values(resource, flat_json),\n",
    "             **_process_categories(flat_json)}\n",
    "\n",
    "    # merge keys\n",
    "    dict_ = _merge_keys(dict_)\n",
    "\n",
    "    # clean fields\n",
    "    dict_ = _clean_fields(dict_)\n",
    "\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _process_json_keys(resource, flat_json):\n",
    "    dict_ = {}\n",
    "\n",
    "    # loop over the keys\n",
    "    for json_key in DBPEDIA_JSON_KEYS:\n",
    "        flat_json_key = resource + '.' + json_key\n",
    "        filename = get_filename_from_url(json_key)\n",
    "        dict_key = filename[filename.rfind('#') + 1:]  # take fragment\n",
    "        # sanitize for later merging in _merge_influences\n",
    "        if dict_key == 'influenced':\n",
    "            dict_key = 'influenced_'\n",
    "        if flat_json_key not in flat_json:\n",
    "            continue\n",
    "\n",
    "        # loop and get the values\n",
    "        for list_ in flat_json[flat_json_key].values:\n",
    "            val_list = []\n",
    "            for val in list_:\n",
    "                if not _val_is_english(val):\n",
    "                    continue\n",
    "\n",
    "                if isinstance(val['value'], int):\n",
    "                    value = val['value']\n",
    "                else:  # str\n",
    "                    value = val['value'].lstrip('* ')\n",
    "\n",
    "                if value in DBPEDIA_IGNORE_URLS:\n",
    "                    continue\n",
    "\n",
    "                if not _value_has_information_content(value):\n",
    "                    continue\n",
    "\n",
    "                if isinstance(value, str) and value.startswith(\n",
    "                    'http://wikidata.dbpedia.org/'):\n",
    "                        continue\n",
    "                val_list.append(value)\n",
    "\n",
    "            if not val_list:\n",
    "                continue\n",
    "            elif len(val_list) == 1 and dict_key not in dict_:\n",
    "                dict_[dict_key] = val_list[0]\n",
    "            else:\n",
    "                if not all(isinstance(v, str) for v in val_list):\n",
    "                    # multiple values must all be of 'str' type\n",
    "                    # (some child and spouse have int values)\n",
    "                    val_list = [v for v in val_list if\n",
    "                                not isinstance(v, int)]\n",
    "                if dict_key not in dict_:\n",
    "                    val_list.sort(key=locale.strxfrm)\n",
    "                    dict_[dict_key] = '|'.join(val_list)\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _process_json_values(resource, flat_json):\n",
    "    dict_ = {}\n",
    "\n",
    "    # loop over the values\n",
    "    for json_value in DBPEDIA_JSON_VALUES:\n",
    "        # loop and get the keys\n",
    "        key_list = []\n",
    "        for json_key in flat_json:\n",
    "            sep_position = json_key.rfind('.http')\n",
    "            key, val = json_key[:sep_position], json_key[sep_position + 1:]\n",
    "            # only consider keys other than the resource\n",
    "            if json_value == val and not key == resource:\n",
    "                dict_key = key\n",
    "                key_list.append(dict_key)\n",
    "                dict_val = get_filename_from_url(val).replace('_', ' ')\n",
    "        if not key_list:\n",
    "            continue\n",
    "        elif len(key_list) == 1:\n",
    "            dict_[dict_val] = key_list[0]\n",
    "        else:\n",
    "            key_list.sort(key=locale.strxfrm)\n",
    "            dict_[dict_val] = '|'.join(key_list)\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _process_categories(flat_json):\n",
    "    dict_ = {}\n",
    "\n",
    "    val_list = []\n",
    "    for list_ in flat_json.values:\n",
    "        for item in list_:\n",
    "            for val in item:\n",
    "                value = val['value']\n",
    "                if (not isinstance(value, str) or not value.startswith(\n",
    "                        DBPEDIA_RESOURCE_URL + 'Category:')):\n",
    "                    continue\n",
    "                val_list.append(value)\n",
    "    if val_list:\n",
    "        val_list.sort(key=locale.strxfrm)\n",
    "        dict_['categories'] = '|'.join(val_list)\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _merge_keys(dict_):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    merged_dict = _merge_influences(merged_dict)\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'almaMater', 'education')\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'workplaces', 'workInstitutions')\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'child', 'children')\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _clean_fields(dict_):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    cleaned_dict = _clean_dates(cleaned_dict, 'birthDate')\n",
    "    cleaned_dict = _clean_dates(cleaned_dict, 'deathDate')\n",
    "    cleaned_dict = _clean_citizenship_nationality(cleaned_dict,\n",
    "                                                  'citizenship')\n",
    "    cleaned_dict = _clean_citizenship_nationality(cleaned_dict,\n",
    "                                                  'nationality')\n",
    "    cleaned_dict = _clean_spouse_child(cleaned_dict, 'spouse')\n",
    "    cleaned_dict = _clean_spouse_child(cleaned_dict, 'child')\n",
    "    cleaned_dict = _clean_almaMater(cleaned_dict)\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _find_resource(flat_json):\n",
    "    # resource is the most common value\n",
    "    vals = [val.split('.h')[0] for val in flat_json.columns.values]\n",
    "    return collections.Counter(vals).most_common(1)[0][0]\n",
    "\n",
    "\n",
    "def _get_source(resource):\n",
    "    parsed_url = parse.urlparse(resource)\n",
    "    filename = get_filename_from_url(resource)\n",
    "    return ('{uri.scheme}://{uri.netloc}/'.format(uri=parsed_url) +\n",
    "            'data/' + filename + '.json')\n",
    "\n",
    "\n",
    "\n",
    "def _val_is_english(val):\n",
    "    language = val.get('lang')\n",
    "    if not language or language == 'en':\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def _value_has_information_content(value):\n",
    "    if not isinstance(value, str):\n",
    "        return True\n",
    "    if (not value or\n",
    "            value == '*' or\n",
    "            value.startswith('* \\n') or\n",
    "            value.endswith('\\n*') or\n",
    "            value.startswith('--') or\n",
    "            '_family' in value or  # e.g. child contains this\n",
    "            value in spacy.lang.en.STOP_WORDS):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def _merge_influences(dict_):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    influenced = merged_dict.get('influenced')\n",
    "    influenced_ = merged_dict.get('influenced_')\n",
    "\n",
    "    if influenced_:\n",
    "        if influencedBy:\n",
    "            merged_dict['influencedBy'] += '|' + influenced_\n",
    "        else:\n",
    "            merged_dict['influencedBy'] = influenced_\n",
    "        del merged_dict['influenced_']\n",
    "\n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    if influencedBy and influenced:\n",
    "        merged_dict['influencedBy'], merged_dict['influenced'] = \\\n",
    "            merged_dict['influenced'], merged_dict['influencedBy']\n",
    "    elif influencedBy and not influenced:\n",
    "        merged_dict['influenced'] = influencedBy\n",
    "        del merged_dict['influencedBy']\n",
    "    elif not influencedBy and influenced:\n",
    "        merged_dict['influencedBy'] = influenced\n",
    "        del merged_dict['influenced']\n",
    "    \n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    influenced = merged_dict.get('influenced')\n",
    "    if influencedBy:\n",
    "        merged_dict['influencedBy'] = '|'.join(sorted(\n",
    "            merged_dict['influencedBy'].split('|'), key=locale.strxfrm))\n",
    "    if influenced:\n",
    "        merged_dict['influenced'] = '|'.join(sorted(\n",
    "            merged_dict['influenced'].split('|'), key=locale.strxfrm))\n",
    "\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _merge_right_key_into_left_key(dict_, left_key, right_key):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    left = merged_dict.get(left_key)\n",
    "    right = merged_dict.get(right_key)\n",
    "\n",
    "    if not left and right:\n",
    "        merged_dict[left_key] = right\n",
    "    merged_dict.pop(right_key, None)\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _clean_citizenship_nationality(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get(key)\n",
    "    if (key_present and not\n",
    "        key_present == 'United Kingdom of Great Britain and Ireland'):\n",
    "        cleaned_dict[key] = (\n",
    "            key_present\n",
    "            .replace(' / ', '|')\n",
    "            .replace(' , ', '|')\n",
    "            .replace(', ', '|')\n",
    "            .replace(' and then ', '|')\n",
    "            .replace(' and ', '|')\n",
    "            .replace('Citizenship in the ', '')\n",
    "            .replace('Citizen of ', '')\n",
    "            .replace(' citizen', '')\n",
    "            .replace(' law', '')\n",
    "            .replace('List of ', '')\n",
    "            .replace('Nationality law of the ', '')\n",
    "            .replace('Nationality of the ', '')\n",
    "            .replace(' nationality law', '')\n",
    "            .replace(' nationality', '')\n",
    "            .replace('People of ', '')\n",
    "            .replace(' people', '')\n",
    "            .replace(' subject', '')\n",
    "        )\n",
    "        cleaned_dict[key] = '|'.join(sorted(cleaned_dict[key].split('|'),\n",
    "                                            key=locale.strxfrm))\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_almaMater(dict_):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get('almaMater')\n",
    "    if key_present:\n",
    "        cleaned_dict['almaMater'] = (\n",
    "             key_present\n",
    "            .replace('|Theoretical Physics', '')\n",
    "            .replace('|Physics', '')\n",
    "        )\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_spouse_child(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get(key)\n",
    "    if key_present and isinstance(key_present, str):\n",
    "        cleaned_dict[key] = (\n",
    "            key_present\n",
    "            .replace(', and ', '|')\n",
    "            .replace(' and ', '|')\n",
    "            .replace(', ', '|')\n",
    "            .replace(' ; 1 child', '')\n",
    "            .replace('|divorced', '')\n",
    "            .replace('divorced|', '')\n",
    "            .replace('sons ', '')\n",
    "        )\n",
    "        cleaned_dict[key] = '|'.join(sorted(cleaned_dict[key].split('|'),\n",
    "                                            key=locale.strxfrm))\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_dates(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    date = dict_.get(key)\n",
    "    if date:\n",
    "        # replace invalid months and days with first day and first month\n",
    "        # the dates only need to be approximately correct as years lived\n",
    "        # will be estimated from them\n",
    "        if isinstance(date, int):  # year only\n",
    "            first_date = str(date) + '-01-01'\n",
    "        else:\n",
    "            first_date = date.split('|')[0]\n",
    "            first_date = first_date.replace('-0-0', '-01-01')\n",
    "            if first_date.endswith('-0'):\n",
    "                first_date = first_date[:-2] + '-1'\n",
    "            if 'c. ' in date:  # deal with circa\n",
    "                first_date = date.replace('c. ', '') + '-01-01'\n",
    "            # ignore dates with a year below 1000 as datetime does not\n",
    "            # handle these\n",
    "            if (first_date.startswith('-') or\n",
    "                int(first_date.split('-')[0]) < 1000):\n",
    "                return cleaned_dict\n",
    "        cleaned_dict[key] = str(\n",
    "            datetime.strptime(first_date, '%Y-%m-%d').date())\n",
    "    return cleaned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [create_physicist_data(json_line) for json_line in json_lines]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that many of the values in the dictionary contain lists of semantic URLs which are each meant to refer to a unique \"thing\" such as a person or a place. If the URL is redirected, it is important to know where it is redirected to so that identical \"things\" in fact resolve to the same URL. It is very important to do this so that the machine learning models have clean data in order to differentiate signal from noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Redirects in the Physicists Dictionaries\n",
    "\n",
    "In order to impute redirects in the dictionaries I need to perform the following steps:\n",
    "\n",
    "1. *Parse the dictionaries to obtain a list of all URLs.* I restrict this to the redirect keys in the list that you see below since these are the only fields of interest involving URLs that features will be extracted from.\n",
    "2. *Submit HTTP requests to fetch the URLs and determine their redirect URLs.* A cache is kept mapping the URLs to the redirect URLs and as a consequence a HTTP request is only made to fetch a URL if the URL is not found in the cache. This greatly helps with performance.\n",
    "3. *Replace the URLs in the dictionaries with the redirect URLs.* In fact I use just the filename since the paths are identical for every URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REDIRECT_KEYS = ['academicAdvisor', 'almaMater', 'award', 'birthPlace',\n",
    "                 'categories', 'child', 'citizenship', 'deathPlace',\n",
    "                 'doctoralAdvisor', 'doctoralStudent', 'field', 'influenced',\n",
    "                 'influencedBy', 'knownFor', 'nationality', 'notableStudent',\n",
    "                 'parent', 'residence', 'spouse', 'theorized', 'workplaces']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contruct_urls(data):\n",
    "    \"\"\"Construct DBpedia resource URLs from physicists data.\n",
    "\n",
    "    Args:\n",
    "        physicists (dict): Dict containing physicists data.\n",
    "\n",
    "    Returns:\n",
    "        list of `str`: List of URLs.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    urls_to_check = set()\n",
    "    for key in REDIRECT_KEYS:\n",
    "         for datum in data:\n",
    "            text = datum.get(key)\n",
    "            if not text or isinstance(text, int):\n",
    "                continue\n",
    "            texts = text.split('|')\n",
    "            urls = (item for item in texts if item.startswith(\n",
    "                DBPEDIA_RESOURCE_URL))\n",
    "            for url in urls:\n",
    "                quoted_url = quote_url(url)\n",
    "                urls_to_check.add(quoted_url)\n",
    "    \n",
    "    urls_to_check = list(urls_to_check)\n",
    "    return urls_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_check = contruct_urls(data)\n",
    "len(urls_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_cache_path = '../data/raw/dbpedia-redirects.csv'\n",
    "redirects = get_redirect_urls(\n",
    "    urls_to_check, url_cache_path=url_cache_path, max_workers=20,\n",
    "    timeout=30, progress_bar=urls_progress_bar(len(urls_to_check)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls_to_check) - len(redirects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that only one of the redirected URLs was not found. It is safe to ignore and proceed as this is not important for feature extraction.\n",
    "\n",
    "Now I sort and persist the URL cache to disk in case any new URLs are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_redirects = pd.DataFrame(\n",
    "    sorted(list(zip(redirects.keys(), redirects.values())),\n",
    "           key=lambda x: locale.strxfrm(x[0])), columns=['url', 'redirect_url'])\n",
    "dbpedia_redirects.to_csv(url_cache_path, index=False)\n",
    "dbpedia_redirects.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I replace the URLs in the dictionaries with the redirect URLs making sure to just use the filename since the paths are identical for every URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_redirects(data, redirects):\n",
    "    \"\"\"Impute the filenames from redirected URLs in the physicists data.\n",
    "\n",
    "    Args:\n",
    "        data (list of `dict`): List of dicts containing physicists\n",
    "            data.\n",
    "        redirects (dict): The DBpedia redirected URLs. The key is\n",
    "            original URL and the value is the redirected URL.\n",
    "\n",
    "    Returns:\n",
    "        list of `dict`: List of dicts containing physicists data.\n",
    "\n",
    "        Identical to `data` except that it contains the filenames\n",
    "        from the redirected URLs.\n",
    "    \"\"\"\n",
    "    \n",
    "    imputed_data = data.copy()\n",
    "    \n",
    "    for key in REDIRECT_KEYS:\n",
    "        for datum in data:\n",
    "            text = datum.get(key)\n",
    "            if not text or isinstance(text, int):\n",
    "                continue\n",
    "\n",
    "            # split up fields with these symbols\n",
    "            text = text.replace(' \\n* ', '|')\n",
    "            text = text.replace('\\n* ', '|')\n",
    "            text = text.replace(' \\n', '|')\n",
    "            texts = text.split('|')\n",
    "            \n",
    "            impute_texts = set()\n",
    "            for text in texts:\n",
    "                if text.startswith(DBPEDIA_RESOURCE_URL):\n",
    "                    if text in redirects:\n",
    "                        name = get_filename_from_url(\n",
    "                            redirects[text]).replace('_', ' ')\n",
    "                    else:\n",
    "                        name = get_filename_from_url(\n",
    "                            text).replace('_', ' ')\n",
    "                else:\n",
    "                    name = text\n",
    "                if name.startswith('Category:'):\n",
    "                    name = name.replace('Category:', '')\n",
    "                impute_texts.add(name)\n",
    "            impute_texts = list(impute_texts)\n",
    "            impute_texts.sort(key=locale.strxfrm)\n",
    "            datum[key] = '|'.join(impute_texts) \n",
    "   \n",
    "    return imputed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = impute_redirects(data, redirects)\n",
    "imputed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Physicists Dataframe\n",
    "\n",
    "Now I use the dictionaries of imputed data to create a dataframe. Let's confirm that it contains the expected number of physicists and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicists = pd.DataFrame(imputed_data)\n",
    "assert(len(physicists) == 1049)\n",
    "with pd.option_context('display.max_rows', 1100):\n",
    "    display(physicists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now I have the dataframe, I'd like to persist it for later analysis. So I'll write out the contents to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicists.to_csv('../data/interim/notable_physicists.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "A clean up step is needed:\n",
    "\n",
    "- Remove the top-level module directory of the repository from the system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.remove(repo_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
