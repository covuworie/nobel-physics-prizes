{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Physicists Raw Data\n",
    "\n",
    "Now I aim to convert the *JSON lines* format [physicists raw data](../data/raw/physicists.jsonl.gz) into an intermediate format *pandas dataframe* that is more convenient to work with. This intermediate format will be close to the final format of the data that I'll be working with for analysis.\n",
    "\n",
    "My goal is to parse the JSON data in order to extract the interesting fields of information such as *name*, *birth date*, *death date*, *citizenships*, *workplaces*, *awards*, *alma mater*, *academic advisors*, *doctoral students*, *DBpedia categories*, *description*, etc. I believe that this information may be useful in helping to predict whether or not a physicist is awarded a Nobel Prize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "An initialization step is needed to setup the environment:\n",
    "- The locale needs to be set for all categories to the userâ€™s default setting (typically specified in the LANG environment variable) to enable correct sorting of words with accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "\n",
    "locale.setlocale(locale.LC_ALL, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "from src.data.dbpedia_utils import construct_resource_urls\n",
    "from src.data.dbpedia_utils import find_resource_url\n",
    "from src.data.dbpedia_utils import get_source_url\n",
    "from src.data.dbpedia_utils import impute_redirect_filenames\n",
    "from src.data.dbpedia_utils import json_categories_to_dict\n",
    "from src.data.dbpedia_utils import json_keys_to_dict\n",
    "from src.data.dbpedia_utils import json_values_to_dict\n",
    "from src.data.dbpedia_utils import PHYSICISTS_IGNORE_REDIRECT_KEYS\n",
    "from src.data.dbpedia_utils import PHYSICISTS_IMPUTE_KEYS\n",
    "from src.data.jsonl_utils import read_jsonl\n",
    "from src.data.progress_bar import progress_bar\n",
    "from src.data.url_utils import get_filename_from_url\n",
    "from src.data.url_utils import get_redirect_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the JSON Lines Data\n",
    "\n",
    "First let's read the JSON lines data into a list so that we can parse it later and take a look at the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lines = read_jsonl('../data/raw/physicists.jsonl.gz')\n",
    "json_lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Fields of Interest\n",
    "\n",
    "Here I define the keys and values that I wish to extract from the JSON lines data and also some that I explicitly wish to exclude. These keys and values are in the form of *semantic URLs* which allows anyone to visit the resource in a web browser and see their meaning. The URLs are in 5 namespaces:\n",
    "\n",
    "- [DBpedia Ontology](https://wiki.dbpedia.org/services-resources/ontology)\n",
    "- DBpedia Property\n",
    "- PURL\n",
    "- W3\n",
    "- FOAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPEDIA_JSON_KEYS = [\n",
    "    # DBpedia ontology\n",
    "    'http://dbpedia.org/ontology/abstract',\n",
    "    'http://dbpedia.org/ontology/academicAdvisor',\n",
    "    'http://dbpedia.org/ontology/almaMater',\n",
    "    'http://dbpedia.org/ontology/award',\n",
    "    'http://dbpedia.org/ontology/birthDate',\n",
    "    'http://dbpedia.org/ontology/birthName',\n",
    "    'http://dbpedia.org/ontology/birthPlace',\n",
    "    'http://dbpedia.org/ontology/child',\n",
    "    'http://dbpedia.org/ontology/citizenship',\n",
    "    'http://dbpedia.org/ontology/deathDate',\n",
    "    'http://dbpedia.org/ontology/deathPlace',\n",
    "    'http://dbpedia.org/ontology/doctoralAdvisor',\n",
    "    'http://dbpedia.org/ontology/doctoralStudent',\n",
    "    'http://dbpedia.org/ontology/education',\n",
    "    'http://dbpedia.org/ontology/field',\n",
    "    'http://dbpedia.org/ontology/influenced',\n",
    "    'http://dbpedia.org/ontology/knownFor',\n",
    "    'http://dbpedia.org/ontology/nationality',\n",
    "    'http://dbpedia.org/ontology/notableStudent',\n",
    "    'http://dbpedia.org/ontology/parent',\n",
    "    'http://dbpedia.org/ontology/residence',\n",
    "    'http://dbpedia.org/ontology/spouse',\n",
    "    'http://dbpedia.org/ontology/thumbnail',\n",
    "    'http://dbpedia.org/ontology/wikiPageID',\n",
    "    'http://dbpedia.org/ontology/wikiPageRevisionID',\n",
    "\n",
    "    # DBpedia property\n",
    "    'http://dbpedia.org/property/academicAdvisor',\n",
    "    'http://dbpedia.org/property/almaMater',\n",
    "    'http://dbpedia.org/property/award',\n",
    "    'http://dbpedia.org/property/birthDate',\n",
    "    'http://dbpedia.org/property/birthName',\n",
    "    'http://dbpedia.org/property/birthPlace',\n",
    "    'http://dbpedia.org/property/child',\n",
    "    'http://dbpedia.org/property/children',\n",
    "    'http://dbpedia.org/property/citizenship',\n",
    "    'http://dbpedia.org/property/deathDate',\n",
    "    'http://dbpedia.org/property/deathPlace',\n",
    "    'http://dbpedia.org/property/doctoralAdvisor',\n",
    "    'http://dbpedia.org/property/doctoralStudent',\n",
    "    'http://dbpedia.org/property/education',\n",
    "    'http://dbpedia.org/property/field',\n",
    "    'http://dbpedia.org/property/influenced',\n",
    "    'http://dbpedia.org/property/knownFor',\n",
    "    'http://dbpedia.org/property/nationality',\n",
    "    'http://dbpedia.org/property/notableStudent',\n",
    "    'http://dbpedia.org/property/parent',\n",
    "    'http://dbpedia.org/property/residence',\n",
    "    'http://dbpedia.org/property/signature',\n",
    "    'http://dbpedia.org/property/spouse',\n",
    "    'http://dbpedia.org/property/thesisTitle',\n",
    "    'http://dbpedia.org/property/thesisUrl',\n",
    "    'http://dbpedia.org/property/thesisYear',\n",
    "    'http://dbpedia.org/property/thumbnail',\n",
    "    'http://dbpedia.org/property/workInstitutions',\n",
    "    'http://dbpedia.org/property/workplaces',\n",
    "\n",
    "    # PURL\n",
    "    'http://purl.org/dc/terms/description',\n",
    "\n",
    "    # W3\n",
    "    'http://www.w3.org/2000/01/rdf-schema#comment',\n",
    "    'http://www.w3.org/ns/prov#wasDerivedFrom',\n",
    "\n",
    "    # FOAF\n",
    "    'http://xmlns.com/foaf/0.1/gender',\n",
    "    'http://xmlns.com/foaf/0.1/givenName',\n",
    "    'http://xmlns.com/foaf/0.1/isPrimaryTopicOf',\n",
    "    'http://xmlns.com/foaf/0.1/name',\n",
    "    'http://xmlns.com/foaf/0.1/surname'\n",
    "]\n",
    "\n",
    "DBPEDIA_JSON_VALUES = [\n",
    "    # DBpedia ontology\n",
    "    'http://dbpedia.org/ontology/influenced',  # target is influencedBy\n",
    "    'http://dbpedia.org/ontology/influencedBy',  # target is influenced\n",
    "\n",
    "    # DBpedia property\n",
    "    'http://dbpedia.org/property/theorized'\n",
    "]\n",
    "\n",
    "DBPEDIA_IGNORE_URLS = [\n",
    "    'http://dbpedia.org/resource/Bachelor_of_Arts',\n",
    "    'http://dbpedia.org/resource/Bachelor_of_Science',\n",
    "    'http://dbpedia.org/resource/Doctor_of_Philosophy',\n",
    "    'http://dbpedia.org/resource/Master_of_Arts',\n",
    "    'http://dbpedia.org/resource/Master_of_Philosophy',\n",
    "    'http://dbpedia.org/resource/Master_of_Science',\n",
    "    'http://dbpedia.org/resource/None',\n",
    "    'http://dbpedia.org/resource/PhD',\n",
    "    'http://dbpedia.org/resource/Ph.D',\n",
    "    'http://dbpedia.org/resource/Ph.D.'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there are a lot of duplicate keys in the ontology and property namespaces (e.g. citizenship). Also, there are a lot of slightly different named keys that sound like they hold similar information (e.g. alma mater and education). Clearly, a set of rules will need to be defined about how to handle these duplicate keys and named variants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Physicists Dictionaries\n",
    "\n",
    "Now I parse the JSON lines data to create dictionaries. The following rules apply when creating the dictionaries:\n",
    "\n",
    "1. Values in the DBpedia Ontology namespace takes precedence over those in the DBpedia Property namespace since, as described in section 4.3 of [DBpedia datasets](https://wiki.dbpedia.org/services-resources/datasets/dbpedia-datasets), it contains the cleanest data. Property namespace values are only used when there are no corresponding ontology namespace values.\n",
    "\n",
    "2. Several keys holding similar data are merged together. *Education* is merged into *alma mater*, *work institutions* is merged into *workplaces* and *children* is merged into *child*. *InfluencedBy* and *Influenced* are also merged appropriately.\n",
    "\n",
    "3. Some of the fields (e.g. the *abstract*) is multilingual. In such cases, only the English is extracted.\n",
    "\n",
    "4. The data is messy and some cleanup is done on the fields involving *dates*, *nationality*, *citizenship*, *spouse* and *children*. However, a lot of noise remains in the data. The sources of the noise are: \n",
    "    - Fields containing semantic URLs that are redirected.\n",
    "    - Semi-structured text which contains no information content (e.g. *stopwords*).\n",
    "    - Semi-structured text containing valuable information that is not easy for a machine to understand. Either the text mixes concepts which should be in multiple fields or contains variant representations that essentially refer to the same \"thing\".\n",
    "    \n",
    "I will handle some of these issues now and some of them later prior to and when generating features for machine learning.\n",
    "\n",
    "OK let's now generate the dictionaries and take a look at the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_physicist_data(json_line):\n",
    "    \"\"\"Create physicist data from json_line data.\n",
    "\n",
    "    Args:\n",
    "        json_line (dict): JSON dict.\n",
    "    Returns:\n",
    "        dict: Dictionary of physicist data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # flatten the json\n",
    "    flat_json = json_normalize(json_line)\n",
    "\n",
    "    # find the resource, source and fullName\n",
    "    resource_url = find_resource_url(flat_json)\n",
    "    source_url = get_source_url(resource_url)\n",
    "    full_name = get_filename_from_url(resource_url).replace('_', ' ')\n",
    "\n",
    "    # construct the dictionary\n",
    "    dict_ = {'resource': resource_url, 'source': source_url, 'fullName': full_name,\n",
    "             **json_keys_to_dict(resource_url, flat_json,\n",
    "                                 DBPEDIA_JSON_KEYS,\n",
    "                                 ignore_urls=DBPEDIA_IGNORE_URLS),\n",
    "             **json_values_to_dict(resource_url, flat_json, DBPEDIA_JSON_VALUES),\n",
    "             **json_categories_to_dict(flat_json)}\n",
    "\n",
    "    # merge keys\n",
    "    dict_ = _merge_keys(dict_)\n",
    "\n",
    "    # clean fields\n",
    "    dict_ = _clean_fields(dict_)\n",
    "\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _merge_keys(dict_):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    merged_dict = _merge_influences(merged_dict)\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'almaMater', 'education')\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'workplaces', 'workInstitutions')\n",
    "    merged_dict = _merge_right_key_into_left_key(\n",
    "        merged_dict, 'child', 'children')\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _clean_fields(dict_):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    cleaned_dict = _clean_dates(cleaned_dict, 'birthDate')\n",
    "    cleaned_dict = _clean_dates(cleaned_dict, 'deathDate')\n",
    "    cleaned_dict = _clean_citizenship_nationality(cleaned_dict,\n",
    "                                                  'citizenship')\n",
    "    cleaned_dict = _clean_citizenship_nationality(cleaned_dict,\n",
    "                                                  'nationality')\n",
    "    cleaned_dict = _clean_spouse_child(cleaned_dict, 'spouse')\n",
    "    cleaned_dict = _clean_spouse_child(cleaned_dict, 'child')\n",
    "    cleaned_dict = _clean_almaMater(cleaned_dict)\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _merge_influences(dict_):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    influenced = merged_dict.get('influenced')\n",
    "    influenced_ = merged_dict.get('influenced_')\n",
    "\n",
    "    if influenced_:\n",
    "        if influencedBy:\n",
    "            merged_dict['influencedBy'] += '|' + influenced_\n",
    "        else:\n",
    "            merged_dict['influencedBy'] = influenced_\n",
    "        del merged_dict['influenced_']\n",
    "\n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    if influencedBy and influenced:\n",
    "        merged_dict['influencedBy'], merged_dict['influenced'] = \\\n",
    "            merged_dict['influenced'], merged_dict['influencedBy']\n",
    "    elif influencedBy and not influenced:\n",
    "        merged_dict['influenced'] = influencedBy\n",
    "        del merged_dict['influencedBy']\n",
    "    elif not influencedBy and influenced:\n",
    "        merged_dict['influencedBy'] = influenced\n",
    "        del merged_dict['influenced']\n",
    "    \n",
    "    influencedBy = merged_dict.get('influencedBy')\n",
    "    influenced = merged_dict.get('influenced')\n",
    "    if influencedBy:\n",
    "        merged_dict['influencedBy'] = '|'.join(sorted(\n",
    "            merged_dict['influencedBy'].split('|'), key=locale.strxfrm))\n",
    "    if influenced:\n",
    "        merged_dict['influenced'] = '|'.join(sorted(\n",
    "            merged_dict['influenced'].split('|'), key=locale.strxfrm))\n",
    "\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _merge_right_key_into_left_key(dict_, left_key, right_key):\n",
    "    merged_dict = dict_.copy()\n",
    "\n",
    "    left = merged_dict.get(left_key)\n",
    "    right = merged_dict.get(right_key)\n",
    "\n",
    "    if not left and right:\n",
    "        merged_dict[left_key] = right\n",
    "    merged_dict.pop(right_key, None)\n",
    "    return merged_dict\n",
    "\n",
    "\n",
    "def _clean_citizenship_nationality(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get(key)\n",
    "    if (key_present and not\n",
    "        key_present == 'United Kingdom of Great Britain and Ireland'):\n",
    "        cleaned_dict[key] = (\n",
    "            key_present\n",
    "            .replace(' / ', '|')\n",
    "            .replace(' , ', '|')\n",
    "            .replace(', ', '|')\n",
    "            .replace(' and then ', '|')\n",
    "            .replace(' and ', '|')\n",
    "            .replace('Citizenship in the ', '')\n",
    "            .replace('Citizen of ', '')\n",
    "            .replace(' citizen', '')\n",
    "            .replace(' law', '')\n",
    "            .replace('List of ', '')\n",
    "            .replace('Nationality law of the ', '')\n",
    "            .replace('Nationality of the ', '')\n",
    "            .replace(' nationality law', '')\n",
    "            .replace(' nationality', '')\n",
    "            .replace('People of ', '')\n",
    "            .replace(' people', '')\n",
    "            .replace(' subject', '')\n",
    "        )\n",
    "        cleaned_dict[key] = '|'.join(sorted(cleaned_dict[key].split('|'),\n",
    "                                            key=locale.strxfrm))\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_almaMater(dict_):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get('almaMater')\n",
    "    if key_present:\n",
    "        cleaned_dict['almaMater'] = (\n",
    "             key_present\n",
    "            .replace('|Theoretical Physics', '')\n",
    "            .replace('|Physics', '')\n",
    "        )\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_spouse_child(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key_present = cleaned_dict.get(key)\n",
    "    if key_present and isinstance(key_present, str):\n",
    "        cleaned_dict[key] = (\n",
    "            key_present\n",
    "            .replace(', and ', '|')\n",
    "            .replace(' and ', '|')\n",
    "            .replace(', ', '|')\n",
    "            .replace(' ; 1 child', '')\n",
    "            .replace('|divorced', '')\n",
    "            .replace('divorced|', '')\n",
    "            .replace('sons ', '')\n",
    "        )\n",
    "        cleaned_dict[key] = '|'.join(sorted(cleaned_dict[key].split('|'),\n",
    "                                            key=locale.strxfrm))\n",
    "    return cleaned_dict\n",
    "\n",
    "\n",
    "def _clean_dates(dict_, key):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    date = dict_.get(key)\n",
    "    if date:\n",
    "        # replace invalid months and days with first day and first month\n",
    "        # the dates only need to be approximately correct as years lived\n",
    "        # will be estimated from them\n",
    "        if isinstance(date, int):  # year only\n",
    "            first_date = str(date) + '-01-01'\n",
    "        else:\n",
    "            first_date = date.split('|')[0]\n",
    "            first_date = first_date.replace('-0-0', '-01-01')\n",
    "            if first_date.endswith('-0'):\n",
    "                first_date = first_date[:-2] + '-1'\n",
    "            if 'c. ' in date:  # deal with circa\n",
    "                first_date = date.replace('c. ', '') + '-01-01'\n",
    "            # ignore dates with a year below 1000 as datetime does not\n",
    "            # handle these\n",
    "            if (first_date.startswith('-') or\n",
    "                int(first_date.split('-')[0]) < 1000):\n",
    "                return cleaned_dict\n",
    "        cleaned_dict[key] = str(\n",
    "            datetime.strptime(first_date, '%Y-%m-%d').date())\n",
    "    return cleaned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [create_physicist_data(json_line) for json_line in json_lines]\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that many of the values in the dictionary contain lists of semantic URLs which are each meant to refer to a unique \"thing\" such as a person or a place. If the URL is redirected, it is important to know where it is redirected to so that identical \"things\" in fact resolve to the same URL. It is very important to do this so that the machine learning models have clean data in order to differentiate signal from noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Redirects in the Physicists Dictionaries\n",
    "\n",
    "In order to impute redirects in the dictionaries I need to perform the following steps:\n",
    "\n",
    "1. *Parse the dictionaries to obtain a list of all URLs*. These are either resource URLs or URLs constructed in an *ad hoc* manner from the free form text in the hope that they resolve to a genuine resource URL. The aim is to resolve as many items as possible in the fields to semantic URLs. I restrict URL selection and construction to the following impute keys: `academicAdvisor`, `almaMater`, `award`, `birthPlace`, `categories`, `child`, `citizenship`, `deathPlace`, `doctoralAdvisor`, `doctoralStudent`, `field`, `influenced`, `influencedBy`, `knownFor`, `nationality`, `notableStudent`, `parent`, `residence`, `spouse`, `theorized`, `workplaces` as these are fields of interest involving URLs that features may be extracted from.\n",
    "2. *Submit HTTP requests to fetch the URLs and determine their redirect URLs.* A cache is kept mapping the URLs to the redirect URLs and as a consequence a HTTP request is only made to fetch a URL if it is not found in the cache. This greatly helps with performance.\n",
    "3. *Replace the URLs in the dictionaries with the redirect URLs.* In fact I use just the filename since the paths are identical for every URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_check = construct_resource_urls(data, PHYSICISTS_IMPUTE_KEYS)\n",
    "len(urls_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_cache_path = '../data/raw/dbpedia-redirects.csv'\n",
    "redirects = get_redirect_urls(\n",
    "    urls_to_check, url_cache_path=url_cache_path, max_workers=20,\n",
    "    timeout=30, progress_bar=progress_bar(len(urls_to_check)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(redirects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that many of the requested URLs were not found. This is to be expected as there is free form text in the fields that do not map to a semantic URL in DBpedia. However, my *ad hoc* approach of constructing URLs from free form texts does find legitimate URLs in many instances.\n",
    "\n",
    "Now I sort and persist the URL cache to disk in case any new URLs are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_redirects = pd.DataFrame(\n",
    "    sorted(list(zip(redirects.keys(), redirects.values())),\n",
    "           key=lambda x: locale.strxfrm(x[0])), columns=['url', 'redirect_url'])\n",
    "dbpedia_redirects.to_csv(url_cache_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I replace the URLs in the dictionaries with the redirect URLs making sure to just use the filename since the paths are identical for every URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = impute_redirect_filenames(data, PHYSICISTS_IMPUTE_KEYS, redirects,\n",
    "                                         PHYSICISTS_IGNORE_REDIRECT_KEYS)\n",
    "imputed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Physicists Dataframe\n",
    "\n",
    "Now I use the dictionaries of imputed data to create a dataframe. Let's confirm that it contains the expected number of physicists and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicists = pd.DataFrame(imputed_data)\n",
    "assert(len(physicists) == 1049)\n",
    "with pd.option_context('display.max_rows', 1100):\n",
    "    display(physicists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now I have the dataframe, I'd like to persist it for later analysis. So I'll write out the contents to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicists.to_csv('../data/interim/physicists.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
