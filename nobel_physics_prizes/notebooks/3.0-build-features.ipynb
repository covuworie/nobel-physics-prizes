{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Features\n",
    "\n",
    "As a recap, the [training data](../data/processed/train-physicists-from-1901.csv), [validation data](../data/processed/validation-physicists-from-1901.csv) and [test data](../data/processed/test-physicists-from-1901.csv) contain information on physicists who were eligible to receive a Nobel Prize in Physics. That is, they were alive on and after 10 December 1901, the date the prize was first awarded. \n",
    "\n",
    "All of the physicists in the training data are deceased and all the physicists in the validation and test data are alive. Recall that the Nobel Prize in Physics cannot be awarded posthumously and one of the goals of this project is to try to predict the next Physics Nobel Laureates. As a result, the data was purposely sampled in this way, so that the training set can be used to build models, which predict whether a living physicist is likely to be awarded the Nobel Prize in Physics.\n",
    "\n",
    "It is time to use the training, validation and test data, along with the other various pieces of data: [Nobel Physics Laureates](../data/raw/nobel-physics-prize-laureates.csv), [Nobel Chemistry Laureates](../data/raw/nobel-chemistry-prize-laureates.csv), [Places](../data/processed/places.csv) and [Countries](../data/processed/Countries-List.csv), to create features that may help in predicting Physics Nobel Laureates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "An initialization step is needed to setup the environment:\n",
    "- The locale needs to be set for all categories to the user’s default setting (typically specified in the LANG environment variable) to enable correct sorting of words with accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "    \n",
    "locale.setlocale(locale.LC_ALL, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycountry_convert import country_alpha2_to_country_name\n",
    "from pycountry_convert import country_name_to_country_alpha3\n",
    "from pycountry_convert import country_alpha2_to_continent_code\n",
    "from pycountry_convert import country_alpha3_to_country_alpha2\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from src.data.country_utils import nationality_to_alpha2_code\n",
    "from src.features.features_utils import rank_hot_encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in the training, validation and test data and the list of Nobel Physics Laureates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_physicists = pd.read_csv('../data/processed/train-physicists-from-1901.csv')\n",
    "train_physicists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_physicists = pd.read_csv('../data/processed/validation-physicists-from-1901.csv')\n",
    "validation_physicists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_physicists = pd.read_csv('../data/processed/test-physicists-from-1901.csv')\n",
    "test_physicists.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobel_physicists = pd.read_csv('../data/raw/nobel-physics-prize-laureates.csv')\n",
    "nobel_physicists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some variants of laureate names in the training, validation and test data. As we will be searching for whether academic advisors, students, spouses, children, etc. of a physicist are physics laureates, for convenience it's useful to merge the `name` field into Nobel Physicists dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobel_columns = ['Year', 'Laureate', 'name', 'Country', 'Rationale']\n",
    "nobel_physicists = pd.merge(nobel_physicists,\n",
    "                            train_physicists.append(validation_physicists).append(test_physicists),\n",
    "                            how = 'left', left_on = 'Laureate',\n",
    "                            right_on = 'fullName')[nobel_columns]\n",
    "nobel_physicists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read in the list of Nobel Chemistry Laureates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobel_chemists = pd.read_csv('../data/raw/nobel-chemistry-prize-laureates.csv')\n",
    "nobel_chemists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we will be searching for whether academic advisors, students, spouses, children, etc. of a physicist are chemistry laureates. So for convenience it's useful to merge the `name` field into Nobel Chemists dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobel_chemists = pd.merge(nobel_chemists,\n",
    "                          train_physicists.append(validation_physicists).append(test_physicists),\n",
    "                          how = 'left', left_on = 'Laureate',\n",
    "                          right_on = 'fullName')[nobel_columns]\n",
    "nobel_chemists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are essentially physicists who are Chemistry Nobel Laureates. Surpringly there are quite a few of them. Of course, as noted previously, *Marie Curie* is the only double laureate in Physics and Chemistry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nobel_chemists[nobel_chemists.name.notna()].name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is worth noting that if there are alternative names of Chemistry Nobel Laureates in the physicists dataframe other than those above, they will *not* be found. However, we do not expect many of these, as at one point all the redirected URLS for the names were retrieved and very few were associated with laureates. In fact you can still see that some of these names are present in the [DBpedia redirects](../data/raw/dbpedia-redirects.csv) (e.g. search for \"Marie_Curie\"). When processing the physicist data earlier, the imputing of these redirects was removed for the names as a few of them were wrong. For instance, *Richard Feynman's* children redirect back to him! (e.g. search for \"Carl_Feynman\" and \"Michelle_Feynmann\" in the DBpedia redirects or directly try http://dbpedia.org/page/Carl_Feynman or http://dbpedia.org/page/Michelle_Feynmann in your browser.\n",
    "\n",
    "Another interesting observation is that the only one in this list still alive is *Manfred Eigen*. So we should not expect to see many, if any, physicists in the validation or test set who have Chemistry Nobel Laureate academic advisors, notable students, spouses etc. This is clearly a facet in which the training data is different from the validation and test data. Such differences can make learning difficult.\n",
    "\n",
    "Now, let's read the places and nationalities data into a dataframe. It's important at this point to turn off the default behavior of pandas which is to treat the string literal 'NA' as a missing value. In the dataset, 'NA' is both the continent code of North America and the ISO 3166 alpha-2 country code of Namibia. We then have to impute the missing values since pandas replaces them with the empty string.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pd.read_csv('../data/processed/places.csv', keep_default_na=False)\n",
    "places = places.replace('', np.nan)\n",
    "assert(all(places[places.countryAlpha3Code == 'USA']['continentCode'].values == 'NA'))\n",
    "places.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nationalities = pd.read_csv('../data/processed/Countries-List.csv', keep_default_na=False)\n",
    "nationalities = nationalities.replace('', np.nan)\n",
    "assert(nationalities[nationalities.Name == 'Namibia']['ISO 3166 Code'].values == 'NA')\n",
    "nationalities.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, with all the data read in, we can now move on to the real work, creating the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Features\n",
    "\n",
    "It is now time to create the features from the collected data. The features we will create are listed in the table below along with their type and description. The features can be grouped into three main groups, with the bulk of features falling in the first group, then the second group and so on:\n",
    "\n",
    "1. Features related to *professional and personal relationships* that the physicists have to *physics or chemistry laureates*, *educational institutions*, *work institutions*, *countries* and *continents*.\n",
    "\n",
    "2. Features related to the subfield of focus of the physicist denoting whether s/he is a *experimental physicist*, *theoretical physicist* or an *astronomer*.\n",
    "\n",
    "3. Features related to personal characteristics of the physicist, namely, *gender* and *number of years lived*.\n",
    "\n",
    "Remember that in the first group, there are people and institutions from different countries and continents that are directly involved in the [selection and voting process for the Nobel Prize in Physics](https://www.nobelprize.org/nomination/physics/) and therefore have a direct influence on those who become laureates. The second group is connected to subjective biases that may or may not exist concerning the major subfield of research of the physicist. While the third group is connected to subjective biases that may or may not exist concerning the gender and age of a physicist. Although the latter is also related to the invention or discovery \"standing the test of time\".\n",
    "\n",
    "| Feature                                        | Type        | Description                                         |\n",
    "| :---:                                          | :---:       | :---:                                               |\n",
    "| alma_mater                                     | Categorical | List of universities attended                       |\n",
    "| alma_mater_continent_codes                     | Categorical | List of continent codes of universities attended    |\n",
    "| alma_mater_country_alpha_3_codes               | Categorical | List of country codes of universities attended      |\n",
    "| birth_continent_codes                          | Categorical | List of continent codes of birth countries          |\n",
    "| birth_country_alpha_3_codes                    | Categorical | List of country codes of birth countries            |\n",
    "| citizenship_continent_codes                    | Categorical | List of continent codes of coutries of citizenship  |\n",
    "| citizenship_country_alpha_3_codes              | Categorical | List of country codes of citizenship                |\n",
    "| gender                                         | Binary      | Gender of the physicist (male / female)             |\n",
    "| is_astronomer                                  | Binary      | Is the physicist an astronomer? (yes / no)          |\n",
    "| is_experimental_physicist                      | Binary      | Is the physicist an experimental physicist? (yes / no) |\n",
    "| is_theoretical_physicist                       | Binary      | Is the physicist a theoretical physicist? (yes / no) |\n",
    "| num_alma_mater                                 | Ordinal     | No. of universities attended                       |\n",
    "| num_alma_mater_continent_codes                 | Ordinal     | No. of continent codes of universities attended     |\n",
    "| num_alma_mater_country_alpha_3_codes           | Ordinal     | No. of country codes of universities attended       |\n",
    "| num_birth_continent_codes                      | Ordinal     | No. of continent codes of birth countries           |\n",
    "| num_birth_country_alpha_3_codes                | Ordinal     | No. of birth country codes                          | \n",
    "| num_chemistry_laureate_academic_advisors       | Ordinal     | No. of chemistry laureate academic advisors         |\n",
    "| num_chemistry_laureate_children                | Ordinal     | No. of chemistry laureate children                  |\n",
    "| num_chemistry_laureate_doctoral_advisors       | Ordinal     | No. of chemistry laureate doctoral advisors         |\n",
    "| num_chemistry_laureate_doctoral_students       | Ordinal     | No. of chemistry laureate doctoral students         |\n",
    "| num_chemistry_laureate_influenced              | Ordinal     | No. of chemistry laureates the physicist influenced |\n",
    "| num_chemistry_laureate_influenced_by           | Ordinal     | No. of chemistry laureates the physicist was influenced by | \n",
    "| num_chemistry_laureate_notable_students        | Ordinal     | No. of chemistry laureate notable students          |\n",
    "| num_chemistry_laureate_parents                 | Ordinal     | No. of chemistry laureate parents                   |\n",
    "| num_chemistry_laureate_spouses                 | Ordinal     | No. of chemistry laureate spouses                   | \n",
    "| num_citizenship_continent_codes                | Ordinal     | No. continent codes of countries of citizenship  |\n",
    "| num_citizenship_country_alpha_3_codes          | Ordinal     | No. of country codes of citizenship                 |\n",
    "| num_physics_laureate_academic_advisors         | Ordinal     | No. of physics laureate academic advisors           |\n",
    "| num_physics_laureate_children                  | Ordinal     | No. of physics laureate children                    |\n",
    "| num_physics_laureate_doctoral_advisors         | Ordinal     | No. of physics laureate doctoral advisors           |\n",
    "| num_physics_laureate_doctoral_students         | Ordinal     | No. of physics laureate doctoral students           |\n",
    "| num_physics_laureate_influenced                | Ordinal     | No. of physics laureates the physicist influenced   |\n",
    "| num_physics_laureate_influenced_by             | Ordinal     | No. of physics laureates the physicist was influenced by |\n",
    "| num_physics_laureate_notable_students          | Ordinal     | No. of physics laureate notable students            |\n",
    "| num_physics_laureate_parents                   | Ordinal     | No. of physics laureate parents                     |\n",
    "| num_physics_laureate_spouses                   | Ordinal     | No. of physics laureate spouses                     |\n",
    "| num_residence_continent_codes                  | Ordinal     | No. of continent codes of residence countries       |\n",
    "| num_residence_country_alpha_3_codes            | Ordinal     | No. of residence country codes                      |\n",
    "| num_workplaces                                 | Ordinal     | No. of workplaces                                   |\n",
    "| num_workplaces_continent_codes                 | Ordinal     | No. of continent codes of countries of workplaces   |\n",
    "| num_workplaces_country_alpha_3_codes           | Ordinal     | No. of country codes of countries worked in         |\n",
    "| num_years_lived_group                          | Ordinal     | No. of years lived group (18-24, 25-34, etc.)             |\n",
    "| residence_continent_codes                      | Categorical | List of continent codes of countries of residence   |\n",
    "| residence_country_alpha_3_codes                | Categorical | List of country codes of countries of residence     |\n",
    "| workplaces                                     | Categorical | List of workplaces                                  |\n",
    "| workplaces_continent_codes                     | Categorical | List of continent codes of countries worked in      |\n",
    "| workplaces_country_alpha_3_codes               | Categorical | List of country codes of countries worked in        |\n",
    "\n",
    "Some comments are also warranted with regards to the types of the feature variables. As you can see, there are three types of variables:\n",
    "\n",
    "1. **Ordinal** variables.\n",
    "\n",
    "2. **Categorical** variables.\n",
    "\n",
    "3. **Binary** (**dichotomous**) variables.\n",
    "\n",
    "\n",
    "The categorical variables are all lists of varying lengths of **places** and therefore are not in the appropriate form for machine learning. Once we create them they will be encoded into binary variables and the lists will be discarded. You may ask why the encoding is done with categorical yes / no values rather than 0 / 1 values? It is because the algorithms that we will be processing the data with would treat 0 / 1 values as quantitive in nature, which clearly is not desired. Essentially, we will be left with two variable types, binary variables and ordinal variables. OK time to create the features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_features(physicists, nobel_physicists, nobel_chemists, places, nationalities):\n",
    "    \"\"\"Build features for the physicists.\n",
    "\n",
    "    Args:\n",
    "        physicists (pandas.DataFrame): Physicists dataframe.\n",
    "        nobel_physicists (pandas.DataFrame): Nobel Physics\n",
    "            Laureate dataframe.\n",
    "        nobel_chemists (pandas.DataFrame): Nobel Chemistry\n",
    "            Laureate dataframe.\n",
    "        places (pandas.DataFrame): Places dataframe.\n",
    "        nationality (pandas.DataFrame): Nationalies dataframe.\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame: Features dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    features = physicists.copy()[['fullName', 'name', 'gender']].rename(\n",
    "        mapper={'fullName': 'full_name'}, axis='columns')\n",
    "    features['num_years_lived_group'] = _build_num_years_lived_group(\n",
    "        physicists.birthDate, physicists.deathDate)\n",
    "    \n",
    "    _build_physics_subfield_features(features, physicists)\n",
    "    _build_num_laureates_features(features, physicists,\n",
    "                                  nobel_physicists, nobel_chemists)\n",
    "    \n",
    "    _build_citizenship_features(features, physicists, nationalities)\n",
    "    \n",
    "    _build_places_features(features, physicists, places)\n",
    "    \n",
    "    features = features.drop('name', axis='columns')\n",
    "    return features\n",
    "\n",
    "\n",
    "def _build_physics_subfield_features(features, physicists):\n",
    "    features_to_build = {\n",
    "        'is_theoretical_physicist': {'categories': 'Theoretical physicists',\n",
    "                                     'others': 'theoretical physic'},\n",
    "        'is_experimental_physicist': {'categories': 'Experimental physicists',\n",
    "                                      'others': 'experimental physic'},\n",
    "        'is_astronomer': {'categories': 'astronomers',\n",
    "                          'others': 'astronom'}\n",
    "    }\n",
    "    \n",
    "    for feature, search_terms in features_to_build.items():\n",
    "        features[feature] = _build_physics_subfield(\n",
    "            physicists.categories, physicists.field, physicists.description,\n",
    "            physicists.comment, search_terms=search_terms)\n",
    "    \n",
    "\n",
    "\n",
    "def _build_num_laureates_features(features, physicists, nobel_physicists,\n",
    "                                  nobel_chemists):\n",
    "    features_to_build = {\n",
    "        'laureate_academic_advisors': 'academicAdvisor',\n",
    "        'laureate_doctoral_advisors': 'doctoralAdvisor',\n",
    "        'laureate_doctoral_students': 'doctoralStudent',\n",
    "        'laureate_notable_students': 'notableStudent',\n",
    "        'laureate_children': 'child',\n",
    "        'laureate_parents': 'parent',\n",
    "        'laureate_spouses': 'spouse',\n",
    "        'laureate_influenced': 'influenced',\n",
    "        'laureate_influenced_by': 'influencedBy'\n",
    "    }\n",
    "    \n",
    "    for feature, relation in features_to_build.items():\n",
    "        features['num_physics_' + feature] = _build_num_laureates(\n",
    "            physicists[relation], nobel_physicists.Laureate, nobel_physicists.name)\n",
    "        features['num_chemistry_' + feature] = _build_num_laureates(\n",
    "            physicists[relation], nobel_chemists.Laureate, nobel_chemists.name)\n",
    "    # drop columns where the counts are all zeros\n",
    "    non_zero = (features != 0).any(axis='rows')\n",
    "    features.drop(non_zero[non_zero == False].index, axis='columns', inplace=True)\n",
    "\n",
    "\n",
    "    \n",
    "def _build_places_features(features, physicists, places):\n",
    "    features_to_build = {\n",
    "        'birth_country_alpha_3_codes': 'birthPlace',\n",
    "        'birth_continent_codes': 'birthPlace',\n",
    "        'residence_country_alpha_3_codes': 'residence',\n",
    "        'residence_continent_codes': 'residence',\n",
    "        'alma_mater': 'almaMater',\n",
    "        'alma_mater_country_alpha_3_codes': 'almaMater',\n",
    "        'alma_mater_continent_codes': 'almaMater',\n",
    "        'workplaces': 'workplaces',\n",
    "        'workplaces_country_alpha_3_codes': 'workplaces',\n",
    "        'workplaces_continent_codes': 'workplaces'\n",
    "    }\n",
    "    \n",
    "    for feature, place in features_to_build.items():\n",
    "        code = 'countryAlpha3Code'\n",
    "        if 'continent' in feature:\n",
    "            code = 'continentCode'\n",
    "            \n",
    "        if feature in ['alma_mater', 'workplaces']:\n",
    "            features[feature] = physicists[place].apply(\n",
    "                _get_alma_mater_or_workplaces)           \n",
    "        else:\n",
    "            features[feature] = _build_places_codes(\n",
    "                physicists[place], places.fullName, places[code])\n",
    "        features['num_' + feature] = features[feature].apply(len)\n",
    "\n",
    "\n",
    "    \n",
    "def _build_citizenship_features(features, physicists, nationalities):\n",
    "    citizenship = physicists.citizenship.apply(\n",
    "        _get_citizenship_codes, args=(nationalities,))\n",
    "    nationality = physicists.nationality.apply(\n",
    "        _get_citizenship_codes, args=(nationalities,))\n",
    "    citizenship_description = physicists.description.apply(\n",
    "        _get_citizenship_codes, args=(nationalities,))\n",
    "    features['citizenship_country_alpha_3_codes'] = (\n",
    "        (citizenship + nationality + citizenship_description).apply(\n",
    "            lambda ctz: list(sorted(set(ctz)))))\n",
    "    features['num_citizenship_country_alpha_3_codes'] = (\n",
    "        features.citizenship_country_alpha_3_codes.apply(len))\n",
    "    features['citizenship_continent_codes'] = (\n",
    "        features.citizenship_country_alpha_3_codes.apply(\n",
    "            lambda al3: list(sorted({country_alpha2_to_continent_code(\n",
    "                country_alpha3_to_country_alpha2(cd)) for cd in al3}))))\n",
    "    features['num_citizenship_continent_codes'] = (\n",
    "        features.citizenship_continent_codes.apply(len))\n",
    "\n",
    "\n",
    "def _build_num_years_lived_group(birth_date, death_date):\n",
    "    death_date_no_nan = death_date.apply(_date_no_nan)\n",
    "    birth_date_no_nan = birth_date.apply(_date_no_nan)\n",
    "    years_lived = ((death_date_no_nan - birth_date_no_nan) /\n",
    "                   pd.to_timedelta(1, 'Y'))\n",
    "    years_lived = years_lived.apply(np.floor)\n",
    "    years_lived_group = years_lived.apply(_years_lived_group)\n",
    "    return years_lived_group\n",
    "\n",
    "        \n",
    "def _years_lived_group(years_lived):\n",
    "    assert(years_lived >= 18 and years_lived <= 120)\n",
    "    \n",
    "    groups = {\n",
    "        range(18, 25): '18-24',\n",
    "        range(25, 35): '25-34',\n",
    "        range(35, 50): '35-49',\n",
    "        range(50, 65): '50-64',\n",
    "        range(65, 80): '65-79',\n",
    "        range(80, 95): '80-94',\n",
    "        range(95, 121): '95-120'\n",
    "    }\n",
    "    \n",
    "    for range_, code in groups.items():\n",
    "        if years_lived in range_:\n",
    "            return groups[range_]\n",
    "\n",
    "\n",
    "def _build_physics_subfield(categories, field, description, comment, search_terms):\n",
    "    cat_theoretical_physicist = categories.apply(\n",
    "        lambda cat: search_terms['categories'] in cat)\n",
    "    field_theoretical_physicist = field.apply(\n",
    "        lambda fld: search_terms['others'] in fld.lower() if isinstance(fld, str)\n",
    "        else False)\n",
    "    desc_theoretical_physicist = description.apply(\n",
    "        lambda desc: search_terms['others'] in desc.lower() if isinstance(desc, str)\n",
    "        else False)\n",
    "    comm_theoretical_physicist = description.apply(\n",
    "        lambda comm: search_terms['others'] in comm.lower() if isinstance(comm, str)\n",
    "        else False)\n",
    "    subfield = (cat_theoretical_physicist |\n",
    "                field_theoretical_physicist |\n",
    "                desc_theoretical_physicist |\n",
    "                comm_theoretical_physicist)\n",
    "    subfield = subfield.apply(lambda val: 'yes' if val == True else 'no')\n",
    "    return subfield\n",
    "\n",
    "\n",
    "def _build_num_laureates(series, laureates, names):\n",
    "    laureate_names = series.apply(_get_nobel_laureates,\n",
    "                                  args=(laureates, names))\n",
    "    return laureate_names.apply(len)\n",
    "\n",
    "\n",
    "def _build_places_codes(places_in_physicists, full_name_in_places, places_codes):\n",
    "    codes = places_in_physicists.apply(_get_places_codes,\n",
    "                                       args=(full_name_in_places, places_codes))\n",
    "    return codes\n",
    "\n",
    "\n",
    "def _get_alma_mater_or_workplaces(cell):\n",
    "    if isinstance(cell, float):\n",
    "        return list()\n",
    "    \n",
    "    places = set()\n",
    "    places_in_cell = cell.split('|')\n",
    "    for place_in_cell in places_in_cell:\n",
    "        # group colleges of University of Oxford and University of Cambridge\n",
    "        # with their respective parent university\n",
    "        if place_in_cell.endswith(', Cambridge'):\n",
    "            places.add('University of Cambridge')\n",
    "        elif place_in_cell.endswith(', Oxford'):\n",
    "            places.add('University of Oxford')\n",
    "        else:\n",
    "            places.add(place_in_cell)\n",
    "    \n",
    "    places = list(places)\n",
    "    places.sort(key=locale.strxfrm)\n",
    "    return places\n",
    "\n",
    "\n",
    "def _get_citizenship_codes(series, nationalities):\n",
    "    alpha_2_codes = nationality_to_alpha2_code(series, nationalities)\n",
    "    if isinstance(alpha_2_codes, float):\n",
    "        return list()\n",
    "    alpha_2_codes = alpha_2_codes.split('|')\n",
    "    alpha_3_codes = [country_name_to_country_alpha3(\n",
    "        country_alpha2_to_country_name(alpha_2_code))\n",
    "                     for alpha_2_code in alpha_2_codes]\n",
    "    return alpha_3_codes\n",
    "\n",
    "\n",
    "def _get_nobel_laureates(cell, laureates, names):\n",
    "    laureates_in_cell = set()\n",
    "    \n",
    "    if isinstance(cell, str):\n",
    "        # assume the same name if only differs by a hyphen\n",
    "        # or whitespace at front or end of string\n",
    "        values = cell.strip().replace('-', ' ').split('|')\n",
    "        for value in values:\n",
    "            if value in laureates.values:\n",
    "                laureates_in_cell.add(value)\n",
    "            if names.str.contains(value, regex=False).sum() > 0:\n",
    "                laureates_in_cell.add(value)\n",
    "                    \n",
    "    laureates_in_cell = list(laureates_in_cell)\n",
    "    return laureates_in_cell\n",
    "\n",
    "    \n",
    "def _get_places_codes(cell, full_name_in_places, places_codes):\n",
    "    codes = set()\n",
    "\n",
    "    if isinstance(cell, str):\n",
    "        places = cell.split('|')\n",
    "        for place in places:\n",
    "            code_indices = full_name_in_places[\n",
    "                full_name_in_places == place].index\n",
    "            assert(len(code_indices) <= 1)\n",
    "            if len(code_indices) != 1:\n",
    "                continue\n",
    "            code_index = code_indices[0]\n",
    "            codes_text = places_codes[code_index]\n",
    "            if isinstance(codes_text, float):\n",
    "                continue\n",
    "            codes_in_cell = codes_text.split('|')\n",
    "            for code_in_cell in codes_in_cell:\n",
    "                if code_in_cell:\n",
    "                    codes.add(code_in_cell)\n",
    "\n",
    "    codes = list(codes)\n",
    "    codes.sort()\n",
    "    return codes\n",
    "    \n",
    "\n",
    "def _date_no_nan(date):\n",
    "    if isinstance(date, str):\n",
    "        return datetime.strptime(date, '%Y-%m-%d').date()\n",
    "    return datetime(2018, 10, 24).date()  # fix the date for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = build_features(train_physicists, nobel_physicists, nobel_chemists, places, nationalities)\n",
    "assert((len(train_features) == len(train_physicists)))\n",
    "assert(len(train_features.columns) == 45)\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = build_features(\n",
    "    validation_physicists, nobel_physicists, nobel_chemists, places, nationalities)\n",
    "assert((len(validation_features) == len(validation_physicists)))\n",
    "assert(len(validation_features.columns) == 37)\n",
    "validation_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = build_features(test_physicists, nobel_physicists, nobel_chemists, places, nationalities)\n",
    "assert((len(test_features) == len(test_physicists)))\n",
    "assert(len(test_features.columns) == 36)\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are more features in the training set than in the validation and test sets. So what are these extra features? These are mainly related to the relationships physicists have with chemistry and physics laureates. It seems like the data is not so rich, especially with regards to more modern physicists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.columns.difference(validation_features.columns).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.columns.difference(test_features.columns).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any machine models that we build will have parameters chosen using the validation set and be evaluated on the test set. The tempting thing to do is to reduce the features to the common set of features between the training, validation and test sets, which have variability across all three datasets. We will do this for the training and validation sets as it seems a perfectly reasonable thing to do. However, using the test set would clearly be *data snooping* (cheating) as it is meant to be unseen data, and as such, cannot be used to make any decisions during the modeling process. So to ensure that the test set features are identical to the training set features, we will \"pad\" the extra features in the test set with all \"0\" values and remove any extra features that are not present in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = train_features.columns.intersection(validation_features.columns)\n",
    "assert(validation_features.equals(validation_features[feature_cols]))\n",
    "train_features = train_features[feature_cols]\n",
    "assert((len(train_features.columns) == len(validation_features.columns)))\n",
    "assert(sorted(train_features.columns.tolist()) == sorted(validation_features.columns.tolist()))\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = test_features.columns.intersection(train_features.columns)\n",
    "test_features = test_features[feature_cols]\n",
    "test_features['num_physics_laureate_influenced'] = 0\n",
    "test_features['num_physics_laureate_influenced_by'] = 0\n",
    "assert((len(test_features.columns) == len(train_features.columns)))\n",
    "assert(sorted(test_features.columns.tolist()) == sorted(train_features.columns.tolist()))\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will binary encode the list features. Due to the binary encoding there will be a differing number of features in the training, test and validation sets. We will follow a methodology analagous to the above in order to ensure that the features are identical in the training, validation and test sets. \n",
    "\n",
    "The differing features occur due to the differing *country codes*, *workplaces*, *educational institutions*, etc. that the physicists are associated with. Some of the differences are due variability in the data and some are caused by the **selection** bias that we deliberately introduced in our data sampling process. The latter issue is an important one that we will return to in a later notebook.\n",
    "\n",
    "We will also be using a `presence_threshold` to group binary features that only appear in a few instances into an \"other\" category. This is intended to reduce the dimensionality of the feature space and help to prevent overfitting during the model building phase. Let's go ahead and \"binarize\" the list features now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_list_features(features, train_features=None, presence_threshold=0.0, pad_features=False):\n",
    "    \"\"\"Binarize list features.\n",
    "    \n",
    "    Binary encode the list categorical features in the\n",
    "    features dataframe.\n",
    "\n",
    "    Args:\n",
    "        features (pandas.DataFrame): Features dataframe.\n",
    "        train_features (pandas.DataFrame, optional): Defaults to None.\n",
    "            Training features dataframe. Pass this parameter when \n",
    "            building features for a test or validation set so that \n",
    "            features not found in the training features are grouped\n",
    "            into the \"other category\" that is mentioned in\n",
    "            `presence_threshold` below.\n",
    "        presence_threshold (float, optional): Defaults to 0.0. For\n",
    "            each category in a categorical list feature, the \n",
    "            fraction of physicists for which the category is \n",
    "            present will be calculated. If the fraction is below\n",
    "            this threshold it will grouped into the \"other\"\n",
    "            category (represented by one or more \"*'s'\" in its\n",
    "            name). This is intended for \"bucketing\" rare\n",
    "            values to keep the dimensionality of the feature\n",
    "            space down and reduce chances of overfitting. Set\n",
    "            this value to zero to prevent any grouping of\n",
    "            values. Note that this value will be ignored when\n",
    "            `train_features` is provided.\n",
    "        pad_features (bool, optional): Defaults to False. Pad binary\n",
    "            features not found in the training set with all 'no'\n",
    "            values. This should be set to True for a test set to\n",
    "            ensure that the test set features will match the training\n",
    "            set features.\n",
    "            \n",
    "    Returns:\n",
    "        pandas.DataFrame: Features dataframe.\n",
    "    \"\"\"\n",
    "    \n",
    "    # union of places and citizenship (without the counts)\n",
    "    series_to_binarize = {\n",
    "        'birth_country_alpha_3_codes': 'born_in_',\n",
    "        'birth_continent_codes': 'born_in_',\n",
    "        'residence_country_alpha_3_codes': 'lived_in_',\n",
    "        'residence_continent_codes': 'lived_in_',\n",
    "        'alma_mater': 'alumnus_of_',\n",
    "        'alma_mater_country_alpha_3_codes': 'alumnus_in_',\n",
    "        'alma_mater_continent_codes': 'alumnus_in_',\n",
    "        'workplaces': 'worked_at_',\n",
    "        'workplaces_country_alpha_3_codes': 'worked_in_',\n",
    "        'workplaces_continent_codes': 'worked_in_',\n",
    "        'citizenship_country_alpha_3_codes': 'citizen_of_',\n",
    "        'citizenship_continent_codes': 'citizen_in_'\n",
    "    }\n",
    "        \n",
    "    for series, prefix in series_to_binarize.items():\n",
    "        binarized = _binarize_list_feature(features[series], prefix,\n",
    "                                           train_features, presence_threshold)\n",
    "        features = features.drop(series, axis='columns').join(binarized)\n",
    "        \n",
    "    # add extra features in test set to sync with training set\n",
    "    if pad_features:\n",
    "        cols_to_add = set(train_features.columns) - set(features.columns)\n",
    "        shape=(len(features), len(cols_to_add))\n",
    "        features_to_pad = pd.DataFrame(\n",
    "            np.full(shape, 'no'), index=features.index, columns=cols_to_add)\n",
    "        features = features.join(features_to_pad)\n",
    "    return features\n",
    "    \n",
    "    \n",
    "def _binarize_list_feature(series, prefix, train_features=None,\n",
    "                           presence_threshold=0.0):\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    binarized = pd.DataFrame(\n",
    "        mlb.fit_transform(series),\n",
    "        columns=[prefix + class_.replace(' ', '_') for class_ in mlb.classes_],\n",
    "        index=series.index)\n",
    "    \n",
    "    if not (presence_threshold <= 0.0) or train_features is not None:\n",
    "        if train_features is not None:\n",
    "            cols_to_group = [col for col in binarized.columns if col not in\n",
    "                             train_features.columns]\n",
    "        else:\n",
    "            cols_to_group = binarized.mean() < presence_threshold\n",
    "            cols_to_group = cols_to_group[cols_to_group.values].index.tolist()\n",
    "            \n",
    "        # look for at least one '1' value in the row for a physicist\n",
    "        if cols_to_group:\n",
    "            other_col = binarized[cols_to_group].applymap(\n",
    "                lambda val: True if val == 1 else False).any(axis='columns')\n",
    "            other_col.name = _series_name(series.name, prefix)\n",
    "            binarized = binarized.drop(cols_to_group, axis='columns').join(other_col)\n",
    "\n",
    "    binarized = binarized.applymap(lambda val: 'yes' if val == 1 else 'no')\n",
    "    return binarized\n",
    "\n",
    "\n",
    "def _series_name(name, prefix):\n",
    "    if name.endswith('alpha_3_codes'):\n",
    "        other_name = '***'\n",
    "    elif name.endswith('continent_codes'):\n",
    "        other_name = '**'\n",
    "    else:\n",
    "        other_name = '*'\n",
    "    return prefix + other_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = binarize_list_features(train_features, presence_threshold=0.01)\n",
    "assert(len(train_features.columns) == 157)\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = binarize_list_features(validation_features, train_features=train_features)\n",
    "assert(len(validation_features.columns) == 147)\n",
    "validation_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are more features in the training set than in the validation set. So what are these extra features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.columns.difference(validation_features.columns).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working on the Manhattan Project certainly makes sense! OK let's reduce the training and validation set to the common set of features amongst them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = train_features.columns.intersection(validation_features.columns)\n",
    "assert(validation_features.equals(validation_features[feature_cols]))\n",
    "train_features = train_features[feature_cols]\n",
    "assert((len(train_features.columns) == len(validation_features.columns)))\n",
    "assert(sorted(train_features.columns.tolist()) == sorted(validation_features.columns.tolist()))\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are less features in the test set so let's \"pad\" the remaining binary features with \"no\" to ensure that the features are identical between the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = binarize_list_features(test_features, train_features=train_features, pad_features=True)\n",
    "assert(sorted(test_features.columns.tolist()) == sorted(train_features.columns.tolist()))\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features almost look good now, but there is one thing that is troubling. The mix of binary and ordinal variables complicates matters when it comes to machine learning. There are issues related to the following:\n",
    "- [How to correctly scale features for machine learning algorithms](https://stats.stackexchange.com/questions/69568/whether-to-rescale-indicator-binary-dummy-predictors-for-lasso)?\n",
    "- [Difficulty of interpretability of coefficients in generalized linear models](https://andrewgelman.com/2009/07/11/when_to_standar/)\n",
    "- [The bias introduced by lower importance given towards binary variables in random forests](https://roamanalytics.com/2016/10/28/are-categorical-variables-getting-lost-in-your-random-forests/)\n",
    "\n",
    "We would like to avoid these issues altogether.\n",
    "\n",
    "Seeing as the majority of the features are binary, it makes sense to convert the ordinal variables to binary variables. However, we do not want to lose the ordinal information that is present in these variables. The [rank-hot encoder](http://scottclowe.com/2016-03-05-rank-hot-encoder/) is an encoding that converts ordinal variables to binary variables whilst maintaining the ordinal information. *Scott C. Lowe*, PhD student studying neuroinformatics at the University of Edinburgh, explains that \"the **rank-hot encoder** is similar to a *one-hot encoder*, except every feature up to and including the current rank is hot.\" He illustrates this with the following example:\n",
    "\n",
    "<table>\n",
    "  <thead>\n",
    "    <tr>\n",
    "      <th>Satisfaction</th>\n",
    "      <th>Rank Index</th>\n",
    "      <th>One-Hot Encoding</th>\n",
    "      <th>Rank-Hot Encoding</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <td>Very bad</td>\n",
    "      <td>0</td>\n",
    "      <td><code class=\"highlighter-rouge\">[1, 0, 0, 0, 0]</code></td>\n",
    "      <td><code class=\"highlighter-rouge\">[0, 0, 0, 0]</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Bad</td>\n",
    "      <td>1</td>\n",
    "      <td><code class=\"highlighter-rouge\">[0, 1, 0, 0, 0]</code></td>\n",
    "      <td><code class=\"highlighter-rouge\">[1, 0, 0, 0]</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Neutral</td>\n",
    "      <td>2</td>\n",
    "      <td><code class=\"highlighter-rouge\">[0, 0, 1, 0, 0]</code></td>\n",
    "      <td><code class=\"highlighter-rouge\">[1, 1, 0, 0]</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Good</td>\n",
    "      <td>3</td>\n",
    "      <td><code class=\"highlighter-rouge\">[0, 0, 0, 1, 0]</code></td>\n",
    "      <td><code class=\"highlighter-rouge\">[1, 1, 1, 0]</code></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <td>Very good</td>\n",
    "      <td>4</td>\n",
    "      <td><code class=\"highlighter-rouge\">[0, 0, 0, 0, 1]</code></td>\n",
    "      <td><code class=\"highlighter-rouge\">[1, 1, 1, 1]</code></td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "\n",
    "He goes on to say, \"Instead of answering the query “Is the satisfaction x?”, the entries in a rank-hot encoder tell us “Is the satisfaction level at least x?”. This representation of the data allows a linear model to explain the effect of a high-rank as the additive composition of the effect of each rank in turn.\"\n",
    "\n",
    "Sounds very useful doesn't it! Plus there are some other very nice properties of this encoding scheme that are explained in the blog. The main cons of rank-hot encoding, which are shared with one-hot encoding, are:\n",
    "\n",
    "- The feature space gets larger.\n",
    "- Information is lost whenever a categorical value is observed in a new instance (i.e. in the test set) that was not observed in the training (or validation) set.\n",
    "\n",
    "However, the benefits mentioned earlier are so important that they outweigh these downsides. Plus there are ways of dealing with the increase in the size of the feature space. OK let's go ahead and **rank-hot encode** the ordinal features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_cols = [col for col in train_features.columns if col.startswith('num_')]\n",
    "enc = OneHotEncoder(categories='auto', sparse=False, dtype='int64', handle_unknown='ignore')\n",
    "enc.fit(train_features[ordinal_cols].append(validation_features[ordinal_cols]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = rank_hot_encode(train_features, enc, columns=ordinal_cols)\n",
    "train_features = train_features.replace({0: 'no', 1: 'yes'})\n",
    "assert(len(train_features.columns) == 206)\n",
    "assert(train_features.select_dtypes('int64').empty)\n",
    "assert(all(train_features.notna()))\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = rank_hot_encode(validation_features, enc, columns=ordinal_cols)\n",
    "validation_features = validation_features.replace({0: 'no', 1: 'yes'})\n",
    "assert(sorted(validation_features.columns.tolist()) == sorted(train_features.columns.tolist()))\n",
    "assert(validation_features.select_dtypes('int64').empty)\n",
    "assert(all(validation_features.notna()))\n",
    "validation_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = rank_hot_encode(test_features, enc, columns=ordinal_cols)\n",
    "test_features = test_features.replace({0: 'no', 1: 'yes'})\n",
    "assert(sorted(test_features.columns.tolist()) == sorted(train_features.columns.tolist()))\n",
    "assert(test_features.select_dtypes('int64').empty)\n",
    "assert(all(test_features.notna()))\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following columns in the training features have no variation in the values. Nothing can be learnt from these features, so let's drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_variation = (train_features != 'no').any(axis='rows')\n",
    "no_variation = no_variation[no_variation == False]\n",
    "assert(len(no_variation) ==  3)\n",
    "no_variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.drop(no_variation.index, axis='columns')\n",
    "assert(len(train_features) == len(train_physicists))\n",
    "assert(len(train_features.columns.tolist()) == 203)\n",
    "assert(len(train_features.select_dtypes('object').columns) == len(train_features.columns.tolist()))\n",
    "assert(all(train_features.notna()))\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = validation_features.drop(no_variation.index, axis='columns')\n",
    "assert(len(validation_features) == len(validation_physicists))\n",
    "assert(sorted(validation_features.columns.tolist()) == sorted(train_features.columns.tolist()))\n",
    "assert(len(validation_features.select_dtypes('object').columns) == len(validation_features.columns.tolist()))\n",
    "assert(all(validation_features.notna()))\n",
    "validation_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_features.drop(no_variation.index, axis='columns')\n",
    "assert(len(test_features) == len(test_physicists))\n",
    "assert(sorted(test_features.columns.tolist()) == sorted(train_features.columns.tolist()))\n",
    "assert(len(test_features.select_dtypes('object').columns) == len(test_features.columns.tolist()))\n",
    "assert(all(test_features.notna()))\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the features that remain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(train_features.drop('full_name', axis='columns').columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The binary encoding has increased the dimensionality of the problem. There are now 202\n",
    "features (excluding the `full_name`) for 542 observations in the training set, 192 observations in the validation set and 193 observations in the test set. A model that is fit to such data could be prone to overfitting and a dimensionality reduction on this data may be warranted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now we have the training, validation and test features dataframes, let's persist them for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_features.reindex(sorted(train_features.columns), axis='columns')\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = validation_features.reindex(sorted(validation_features.columns), axis='columns')\n",
    "validation_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = test_features.reindex(sorted(test_features.columns), axis='columns')\n",
    "test_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.to_csv('../data/processed/train-features.csv', index=False)\n",
    "validation_features.to_csv('../data/processed/validation-features.csv', index=False)\n",
    "test_features.to_csv('../data/processed/test-features.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
