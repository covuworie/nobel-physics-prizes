{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Geocode Places Interim Data\n",
    "\n",
    "The [places interim dataframe](../data/interim/places.csv) consists of many places with a *latitude* and a *longitude* and some with only a *country* defined. Futhermore, there are some places which are actually nationalities and have none of these defined. My goal here is obtain identifiable [ISO 3166-1 alpha 2 country codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2), [ISO 3166-1 alpha 3 country codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-3) and *continent codes* for these places which can be used further down the line for feature construction. The process of mapping from a latitude and longitude to a location is known as [reverse geocoding](https://en.wikipedia.org/wiki/Reverse_geocoding) and here I used the *python* library [reverse-geocoder](https://github.com/thampiman/reverse-geocoder) to help me with that. \n",
    "\n",
    "As mentioned, some places do not have a latitude or longitude, but do have a country defined. For places of this type I will use the python library [pycountry-convert](https://github.com/TuneLab/pycountry-convert) to convert between the *country name* and *country codes*. This certainly will not work in all instances due to some free form text in the country variable and in such cases I will resort to [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) to extract [geopolicatal entities](https://en.wiktionary.org/wiki/geopolitical_entity). For this task, I will use the excellent natural language processing library [spacy](https://spacy.io/usage/linguistic-features#section-named-entities).\n",
    "\n",
    "For places which are nationalities I will [normalize nationalities via a ISO 3166-1 alpha 2 country codes list](https://t2a.io/blog/normalising-nationalities-via-a-good-iso-3166-country-list/) to convert from nationalities to country codes.  It is important to note that some of the places do not have a latitude, longitude, country or nationality defined. In such cases, I'll have to get creative. OK enough said, time to go on a mapping frenzy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "A few initialization steps are needed to setup the environment:\n",
    "- The locale needs to be set for all categories to the userâ€™s default setting (typically specified in the LANG environment variable) to enable correct sorting of words with accents.\n",
    "- Load `en_core_web_sm` which is the default English language model in `spacy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import spacy\n",
    "    \n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycountry_convert import convert_continent_code_to_continent_name\n",
    "from pycountry_convert import country_alpha2_to_continent_code\n",
    "from pycountry_convert import country_alpha2_to_country_name\n",
    "from pycountry_convert import country_name_to_country_alpha2\n",
    "from pycountry_convert import country_name_to_country_alpha3\n",
    "import reverse_geocoder as rg\n",
    "\n",
    "from src.data.country_utils import nationality_to_alpha2_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Places Data\n",
    "\n",
    "First let's read the places data into a dataframe and take a look at the columns of interest for the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pd.read_csv('../data/interim/places.csv')\n",
    "place_cols = ['fullName', 'lat', 'long', 'country']\n",
    "places.head(20)[place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, it's obvious to see that there are places with latitudes, longitudes and countries and some with none of these defined. Exactly how many though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of places: ', len(places))\n",
    "print('Number with lat / long: ',\n",
    "      (places.lat.notna() & places.long.notna()).sum())\n",
    "assert(places.lat.isna().sum() == places.long.isna().sum())\n",
    "print('Number with country: ',\n",
    "      (places.country.notna()).sum())\n",
    "print('Number with neither: ', \n",
    "      (places.lat.isna() & places.long.isna() & places.country.isna()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two reasons why it is clearly better to start with the latitude and longitude first before using the country:\n",
    "\n",
    "- There are more values in the dataframe for latitude and longitude than country.\n",
    "- The latitude and longitude values are more precise than the country values since there is free form text in the latter field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Geocoding\n",
    "\n",
    "OK let's perform the reverse geocoding to obtain the alpha 2 country code and take a look at the first few places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_geocode(places):\n",
    "    \"\"\"Reverse geocode the places dataframe.\n",
    "    \n",
    "    Use latitude and longitudes to find ISO 3166-1 alpha-2 country codes. \n",
    "\n",
    "    Args:\n",
    "        places (pandas.DataFrame): Dataframe of places data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe containing ISO 3166-1 alpha-2 country codes.\n",
    "\n",
    "        Identical to `places` except that it contains an extra column for ISO \n",
    "        3166-1 alpha-2 country codes when latitude and longitude are present.\n",
    "    \"\"\"\n",
    "\n",
    "    rg_places = places.copy()\n",
    "    \n",
    "    coords = list(zip(places.lat, places.long))\n",
    "    coords = [coord for coord in coords if not np.isnan(coord[0])\n",
    "              and not np.isnan(coord[1])]\n",
    "    ccs = [result['cc'] for result in rg.search(coords)]\n",
    "    coords_indices = [i for (i, val) in enumerate(\n",
    "        places.lat.notna().values & places.long.notna().values) if val]\n",
    "    \n",
    "    country_codes = [np.nan] * len(places)\n",
    "    for i in coords_indices:\n",
    "        country_codes[i] = ccs.pop(0)\n",
    "    \n",
    "    rg_places['countryAlpha2Code'] = country_codes\n",
    "    return rg_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = reverse_geocode(places)\n",
    "assert(places.lat.isna().sum() == places.countryAlpha2Code.isna().sum())\n",
    "place_cols.append('countryAlpha2Code')\n",
    "places.head(20)[place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reverse_geocoder` seems to be quite accurate, but I do notice one error. Adelaide is not in Japan (JP)! Let's investigate this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.search([(34.929001, 138.600998)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confirms the value in the dataframe above and matches with the [lat](http://www.w3.org/2003/01/geo/wgs84_pos#lat) and [long](http://www.w3.org/2003/01/geo/wgs84_pos#long) values in the source: http://dbpedia.org/data/Adelaide.json. So what's wrong? A little trial and error reveals that there is an input error in the source. The latitude value is missing a minus sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.search([(-34.929001, 138.600998)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK nice to know it's not a reverse geocoding error. However, it does further raise some questions as to the accuracy of DBpedia data. A quick scan through the data though does reveal that this type of issue is rare though.\n",
    "\n",
    "Time to move on now and check how many places have values for the country but not a country alpha 2 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(places.country.notna() & places.countryAlpha2Code.isna()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too many, but time to take care of them nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Countries to Alpha-2 Country Codes\n",
    "\n",
    "I'm now going to convert the remaining places with only countries to their associated alpha-2 country codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_to_alpha2_code(text):\n",
    "    \"\"\"Create ISO 3166-1 alpha-2 country codes from countries.\n",
    "    \n",
    "    Use the country to find ISO 3166-1 alpha-2 country codes.\n",
    "    This function should only be called for a subset of the\n",
    "    places dataframe where country is defined and latitude or\n",
    "    longitude is not (or equivalently ISO 3166-1 alpha-2\n",
    "    country code is not defined).\n",
    "\n",
    "    Args:\n",
    "        text (str): Text containing countries.\n",
    "\n",
    "    Returns:\n",
    "        `str` or `numpy.nan`: Pipe separated list of ISO 3166-1\n",
    "            alpha-2 country codes if found, otherwise numpy.nan.\n",
    "    \"\"\"\n",
    "    \n",
    "    countries = text.split('|')\n",
    "    alpha2_codes = set()\n",
    "    for country in countries:\n",
    "        try:\n",
    "            alpha2 = country_name_to_country_alpha2(country)\n",
    "            alpha2_codes.add(alpha2)\n",
    "        except KeyError:\n",
    "            doc = nlp(country)\n",
    "            for ent in (ent for ent in doc.ents if ent.label_ == 'GPE'):\n",
    "                try:\n",
    "                    alpha2 = country_name_to_country_alpha2(ent.text)\n",
    "                    alpha2_codes.add(alpha2)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "                    \n",
    "    if alpha2_codes:\n",
    "        alpha2_codes = '|'.join(sorted(alpha2_codes, key=locale.strxfrm))\n",
    "    else:\n",
    "        alpha2_codes = np.nan\n",
    "    return alpha2_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_countries = places[places.countryAlpha2Code.isna() &\n",
    "           places.country.notna()][['country', 'countryAlpha2Code']]\n",
    "places.loc[places_countries.index, 'countryAlpha2Code'] = (\n",
    "    places_countries.country.apply(country_to_alpha2_code))\n",
    "places.loc[places_countries.index][place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Nationalities to Alpha-2 Country Codes\n",
    "\n",
    "Looking at the dataframe, it is clear that some of the remaining places are nationalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places[places.countryAlpha2Code.isna()][place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will now read in the nationality list that will help me in converting these nationalities to their associated alpha-2 country codes. It's important at this point to turn off the default behavior of *pandas* which is to treat the string literal 'NA' as a missing value. In the dataset, 'NA' is the ISO 3166 alpha-2 country code of Namibia. I then have to impute the missing values since *pandas* replaces them with the empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # NB: I have manually fixed the csv to have 'NA' as the country code\n",
    "    # for Namibia. The author of the file clearly did not realize that by\n",
    "    # default 'NA' in a field is treated as NAN by pandas.\n",
    "    nationalities = pd.read_csv('../data/external/Countries-List.csv',\n",
    "                                keep_default_na=False)\n",
    "    nationalities = nationalities.replace('', np.nan)\n",
    "except FileNotFoundError:\n",
    "    nationalities = pd.read_csv(\n",
    "        'https://t2a.io/blog/wp-content/uploads/2014/03/Countries-List.csv',\n",
    "        encoding = 'ISO-8859-1')\n",
    "    nationalities.to_csv('../data/external/Countries-List.csv', index=False)\n",
    "\n",
    "assert(nationalities[\n",
    "    nationalities.Name == 'Namibia']['ISO 3166 Code'].values == 'NA')\n",
    "nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll manually add some commonly used names and demonyms to the dataframe. Despite these being neither countries or nationalities, they either are or were in common use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_nationalities = pd.DataFrame(\n",
    "    [\n",
    "        ['GB', 'England', 'English', np.nan, np.nan],\n",
    "        ['CI', 'Ivory Coast', 'Ivorian', np.nan, np.nan],\n",
    "        ['GB', 'Northern Ireland', 'Northern Irish', np.nan, np.nan],\n",
    "        ['IR', 'Persia', 'Persian', np.nan, np.nan],\n",
    "        ['DE', 'Prussia', 'Prussian', np.nan, np.nan],\n",
    "        ['IE', 'Republic of Ireland', 'Irish', np.nan, np.nan],\n",
    "        ['GB', 'Scotland', 'Scottish', 'Scot', np.nan],\n",
    "        ['RU', 'Soviet Union', 'Soviet', np.nan],\n",
    "        ['US', 'United States', 'American', np.nan, np.nan],\n",
    "        ['GB', 'Wales', 'Welsh', np.nan, np.nan]\n",
    "    ],\n",
    "    columns=nationalities.columns\n",
    ")\n",
    "nationalities = nationalities.append(\n",
    "    other_nationalities, ignore_index=True).sort_values(by='ISO 3166 Code')\n",
    "assert(len(nationalities) - len(other_nationalities) == 249)\n",
    "nationalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm now going to convert the remaining places which are nationalities to their associated alpha-2 country codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_nationalities = places[\n",
    "    places.countryAlpha2Code.isna()][['fullName', 'countryAlpha2Code']]\n",
    "places.loc[places_nationalities.index, 'countryAlpha2Code'] = (\n",
    "    places_nationalities.fullName.apply(nationality_to_alpha2_code,\n",
    "                                        args=(nationalities,)))\n",
    "places[places.lat.isna() & places.country.isna() &\n",
    "       places.countryAlpha2Code.notna()][place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please take note that although this is process is very accurate it is not perfect as it can result in a few false positives. For instance, *Scottish Church Collegiate School* is actually in India and not Scotland and *Petit Luxembourg* is a hotel in Paris and not in Luxembourg. But since the quantity of true positives far outweigh the false positives I'll go with it. Now I'm left with just the following places without a country code. They are a mix of companies, educational institutions, cities and some plain random stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places[places.countryAlpha2Code.isna()][place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, examining the *categories* column of the false positive above gives me the idea of applying the `nationality_to_alpha2_code` function to it also since the correct information is available there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(places[\n",
    "    places.fullName == 'Scottish Church Collegiate School']['categories'].values)\n",
    "print()\n",
    "print(places[places.fullName == 'Petit Luxembourg']['categories'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, rather than blindly applying the function to all the \"nationalities\" in the places dataframe which would give many false positives, such as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(places[places.fullName == 'Albanians']['categories'].values)\n",
    "print()\n",
    "print(places[places.fullName == 'Carpathian Germans']['categories'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll be conservative and now apply it to only the categories column of the remaining places without a country code. That is to the mix of companies, educational institutions, cities and some plain random stuff shown in the dataframe above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_others = places[\n",
    "    places.countryAlpha2Code.isna()][['fullName', 'categories',\n",
    "                                      'countryAlpha2Code']]\n",
    "places_others\n",
    "places.loc[places_others.index, 'countryAlpha2Code'] = (\n",
    "    places_others.categories.apply(nationality_to_alpha2_code,\n",
    "                                   args=(nationalities,)))\n",
    "places.loc[places_others.index][place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again the success rate is so high that it is definitely sufficient to proceed with this. However, as usual there are few false positives. A clear example of this is *Cape Canaveral* which is of course located in the United States and not India. This is due to the fact that it is situated near the *Indian* River Lagoon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(places[places.fullName == 'Cape Canaveral'][place_cols])\n",
    "print(places[places.fullName == 'Cape Canaveral'].categories.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check to see how many places remain without a country code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places[places.countryAlpha2Code.isna()][place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very few indeed. In fact many of these are not even \"places\". I have managed to map nearly all of the places to country codes so it's time to move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Percentage of places mapped to country codes:',\n",
    "      100 * round(places.countryAlpha2Code.notna().sum() / \n",
    "                  len(places), 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Alpha-2 Country Codes to Other Codes and Names\n",
    "\n",
    "Finally, I can now use `pycountry-convert` to map from all the alpha-2 country codes to alpha-3 country codes, continent codes, country names and continent names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha2_to_codes_names(places):\n",
    "    \"\"\"Create other codes and names from ISO 3166-1 alpha-2 country codes.\n",
    "    \n",
    "    Use ISO 3166-1 alpha-2 country codes to find country name, ISO 3166-1\n",
    "    alpha-3 country codes, continent code and continent name. \n",
    "\n",
    "    Args:\n",
    "        places (pandas.DataFrame): Dataframe of places data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe containing the extra fields mentioned above.\n",
    "\n",
    "        Identical to `places` except that it contains extra columns mentioned\n",
    "        above.\n",
    "    \"\"\"\n",
    "\n",
    "    codes_names_places = places.copy()\n",
    "    \n",
    "    codes_names_places['countryName'] = (\n",
    "        codes_names_places.countryAlpha2Code.apply(\n",
    "            _text_to_loc_or_codes, args=(country_alpha2_to_country_name,)))    \n",
    "    codes_names_places['countryAlpha3Code'] = (\n",
    "        codes_names_places.countryName.apply(\n",
    "            _text_to_loc_or_codes, args=(country_name_to_country_alpha3,)))\n",
    "    codes_names_places['continentCode'] = (\n",
    "        codes_names_places.countryAlpha2Code.apply(\n",
    "            _text_to_loc_or_codes, args=(country_alpha2_to_continent_code,))) \n",
    "    codes_names_places['continentName'] = (\n",
    "        codes_names_places.continentCode.apply(\n",
    "            _text_to_loc_or_codes, args=(convert_continent_code_to_continent_name,)))\n",
    "    \n",
    "    return codes_names_places\n",
    "\n",
    "\n",
    "def _text_to_loc_or_codes(text, rg_function):\n",
    "    if isinstance(text, float):\n",
    "        return text\n",
    "\n",
    "    texts = text.split('|')\n",
    "    items = set()\n",
    "    for text in texts:\n",
    "        # Exclude French Southern Territories and Vatican City when\n",
    "        # converting to continents since they are not recognized\n",
    "        exclude_cc = ['TF', 'VA']\n",
    "        if text in exclude_cc:\n",
    "            continue\n",
    "        item = rg_function(text)\n",
    "        items.add(item)\n",
    "\n",
    "    if items:\n",
    "        items = '|'.join(sorted(items, key=locale.strxfrm))\n",
    "    else:\n",
    "        items = np.nan\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = alpha2_to_codes_names(places)\n",
    "assert((places.countryAlpha2Code.isna() & \n",
    "        places.country.notna()).sum() == 0)\n",
    "place_cols = place_cols + ['countryAlpha3Code', 'countryName',\n",
    "                           'continentCode', 'continentName']\n",
    "places[place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now I have the places and nationalities dataframes, I'll persist them for future use in feature construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = places.reindex(sorted(places.columns), axis='columns')\n",
    "places.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places.to_csv('../data/processed/places.csv', index=False)\n",
    "nationalities.to_csv('../data/processed/Countries-List.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
