{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reverse Geocode Places Interim Data\n",
    "\n",
    "The [places interim dataframe](../data/interim/places.csv) consists of many places with a *latitude* and a *longitude* and some with only a *country* defined. My goal here is obtain identifiable [ISO 3166-1 alpha 2 country codes](https://en.wikipedia.org/wiki/ISO_3166-1_alpha-2) and names for these places which can be used further down the line for feature construction. The process of mapping from a latitude and longitude to a location name is known as [reverse geocoding](https://en.wikipedia.org/wiki/Reverse_geocoding) and here I used the *python* library [reverse-geocoder](https://github.com/thampiman/reverse-geocoder) to help me with that. \n",
    "\n",
    "As mentioned, some places do not have a latitude or longitude, but do have a country defined. For places of this type I will use the python library [pycountry-convert](https://github.com/TuneLab/pycountry-convert) to convert between the *country name* and the *country code*, *continent code* and *continent name*. This certainly will not work in all instances due to some free form text in the country variable and in such cases I will resort to [named entity recognition](https://en.wikipedia.org/wiki/Named-entity_recognition) to extract [geopolicatal entities](https://en.wiktionary.org/wiki/geopolitical_entity). For this task, I will use the excellent natural language processing library [spacy](https://spacy.io/usage/linguistic-features#section-named-entities).\n",
    "\n",
    "It is important to note that some of the places do not have a latitude, longitude or country defined. In such cases, there is not much that can be done. OK enough words for now, time to go on a mapping frenzy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "A few initialization steps are needed to setup the environment:\n",
    "- The locale needs to be set for all categories to the userâ€™s default setting (typically specified in the LANG environment variable) to enable correct sorting of words with accents.\n",
    "- Load `en_core_web_sm` which is the default English language model in `spacy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "import spacy\n",
    "    \n",
    "locale.setlocale(locale.LC_ALL, '')\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pycountry_convert import convert_continent_code_to_continent_name\n",
    "from pycountry_convert import country_alpha2_to_continent_code\n",
    "from pycountry_convert import country_alpha2_to_country_name\n",
    "from pycountry_convert import country_name_to_country_alpha2\n",
    "from pycountry_convert import country_name_to_country_alpha3\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Places Data\n",
    "\n",
    "First let's read the places data into a dataframe and take a look at the columns of interest for the first few entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pd.read_csv('../data/interim/places.csv')\n",
    "place_cols = ['fullName', 'lat', 'long', 'country']\n",
    "places.head(20)[place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already, it's obvious to see that there are places with latitudes, longitudes and countries and some with none of these defined. Exactly how many though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of places: ', len(places))\n",
    "print('Number with lat / long: ',\n",
    "      (~places.lat.isna() & ~places.long.isna()).sum())\n",
    "assert(places.lat.isna().sum() == places.long.isna().sum())\n",
    "print('Number with country: ',\n",
    "      (~places.country.isna()).sum())\n",
    "print('Number with neither: ', \n",
    "      (places.lat.isna() & places.long.isna() & places.country.isna()).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two reasons why it is clearly better to start with the latitude and longitude first before using the country:\n",
    "\n",
    "- There are more values in the dataframe for latitude and longitude than country.\n",
    "- The latitude and longitude values are more precise than the country values since there is free form text in the latter field."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse Geocoding\n",
    "\n",
    "OK let's perform the reverse geocoding to obtain the alpha 2 country code and take a look at the first few places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_geocode(places):\n",
    "    \"\"\"Reverse geocode the places dataframe.\n",
    "    \n",
    "    Use latitude and longitudes to find ISO 3166-1 alpha-2 country codes. \n",
    "\n",
    "    Args:\n",
    "        places (pandas.DataFrame): Dataframe of places data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe containing ISO 3166-1 alpha-2 country codes.\n",
    "\n",
    "        Identical to `places` except that it contains an extra column for ISO \n",
    "        3166-1 alpha-2 country codes when latitude and longitude are present.\n",
    "    \"\"\"\n",
    "\n",
    "    rg_places = places.copy()\n",
    "    \n",
    "    coords = list(zip(places.lat, places.long))\n",
    "    coords = [coord for coord in coords if not np.isnan(coord[0])\n",
    "              and not np.isnan(coord[1])]\n",
    "    ccs = [result['cc'] for result in rg.search(coords)]\n",
    "    coords_indices = [i for (i, val) in enumerate(\n",
    "        ~places.lat.isna().values & ~places.long.isna().values) if val]\n",
    "    \n",
    "    country_codes = [np.nan] * len(places)\n",
    "    for i in coords_indices:\n",
    "        country_codes[i] = ccs.pop(0)\n",
    "    \n",
    "    rg_places['countryAlpha2Code'] = country_codes\n",
    "    return rg_places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = reverse_geocode(places)\n",
    "assert(places.lat.isna().sum() == places.countryAlpha2Code.isna().sum())\n",
    "place_cols.append('countryAlpha2Code')\n",
    "places.head(20)[place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`reverse_geocoder` seems to be quite accurate, but I do notice one error. Adelaide is not in Japan (JP)! Let's investigate this further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.search([(34.929001, 138.600998)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above confirms the value in the dataframe above and matches with the [lat](http://www.w3.org/2003/01/geo/wgs84_pos#lat) and [long](http://www.w3.org/2003/01/geo/wgs84_pos#long) values in the source: http://dbpedia.org/data/Adelaide.json. So what's wrong? A little trial and error reveals that there is an input error in the source. The latitude value is missing a minus sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg.search([(-34.929001, 138.600998)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK nice to know it's not a reverse geocoding error. However, it does further raise some questions as to the accuracy of DBpedia data. A quick scan through the data though does reveal that this type of issue is rare though.\n",
    "\n",
    "Time to move on now and check how many places have values for the country but not a country alpha 2 code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(~places.country.isna() & places.countryAlpha2Code.isna()).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too many, but time to take care of them nonetheless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting Countries to Alpha-2 Country Codes\n",
    "\n",
    "I'm now going to convert the remaining places with only countries to their associated alpha-2 country codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_to_alpha2_code(text):\n",
    "    \"\"\"Create ISO 3166-1 alpha-2 country codes from countries.\n",
    "    \n",
    "    Use the country to find ISO 3166-1 alpha-2 country codes.\n",
    "    This function should only be called for a subset of the\n",
    "    places dataframe where country is defined and latitude or\n",
    "    longitude is not (or equivalently ISO 3166-1 alpha-2\n",
    "    country code is defined).\n",
    "\n",
    "    Args:\n",
    "        text (str): Text containing countries.\n",
    "\n",
    "    Returns:\n",
    "        `str` or `numpy.nan`: Pipe separated list of ISO 3166-1\n",
    "            alpha-2 country codes if found, otherwise numpy.nan.\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # country mappings not recognized by reverse_geocoder and spacy\n",
    "    known_country_mappings = {'England': 'GB',\n",
    "                              'Northern Ireland': 'GB',\n",
    "                              'NZL-cats': 'NZ',\n",
    "                              'Prussia': 'DE',\n",
    "                              'Soviet Union': 'RU'}\n",
    "    countries = text.split('|')\n",
    "    alpha2_codes = set()\n",
    "    for country in countries:\n",
    "        try:\n",
    "            alpha2 = country_name_to_country_alpha2(country)\n",
    "            alpha2_codes.add(alpha2)\n",
    "        except KeyError:\n",
    "            if country in known_country_mappings:\n",
    "                alpha2 = known_country_mappings[country]\n",
    "                alpha2_codes.add(alpha2)\n",
    "            else:\n",
    "                doc = nlp(country)\n",
    "                for ent in (ent for ent in doc.ents if ent.label_ == 'GPE'):\n",
    "                    alpha2 = country_name_to_country_alpha2(ent.text)\n",
    "                    alpha2_codes.add(alpha2)\n",
    "                    \n",
    "    if alpha2_codes:\n",
    "        alpha2_codes = '|'.join(sorted(alpha2_codes, key=locale.strxfrm))\n",
    "    else:\n",
    "        alpha2_codes = np.nan\n",
    "    return alpha2_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places_countries = places[places.countryAlpha2Code.isna() &\n",
    "           ~places.country.isna()][['country', 'countryAlpha2Code']]\n",
    "places.loc[places_countries.index, 'countryAlpha2Code'] = (\n",
    "    places_countries.country.apply(country_to_alpha2_code))\n",
    "places[places.lat.isna() & ~places.countryAlpha2Code.isna()][place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Alpha-2 Country Codes to Other Codes and Names\n",
    "\n",
    "Finally, I can now use `pycountry-convert` to map from all the alpha-2 country codes to alpha-3 country codes, continent codes, country names and continent names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alpha2_to_codes_names(places):\n",
    "    \"\"\"Create other codes and names from ISO 3166-1 alpha-2 country codes.\n",
    "    \n",
    "    Use ISO 3166-1 alpha-2 country codes to find country name, ISO 3166-1\n",
    "    alpha-3 country codes, continent code and continent name. \n",
    "\n",
    "    Args:\n",
    "        places (pandas.DataFrame): Dataframe of places data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe containing the extra fields mentioned above.\n",
    "\n",
    "        Identical to `places` except that it contains extra columns mentioned\n",
    "        above.\n",
    "    \"\"\"\n",
    "\n",
    "    codes_names_places = places.copy()\n",
    "    \n",
    "    codes_names_places['countryName'] = (\n",
    "        codes_names_places.countryAlpha2Code.apply(\n",
    "            _text_to_loc_or_codes, args=(country_alpha2_to_country_name,)))    \n",
    "    codes_names_places['countryAlpha3Code'] = (\n",
    "        codes_names_places.countryName.apply(\n",
    "            _text_to_loc_or_codes, args=(country_name_to_country_alpha3,)))\n",
    "    codes_names_places['continentCode'] = (\n",
    "        codes_names_places.countryAlpha2Code.apply(\n",
    "            _text_to_loc_or_codes, args=(country_alpha2_to_continent_code,))) \n",
    "    codes_names_places['continentName'] = (\n",
    "        codes_names_places.continentCode.apply(\n",
    "            _text_to_loc_or_codes, args=(convert_continent_code_to_continent_name,)))\n",
    "    \n",
    "    return codes_names_places\n",
    "\n",
    "\n",
    "def _text_to_loc_or_codes(text, rg_function):\n",
    "    if isinstance(text, float):\n",
    "        return text\n",
    "\n",
    "    texts = text.split('|')\n",
    "    items = set()\n",
    "    for text in texts:\n",
    "        # Exclude French Southern Territories and Vatican City when\n",
    "        # converting to continents since they are not recognized\n",
    "        exclude_cc = ['TF', 'VA']\n",
    "        if text in exclude_cc:\n",
    "            continue\n",
    "        item = rg_function(text)\n",
    "        items.add(item)\n",
    "\n",
    "    if items:\n",
    "        items = '|'.join(sorted(items, key=locale.strxfrm))\n",
    "    else:\n",
    "        items = np.nan\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = alpha2_to_codes_names(places)\n",
    "assert((places.countryAlpha2Code.isna() & ~places.country.isna()).sum() == 0)\n",
    "place_cols = place_cols + ['countryAlpha3Code', 'countryName', 'continentCode',\n",
    "                           'continentName']\n",
    "places[place_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now I have the dataframe, I'd like to persist it for future use in feature construction. I'll first sort the dataframe by the column name, take a quick peek at the first few places and write out the contents of the full dataframe to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = places.reindex(sorted(places.columns), axis=1)\n",
    "places.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places.to_csv('../data/processed/places.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
