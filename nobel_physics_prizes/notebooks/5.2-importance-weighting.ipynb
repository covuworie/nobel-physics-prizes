{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importance Weighting\n",
    "\n",
    "Last time we detected that there was a [covariate shift](5.1-covariate-shift.ipynb) between the training and validation sets. Recall, that we are using the validation set as a proxy for the test set. Here we will try and correct for the covariate shift by reweighting training samples according to their importance, which is the ratio of test to training densities. Tsuboi et al. explain in [Direct density ratio estimation for large-scale covariate shift adaptation](https://pdfs.semanticscholar.org/6b0b/12fbd2455c37131ac1cedc5137a9fd7c3e53.pdf) that \"Under covariate shift, standard learning methods such as maximum likelihood estimation are no longer consistent, i.e., they do not produce the optimal solution even when the number of training samples tends to infinity. Thus, there exists an estimation bias induced by covariate shift. It has been shown that the bias can be asymptotically canceled by weighting the log likelihood terms according to the *importance*:\n",
    "\n",
    "\\begin{equation}\n",
    "w(x) = \\frac{p_{test}(x)}{p_{train}(x)}\n",
    "\\end{equation}\n",
    "\n",
    "where $p_{test}(x)$ and $p_{train}(x)$ are test and training input densities.\" \n",
    "\n",
    "Technically, there is a more rigorous definition of importance that accounts for class imbalance between the training and test sets, which multiplies the above ratio by the ratio of number of training samples to the number of test samples. For a derivation, see section 2.2.1 of [Density-ratio matching under the Bregman divergence:\n",
    "a unified framework of density-ratio estimation](https://www.ism.ac.jp/editsec/aism/pdf/10463_2011_Article_343.pdf) by Sugiyama et al. Or alternatively have a read of the [density ratio trick](http://blog.shakirm.com/2018/01/machine-learning-trick-of-the-day-7-density-ratio-trick/) on *Shakir's machine learning blog*.\n",
    "\n",
    "Importance weighting may still not be clear to you, it certainly wasn't to me, until I read the blog of [Scott Rome, Math Ph.D. who works in machine learning](https://srome.github.io/Covariate-Shift,-i.e.-Why-Prediction-Quality-Can-Degrade-In-Production-and-How-To-Fix-It/). He elucidates exactly what importance weighting does from a mathematical perspective. Essentially the sample weights applied to each training example, change the distribution with respect to which the expected training loss is taken, from the training distribution to the test distribution:\n",
    "\n",
    "\\begin{equation}\n",
    "\\DeclareMathOperator{\\E}{\\mathbb{E}}\n",
    "\\E_{x \\sim w(x)p_{train}(x)}loss(x) = \\int loss(x)w(x)p_{train}(x)dx = \\int loss(x)p_{test}(x)dx = \\E_{x \\sim p_{test}(x)}loss(x)\n",
    "\\end{equation}\n",
    "\n",
    "As he puts it, we can see that the expected loss on the test distribution $p_{test}(x)$ is the same as using the sample weights $w(x)$ on the training set! (Notice, this is trivially the case when $p_{train}(x)=p_{test}(x)$.) Once these sample weights are known, they can be supplied during the training phase via the `sample_weight` parameter in the `fit()` method of an *sklearn* learn estimator.\n",
    "\n",
    "The *importance* is rarely known in practice, so the central question becomes, how to accurately estimate it from the training and test samples? It is a difficult problem that we will answer shortly. If you are still unconvinced as to why estimating the *importance* is critical when covariate shift occurs, read the excellent *bias-variance tradeoff* argument given in the footnote of page 1, in the paper by Tsuboi et al. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from src.features.features_utils import convert_categoricals_to_numerical\n",
    "from src.externals.pykliep.pykliep import DensityRatioEstimator\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in both sets of training and validation features and convert the categorical fields to a numerical form that is suitable for building machine learning models. I'll also read in the target for the training data as it will be useful for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/processed/train-features.csv')\n",
    "X_train = convert_categoricals_to_numerical(train_features)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_topics = pd.read_csv('../data/processed/train-features-topics.csv')\n",
    "X_train_topics = convert_categoricals_to_numerical(train_features_topics)\n",
    "X_train_topics.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('../data/processed/train-target.csv')\n",
    "train_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv('../data/processed/validation-features.csv')\n",
    "X_validation = convert_categoricals_to_numerical(validation_features)\n",
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features_topics = pd.read_csv('../data/processed/validation-features-topics.csv')\n",
    "X_validation_topics = convert_categoricals_to_numerical(validation_features_topics)\n",
    "X_validation_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kullback-Leibler Importance Estimation Procedure (KLIEP)\n",
    "\n",
    "The importance can be estimated by estimating the test and training densities separately and taking their ratio. However, this does not work well in practice as the ratio is sensitive. Sugiyama et al. developed a more stable method known as [Kullback-Leibler Importance Estimation Procedure (KLIEP)](https://www.ism.ac.jp/editsec/aism/pdf/060_4_0699.pdf) that directly estimates the importance without the need of density estimation. They find an importance estimate $\\hat{w}(x)$ such that the [Kullbackâ€“Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) from the true test input density $p_{test}(x)$ to its estimate $\\hat{p}_{test}(x) = \\hat{w}(x)p_{train}(x)$ is minimized. The optimization problem involved in KLIEP is convex and so the unique global solution can be obtained. \n",
    "\n",
    "Technical details of the KLIEP method are described in full in section 2 of the Sugiyama et al. paper. A succint summary, that I will paraphrase here, is given by [Scott Rome, Math Ph.D. who works in machine learning](https://srome.github.io/Covariate-Shift,-i.e.-Why-Prediction-Quality-Can-Degrade-In-Production-and-How-To-Fix-It/). The KLIEP algorithm defines an estimator of the importance as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{w}(x) = \\sum_{x_{i} \\in D_{test}} \\alpha_i K(x, x_i, \\sigma)\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\alpha_i \\in \\mathbb{R}$ are parameters to be learned from data samples and $D_{test} \\subset X_{test}$ is a random sample of test set vectors $X_{test}$. Sugiyama et al. chose $K$ to be the *Gaussian kernel*:\n",
    "\n",
    "\\begin{equation}\n",
    "K(x, x_i, \\sigma) = \\exp \\bigg( -\\frac{|x-x_i|^2}{2 \\sigma^{2}} \\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "where the hyperparameter $\\sigma \\in \\mathbb{R}$ is called the *width* of the kernel. The authors then define the score:\n",
    "\n",
    "\\begin{equation}\n",
    "J = \\frac{1}{|X_{test}|} \\sum_{x \\in X_{test}} \\log \\hat{w}(x)\n",
    "\\end{equation}\n",
    "\n",
    "and prove that maximizing this value is equivalent to minimizing the Kullback-Leibler divergence between $p_{test}(x)$ and $\\hat{w}(x)p_{train}(x)$. Basically, maximizing $J$ leads to the optimal sample weights for training. The KLIEP algorithm uses gradient descent to find the $\\alpha_i$ that maximize $J$ given $D_{test}$ and $\\sigma$ subject to the following constraints:\n",
    "\n",
    "\\begin{equation}\n",
    "\\alpha_i \\ge 0 \\\\\n",
    "\\int \\hat{w}(x)p_{train}(x)dx = 1\n",
    "\\end{equation}\n",
    "\n",
    "The first constraint arises from the fact that the sample weights must be non-negative and the second contraint from the definition of $\\hat{w}(x)p_{train}(x)$ as a probability density function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "There are essentially two hyperparameters that need to be chosen in the KLIEP algorithm, the *kernel width* $\\sigma$ and the ratio $\\frac{D_{test}}{X_{test}}$. Model selection can be carried out in a principled manner through the *likelihood cross-validation* (LCV) procedure that is described by Sugiyama et al. It is very similar to grid-search cross-validation that we are familiar with, except for a few notable differences:\n",
    "\n",
    "1. Split the test set $D_{test}$ into $k$-folds.\n",
    "2. For each test fold, train a model on the union of the training set and $k$-1 test folds. Evaluate the score $J$ on the $k$-th test fold.\n",
    "3. Select the parameters based on the best averaged score and re-train the model on the full training and test sets.\n",
    "\n",
    "The small test set size means that the computational costs of LCV are low, so we will set $\\frac{D_{test}}{X_{test}} = 1$, which places a Gaussian kernel at every test point. This leaves only $\\sigma$ to be selected by LCV. Let's go ahead and use 5-fold LCV in [pykliep](https://github.com/srome/pykliep) to select the optimal value of $\\sigma$ for both the original features and the topics features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ratio = 1.0\n",
    "sigmas = list(np.linspace(0.1, 1.0, 21))\n",
    "cv = 5\n",
    "kliep = DensityRatioEstimator(num_params=test_ratio, cv=cv, sigmas=sigmas, random_state=0)\n",
    "kliep.fit(X_train.values, X_validation.values)\n",
    "print('((num_params, sigma), max j score) = ', kliep._j_scores[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kliep_topics = DensityRatioEstimator(num_params=test_ratio, cv=cv, sigmas=sigmas, random_state=1)\n",
    "kliep_topics.fit(X_train_topics.values, X_validation_topics.values)\n",
    "print('((num_params, sigma), max j score) = ', kliep_topics._j_scores[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Weights\n",
    "\n",
    "Using the fitted models, we can now estimate the sample weights $\\hat{w}(x)$, for both the original features and the topics features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = kliep.predict(X_train.values)\n",
    "weights = pd.DataFrame({'full_name': X_train.index, 'weight': weights})\n",
    "assert(len(weights) == len(train_features))\n",
    "weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_topics = kliep_topics.predict(X_train_topics.values)\n",
    "weights_topics = pd.DataFrame({'full_name': X_train_topics.index, 'weight': weights_topics})\n",
    "assert(len(weights_topics) == len(train_features_topics))\n",
    "weights_topics.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to visualize bar charts of the sample weights for the two sets of features. We identify the physicists by their index, rather than their name, for legibility of the figure. Furthermore, we distinguish between laureates and non-laureates and place the figures on the same scale to ease comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_weights(weights, weights_topics, train_target):\n",
    "    \"\"\"Plot bar charts of the sample weights of original features and topic features\n",
    "    for the training data.\n",
    "\n",
    "    Args:\n",
    "        weights (array, shape (n_samples,)): Sample weights for training features.\n",
    "        weights_topics (array, shape (n_samples,)): Sample weights for training topic features.\n",
    "        train_target (pandas.DataFrame): Training target.\n",
    "\n",
    "    Returns:\n",
    "        tuple of matplotlib.axes.Axes: axes.\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(10, 8))\n",
    "    \n",
    "    laureates_indices, non_laureate_indices = _get_laureate_non_laureate_indices(\n",
    "        weights, train_target)\n",
    "    _plot_axis(ax1, laureates_indices, non_laureate_indices, weights)\n",
    "    ax1.set_title('Sample weights for original features')\n",
    "    \n",
    "    laureates_indices, non_laureate_indices = _get_laureate_non_laureate_indices(\n",
    "        weights_topics, train_target)\n",
    "    _plot_axis(ax2, laureates_indices, non_laureate_indices, weights_topics)\n",
    "    ax2.set_xlabel('Physicist index')\n",
    "    ax2.set_title('Sample weights for topic features')\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return ax1, ax2\n",
    "\n",
    "    \n",
    "def _plot_axis(axes, laureates_indices, non_laureate_indices, weights):\n",
    "    axes.bar(laureates_indices, weights.loc[laureates_indices, 'weight'], label='laureates',\n",
    "            width=1.5, color='orange')\n",
    "    axes.bar(non_laureate_indices, weights.loc[non_laureate_indices, 'weight'],\n",
    "            label='non laureates', width=1.5, color='C0')\n",
    "    axes.set_ylabel('Weight / importance')\n",
    "    axes.set_ylim(0, 18)\n",
    "    axes.set_xlim(0, len(weights))\n",
    "    axes.legend()\n",
    "    \n",
    "\n",
    "def _get_laureate_non_laureate_indices(weights, target):\n",
    "    physicists = pd.merge(weights, target, on='full_name')\n",
    "    laureates_indices = physicists[physicists.physics_laureate == 'yes'].index\n",
    "    non_laureate_indices = physicists[physicists.physics_laureate == 'no'].index\n",
    "    return laureates_indices, non_laureate_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_weights(weights, weights_topics, train_target);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations about the two figures:\n",
    "1. The sample weights, on average, are higher for the original features than the topics features. This is not surprising as the covariate shift was much larger for the original features.\n",
    "2. For the original features, most of the weights are very low for the laureates. The laureates are not given much importance. This is worrying as I suspect that using these weights during learning may actually lead to a worse performing machine learning model! We will see what happends during model building. I'm not sure, but I suspect this bevahior may be caused by overfitting during the LCV in KLIEP.\n",
    "3. For the topics features, the weights for laureates are far more comparable to those of non-laureates. They are certainly given importance that is of significance.\n",
    "4. In both charts, the highest weights are for non-laureate. This is most likely caused by the class imbalance in the data. There are far more non-laureates than laureates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Sample Weights\n",
    "\n",
    "Now we have the sample weights for the original features and the topic features, let's persist them for future use in model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights.to_csv('../models/train-features-sample-weights.csv', index=False)\n",
    "weights_topics.to_csv('../models/train-features-topics-sample-weights.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
