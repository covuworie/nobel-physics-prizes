{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "Prior to any machine learning, it is prudent to establish a baseline model with which to compare any trained models against. If none of the trained models can beat this \"naive\" model, then the conclusion is either that machine learning is not suitable for the predictive task or a different learning approach is needed. My goal here is to create a rules-based baseline classifier that can be used as a benchmark to compare against machine learning classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from src.features.features_utils import convert_categoricals_to_numerical\n",
    "from src.features.features_utils import convert_target_to_numerical\n",
    "from src.models.metrics_utils import confusion_matrix_to_dataframe\n",
    "from src.models.metrics_utils import print_matthews_corrcoef\n",
    "from src.models.models_utils import baseline_model_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in the training and validation features and target and convert the categorical fields to a numerical form that is suitable for building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/processed/train-features.csv')\n",
    "train_features = convert_categoricals_to_numerical(train_features)\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('../data/processed/train-target.csv', squeeze=True)\n",
    "train_target = convert_target_to_numerical(train_target)\n",
    "train_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv('../data/processed/validation-features.csv')\n",
    "validation_features = convert_categoricals_to_numerical(validation_features)\n",
    "validation_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target = pd.read_csv('../data/processed/validation-target.csv', squeeze=True)\n",
    "validation_target = convert_target_to_numerical(validation_target)\n",
    "validation_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measure\n",
    "\n",
    "Before building a baseline classifier, I first need to address the issue of how to compare and assess the quality of different classifiers. A *performance measure* is clearly needed. But which one? [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) is affected by the probability of class membership of the target and therefore it is not a suitable metric for this problem due as there are many more non-laureates than laureates. In situations such as this, accuracy can be very misleading.\n",
    "\n",
    "The [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) (MCC) is a suitable performance measure as it is generally regarded as a balanced measure of binary classification performance that can be used when there is a class imbalance. [Predicting Protein-Protein Interaction by the Mirrortree Method Possibilities and Limitations](https://www.researchgate.net/publication/259354929_Predicting_Protein-Protein_Interaction_by_the_Mirrortree_Method_Possibilities_and_Limitations) says that \"MCC is a more robust measure of effectiveness of binary classification methods than such measures as precision, recall, and F-measure because it takes into account in a balanced way of all four factors contributing to the effectiveness; true positives, false positives, true negatives and false negatives\". The MCC can be calculated directly from the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) using the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "MCC = \\frac{TP \\times TN - FP \\times FN}{{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}}\n",
    "\\end{equation}\n",
    "\n",
    "where TP is the number of [true positives](https://en.wikipedia.org/wiki/True_positive), TN the number of true [negatives](https://en.wikipedia.org/wiki/True_negative), FP the number of [false positives](https://en.wikipedia.org/wiki/False_positive) and FN the number of [false negatives](https://en.wikipedia.org/wiki/False_negative). If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.\n",
    "\n",
    "The MCC is the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between the observed and predicted binary classifications. It has an upper limit of +1 indicating a perfect prediction, a lower limit of -1 indicating total disagreement between prediction and observation and a mid value of 0 representing a random prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier\n",
    "\n",
    "How should I go about creating this baseline classifier? Well my first idea is a classifier that always predicts the majority class. Let's go ahead and look at the MCC and confusion matrix for such a classifier on the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def majority_class_predict(target):\n",
    "    majority = target.value_counts().idxmax()\n",
    "    predict = np.full(len(target,), majority)\n",
    "    return pd.Series(predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_train_predict = majority_class_predict(train_target)\n",
    "majority_train_predict_mcc = matthews_corrcoef(y_true=train_target,\n",
    "                                               y_pred=majority_train_predict)\n",
    "print_matthews_corrcoef(majority_train_predict_mcc, 'Majority class classifier',\n",
    "                        data_label='train')\n",
    "majority_confusion_matrix_train = confusion_matrix_to_dataframe(\n",
    "    confusion_matrix(y_true=train_target, y_pred=majority_train_predict))\n",
    "majority_confusion_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_validation_predict = majority_class_predict(validation_target)\n",
    "majority_validation_predict_mcc = matthews_corrcoef(y_true=validation_target,\n",
    "                                                    y_pred=majority_validation_predict)\n",
    "print_matthews_corrcoef(majority_validation_predict_mcc, 'Majority class classifier',\n",
    "                        data_label='validation')\n",
    "index = ['Observed non-laureate', 'Observed laureate']\n",
    "columns = ['Predicted non-laureate', 'Predicted laureate']\n",
    "majority_confusion_matrix_validation = confusion_matrix_to_dataframe(\n",
    "    confusion_matrix(y_true=validation_target, y_pred=majority_validation_predict),\n",
    "    index=index, columns=columns)\n",
    "majority_confusion_matrix_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a classifier which always predicts the negative class is equivalent to random guessing and therefore is completely useless. The runtime warning is screaming this out loud as the sum of TP and FP is zero. Note that if I had instead used accuracy as the performance measure I would have been completely misled into believing that this was a reasonable classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Majority class classifier accuracy (train):',\n",
    "      round(accuracy_score(y_true=train_target, y_pred=majority_train_predict), 2))\n",
    "print('Majority class classifier accuracy (validation):',\n",
    "      round(accuracy_score(y_true=validation_target,\n",
    "                           y_pred=majority_validation_predict), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surely I can do better than this classifier. If you recall, during the [exploratory data analysis](4.0-exploratory-data-analysis.ipynb), I saw that there was a big positive effect being an [experimental physicist](https://htmlpreview.github.io/?https://github.com/covuworie/nobel-physics-prizes/blob/master/nobel_physics_prizes/notebooks/html_output/4.0-exploratory-data-analysis.html) has on becoming a Nobel Laureate in Physics. So let's try and leverage this information now by creating a classifier which only predicts a physicist is a laureate when s/he is an experimental physicist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_physicist_train_predict_mcc = matthews_corrcoef(\n",
    "    y_true=train_target, y_pred=train_features.is_experimental_physicist)\n",
    "print_matthews_corrcoef(experimental_physicist_train_predict_mcc,\n",
    "                        'Experimental physicist classifier', data_label='train')\n",
    "experimental_physicist_confusion_matrix_train = confusion_matrix_to_dataframe(\n",
    "    confusion_matrix(y_true=train_target, y_pred=train_features.is_experimental_physicist),\n",
    "    index=index, columns=columns)\n",
    "experimental_physicist_confusion_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experimental_physicist_validation_predict_mcc = matthews_corrcoef(\n",
    "    y_true=validation_target, y_pred=validation_features.is_experimental_physicist)\n",
    "print_matthews_corrcoef(experimental_physicist_validation_predict_mcc,\n",
    "                        'Experimental physicist classifier', data_label='validation')\n",
    "experimental_physicist_confusion_matrix_validation = confusion_matrix_to_dataframe(\n",
    "    confusion_matrix(y_true=validation_target,\n",
    "                     y_pred=validation_features.is_experimental_physicist),\n",
    "    index=index, columns=columns)\n",
    "experimental_physicist_confusion_matrix_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is better than the previous one. 0.59 MCC is a moderate correlation for the training set and 0.24 MCC is a low correlation for the validation set. Examination of the confusion matrices illustrates that this classifier is excellent at identifying the negative class, but not very good at identifying the positive class. In fact, on the validation set, it is appalling at predicting the laureates. As such, as a classifier it is also useless.\n",
    "\n",
    "Also, recall during the exploratory data analysis, I saw that the ratio of the number of alma mater and the ratio of the number of workplaces appear to be reasonable predictors of the target as the median values are consistently higher for laureates than non-laureates. It seems reasonable to create a classifier that always predicts a physicist is a laureate when the value of either of these variables is above the lower quartile for laureates. This is a value of approximately 0.8 for the ratio of the number of alma mater and 0 for the ratio of the number of workplaces. OK let's go ahead and look at the performance of thi classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_train_predict = baseline_model_predict(train_features)\n",
    "baseline_train_predict_mcc = matthews_corrcoef(\n",
    "    y_true=train_target, y_pred=baseline_train_predict)\n",
    "print_matthews_corrcoef(baseline_train_predict_mcc, 'Baseline classifier',\n",
    "                        data_label='train')\n",
    "baseline_confusion_matrix_train = confusion_matrix_to_dataframe(\n",
    "    confusion_matrix(y_true=train_target, y_pred=baseline_train_predict))\n",
    "baseline_confusion_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_validation_predict = baseline_model_predict(validation_features)\n",
    "baseline_validation_predict_mcc = matthews_corrcoef(\n",
    "    y_true=validation_target, y_pred=baseline_validation_predict)\n",
    "print_matthews_corrcoef(baseline_validation_predict_mcc, 'Baseline classifier',\n",
    "                        data_label='validation')\n",
    "baseline_confusion_matrix_validation = confusion_matrix_to_dataframe(\n",
    "    confusion_matrix(y_true=validation_target, y_pred=baseline_validation_predict))\n",
    "baseline_confusion_matrix_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is better than the previous two. Although the MCC's are weak for the training and validation sets, examination of the confusion matrices illustrates that this classifier is excellent at identifying the positive class. Or in other words, it has excellent recall on the positive class, although it's precision has plenty of scope for improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=validation_target, y_pred=baseline_validation_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is far from perfect, but it's not too bad for a \"naive\" rules-based classifier. It does seem like a classifier that is reasonable for me to use as a benchmark for comparing machine learning classifiers against."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
