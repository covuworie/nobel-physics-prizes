{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Model\n",
    "\n",
    "Prior to any machine learning, it is prudent to establish a baseline model with which to compare any trained models against. If none of the trained models can beat this \"naive\" model, then the conclusion is that either machine learning is not suitable for the predictive task or a different learning approach is needed. Our goal here is to create a *rules-based classifier* that can be used as a baseline to compare against machine learning classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from src.features.features_utils import convert_categoricals_to_numerical\n",
    "from src.features.features_utils import convert_target_to_numerical\n",
    "from src.models.metrics_utils import confusion_matrix_to_dataframe\n",
    "from src.models.metrics_utils import print_matthews_corrcoef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in the training and validation features and target. Also, let's convert the categorical fields to a numerical form that is suitable for performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/processed/train-features.csv')\n",
    "train_features = convert_categoricals_to_numerical(train_features)\n",
    "train_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('../data/processed/train-target.csv', index_col='full_name', squeeze=True)\n",
    "train_target = convert_target_to_numerical(train_target)\n",
    "train_target.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv('../data/processed/validation-features.csv')\n",
    "validation_features = convert_categoricals_to_numerical(validation_features)\n",
    "validation_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target = pd.read_csv('../data/processed/validation-target.csv', index_col='full_name',\n",
    "                                squeeze=True)\n",
    "validation_target = convert_target_to_numerical(validation_target)\n",
    "validation_target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Measure\n",
    "\n",
    "Before building a baseline classifier, we first need to address the issue of how to compare and assess the quality of different classifiers. A **performance measure** is clearly needed. But which one? [Accuracy](https://en.wikipedia.org/wiki/Accuracy_and_precision) is affected by the probability of class membership of the target and therefore it is not a suitable metric for this problem, as there are many more non-laureates than laureates. In such situations accuracy can be very misleading.\n",
    "\n",
    "The [Matthews Correlation Coefficient](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient) (MCC) (also known as the [phi coefficient](https://en.wikipedia.org/wiki/Phi_coefficient)) is a suitable performance measure that can be used when there is a class imbalance. It is widely regarded as a balanced measure of binary classification performance. [Predicting Protein-Protein Interaction by the Mirrortree Method Possibilities and Limitations](https://www.researchgate.net/publication/259354929_Predicting_Protein-Protein_Interaction_by_the_Mirrortree_Method_Possibilities_and_Limitations) says that \"MCC is a more robust measure of effectiveness of binary classification methods than such measures as precision, recall, and F-measure because it takes into account in a balanced way of all four factors contributing to the effectiveness; true positives, false positives, true negatives and false negatives\". The MCC can be calculated directly from the [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) using the formula:\n",
    "\n",
    "\\begin{equation}\n",
    "MCC = \\frac{TP \\times TN - FP \\times FN}{{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}}\n",
    "\\end{equation}\n",
    "\n",
    "where TP is the number of [true positives](https://en.wikipedia.org/wiki/True_positive), TN the number of true [negatives](https://en.wikipedia.org/wiki/True_negative), FP the number of [false positives](https://en.wikipedia.org/wiki/False_positive) and FN the number of [false negatives](https://en.wikipedia.org/wiki/False_negative). If any of the four sums in the denominator is zero, the denominator can be arbitrarily set to one; this results in a Matthews correlation coefficient of zero, which can be shown to be the correct limiting value.\n",
    "\n",
    "The MCC is the [Pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient) between the observed and predicted binary classifications. It has an upper limit of +1 indicating a perfect prediction, a lower limit of -1 indicating total disagreement between prediction and observation and a mid value of 0 representing a random prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Classifier\n",
    "\n",
    "How should we go about creating this baseline classifier? One idea is a classifier that always predicts the majority class. Let's go ahead and look at the MCC and confusion matrix for such a classifier on the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority = DummyClassifier(strategy='most_frequent')\n",
    "majority.fit(train_features, train_target)\n",
    "majority_train_predict = majority.predict(train_target)\n",
    "majority_train_predict_mcc = matthews_corrcoef(y_true=train_target, y_pred=majority_train_predict)\n",
    "print_matthews_corrcoef(majority_train_predict_mcc, 'Majority class classifier', data_label='train')\n",
    "majority_confusion_matrix_train = confusion_matrix_to_dataframe(\n",
    "    confusion_matrix(y_true=train_target, y_pred=majority_train_predict))\n",
    "majority_confusion_matrix_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "majority_validation_predict = majority.predict(validation_features)\n",
    "majority_validation_predict_mcc = matthews_corrcoef(y_true=validation_target,\n",
    "                                                    y_pred=majority_validation_predict)\n",
    "print_matthews_corrcoef(majority_validation_predict_mcc, 'Majority class classifier',\n",
    "                        data_label='validation')\n",
    "index = ['Observed non-laureate', 'Observed laureate']\n",
    "columns = ['Predicted non-laureate', 'Predicted laureate']\n",
    "majority_confusion_matrix_validation = confusion_matrix_to_dataframe(\n",
    "    confusion_matrix(y_true=validation_target, y_pred=majority_validation_predict), index=index,\n",
    "    columns=columns)\n",
    "majority_confusion_matrix_validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that a classifier which always predicts the negative class is equivalent to random guessing and therefore is completely useless. The runtime warning is screaming this out loud as the sum of TP and FP is zero. Note that if we had instead used accuracy as the performance measure we would have been completely misled into believing that this is a reasonable good classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Majority class classifier accuracy (train):',\n",
    "      round(accuracy_score(y_true=train_target, y_pred=majority_train_predict), 2))\n",
    "print('Majority class classifier accuracy (validation):',\n",
    "      round(accuracy_score(y_true=validation_target,\n",
    "                           y_pred=majority_validation_predict), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surely we can do better than this classifier. The function below is a brute force approach to creating a baseline classifier. It fits a model for each of the predictors in turn and returns the best model, as judged by MCC on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_feature_with_highest_mcc(train_features, train_target, validation_features, validation_target):\n",
    "\n",
    "    \"\"\"Find the feature with the highest Matthews Correlation Coefficient (MCC)\n",
    "    on the validation set.\n",
    "    \n",
    "    Prints the feature, it's MCC values on the training and validation sets as\n",
    "    well as the confusion matrices.\n",
    "\n",
    "    Args:\n",
    "        train_features (pandas.Dataframe): Training features.\n",
    "        train_target (pandas.Series): Training target.\n",
    "        validation_features (pandas.Dataframe): Training features.\n",
    "        validation_target (pandas.Series): Validation target.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    validation_mccs = {}\n",
    "    for feature in train_features.columns:\n",
    "        validation_mccs[feature] = round(matthews_corrcoef(y_true=validation_target,\n",
    "                                                           y_pred=validation_features[feature]), 2)\n",
    "    highest_mcc = sorted(validation_mccs.items(), key=operator.itemgetter(1), reverse=True)[0]\n",
    "    classifier_label = highest_mcc[0] + ' classifier'\n",
    "    \n",
    "    print_matthews_corrcoef(round(matthews_corrcoef(y_true=train_target,\n",
    "                                                    y_pred=train_features[feature]), 2),\n",
    "                            classifier_label, data_label='train')\n",
    "    confusion_matrix_train = confusion_matrix_to_dataframe(\n",
    "        confusion_matrix(y_true=train_target, y_pred=train_features[highest_mcc[0]]), index=index,\n",
    "        columns=columns)\n",
    "    display(confusion_matrix_train)\n",
    "    \n",
    "    print_matthews_corrcoef(highest_mcc[1], classifier_label, data_label='validation')\n",
    "    confusion_matrix_validation = confusion_matrix_to_dataframe(\n",
    "        confusion_matrix(y_true=validation_target, y_pred=validation_features[highest_mcc[0]]),\n",
    "        index=index, columns=columns)\n",
    "    display(confusion_matrix_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_feature_with_highest_mcc(train_features, train_target, validation_features, validation_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is not great, but it's much better than the previous one. The MCC's are low for the training and validation sets, however, they are definitely better than chance level performance. Examination of the confusion matrices illustrates that this classifier is slightly better than 50-50 at identifying the positive class and is quite good at identifying the negative class. This is also confirmed looking at the precision, recall and f1-score of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=validation_target,\n",
    "                            y_pred=validation_features.num_workplaces_at_least_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier is far from perfect, but it's not too bad for a \"naive\" rules-based classifier. It does seem like a classifier that is reasonable to use as a benchmark for comparing machine learning classifiers against."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
