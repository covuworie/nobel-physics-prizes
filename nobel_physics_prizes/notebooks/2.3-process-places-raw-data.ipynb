{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Places Raw Data\n",
    "\n",
    "Now I aim to convert the JSON lines format [places raw data](../data/raw/places.jsonl.gz) into an intermediate format *pandas dataframe* that is more convenient to work with. This intermediate format will be close to the final format of the data that I'll be working with for analysis.\n",
    "\n",
    "My goal is to parse the JSON data in order to extract the interesting fields of information such as *latitude*, *longitude*, *city*, *country*, etc. As mentioned previously, these fields will be useful for constructing features based on the country that the place is located in. In particular, for places with a latitude and longitude (*locations*), I will be able to map from locations to identifiable countries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "An initialization step is needed to setup the environment:\n",
    "- The locale needs to be set for all categories to the userâ€™s default setting (typically specified in the LANG environment variable) to enable correct sorting of words with accents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import locale\n",
    "    \n",
    "locale.setlocale(locale.LC_ALL, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas.io.json import json_normalize\n",
    "\n",
    "from src.data.dbpedia_utils import construct_resource_urls\n",
    "from src.data.dbpedia_utils import find_resource_url\n",
    "from src.data.dbpedia_utils import get_source_url\n",
    "from src.data.dbpedia_utils import impute_redirect_filenames\n",
    "from src.data.dbpedia_utils import json_categories_to_dict\n",
    "from src.data.dbpedia_utils import json_keys_to_dict\n",
    "from src.data.dbpedia_utils import PLACES_IMPUTE_KEYS\n",
    "from src.data.jsonl_utils import read_jsonl\n",
    "from src.data.progress_bar import progress_bar\n",
    "from src.data.url_utils import get_filename_from_url\n",
    "from src.data.url_utils import get_redirect_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the JSON Lines Data\n",
    "\n",
    "First let's read the JSON lines data into a list so that we can parse it later and take a look at the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lines = read_jsonl('../data/raw/places.jsonl.gz')\n",
    "json_lines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Fields of Interest\n",
    "\n",
    "Here I define the keys and values that I wish to extract from the JSON lines data and also some that I explicitly wish to exclude. These keys and values are in the form of *semantic URLs* which allows anyone to visit the resource in a web browser and see their meaning. The URLs are in 5 namespaces:\n",
    "\n",
    "- [DBpedia Ontology](https://wiki.dbpedia.org/services-resources/ontology)\n",
    "- DBpedia Property\n",
    "- PURL\n",
    "- W3\n",
    "- FOAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DBPEDIA_JSON_KEYS = [\n",
    "    # DBpedia ontology\n",
    "    'http://dbpedia.org/ontology/abstract',\n",
    "    'http://dbpedia.org/ontology/city',\n",
    "    'http://dbpedia.org/ontology/country',\n",
    "    'http://dbpedia.org/ontology/type',\n",
    "    'http://dbpedia.org/ontology/thumbnail',\n",
    "    'http://dbpedia.org/ontology/wikiPageID',\n",
    "    'http://dbpedia.org/ontology/wikiPageRevisionID',\n",
    "    \n",
    "    # DBPedia property\n",
    "    'http://dbpedia.org/property/country',\n",
    "\n",
    "    # PURL\n",
    "    'http://purl.org/dc/terms/description',\n",
    "\n",
    "    # W3\n",
    "    'http://www.w3.org/2000/01/rdf-schema#comment',\n",
    "    'http://www.w3.org/2003/01/geo/wgs84_pos#lat',\n",
    "    'http://www.w3.org/2003/01/geo/wgs84_pos#long',\n",
    "    'http://www.w3.org/ns/prov#wasDerivedFrom',\n",
    "    \n",
    "    # FOAF\n",
    "    'http://xmlns.com/foaf/0.1/depiction',\n",
    "    'http://xmlns.com/foaf/0.1/homepage',\n",
    "    'http://xmlns.com/foaf/0.1/isPrimaryTopicOf',\n",
    "    'http://xmlns.com/foaf/0.1/name'\n",
    "]\n",
    "\n",
    "DBPEDIA_IGNORE_URLS = [\n",
    "    'http://dbpedia.org/resource/None'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Places Dictionaries\n",
    "\n",
    "Now I parse the JSON lines data to create dictionaries. The following rules apply when creating the dictionaries:\n",
    "\n",
    "1. Values in the DBpedia Ontology namespace takes precedence over those in the DBpedia Property namespace since, as described in section 4.3 of [DBpedia datasets](https://wiki.dbpedia.org/services-resources/datasets/dbpedia-datasets), it contains the cleanest data. Property namespace values are only used when there are no corresponding ontology namespace values.\n",
    "\n",
    "2. Some of the fields (e.g. the *abstract*) is multilingual. In such cases, only the English is extracted.\n",
    "\n",
    "3. The *country* field is slightly messy and some cleanup is done on it. However, some noise remains in the data. The sources of the noise are: \n",
    "    - Fields containing semantic URLs that are redirected.\n",
    "    - Semi-structured text containing valuable information that is not easy for a machine to understand.\n",
    "    \n",
    "I will handle some of these issues now and some of them later prior to and when generating features for machine learning.\n",
    "\n",
    "OK let's now generate the dictionaries and take a look at the first entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_place_data(json_line):\n",
    "    \"\"\"Create place data from json_line data.\n",
    "\n",
    "    Args:\n",
    "        json_line (dict): JSON dict.\n",
    "    Returns:\n",
    "        dict: Dictionary of place data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # flatten the json\n",
    "    flat_json = json_normalize(json_line)\n",
    "\n",
    "    # find the resource, source and fullName\n",
    "    resource_url = find_resource_url(flat_json)\n",
    "    source_url = get_source_url(resource_url)\n",
    "    full_name = get_filename_from_url(resource_url).replace('_', ' ')\n",
    "\n",
    "    # construct the dictionary\n",
    "    dict_ = {'resource': resource_url,\n",
    "             'source': source_url, 'fullName': full_name,\n",
    "             **json_keys_to_dict(resource_url, flat_json,\n",
    "                                 DBPEDIA_JSON_KEYS,\n",
    "                                 ignore_urls=DBPEDIA_IGNORE_URLS),\n",
    "             **json_categories_to_dict(flat_json)}\n",
    "\n",
    "    dict_ = _clean_country(dict_)\n",
    "\n",
    "    return dict_\n",
    "\n",
    "\n",
    "def _clean_country(dict_):\n",
    "    cleaned_dict = dict_.copy()\n",
    "\n",
    "    key = 'country'\n",
    "    key_present = cleaned_dict.get(key)\n",
    "    if key_present:\n",
    "        cleaned_dict[key] = (\n",
    "            key_present\n",
    "            .replace(', ', '|')\n",
    "            .replace(' and ', '|')\n",
    "            .replace('the ', '')\n",
    "        )\n",
    "        cleaned_dict[key] = '|'.join(sorted(cleaned_dict[key].split('|'),\n",
    "                                            key=locale.strxfrm))\n",
    "    return cleaned_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bar = progress_bar(len(json_lines), banner_text_begin='Creating: ',\n",
    "                   banner_text_end=' dicts')\n",
    "bar.start()\n",
    "\n",
    "data = []\n",
    "for i in range(len(json_lines)):\n",
    "    datum = create_place_data(json_lines[i])\n",
    "    data.append(datum)\n",
    "    bar.update(i)\n",
    "\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that many of the values in the dictionary contain lists of semantic URLs which are each meant to refer to a unique \"thing\" such as a person or a place. If the URL is redirected, it is important to know where it is redirected to so that identical \"things\" in fact resolve to the same URL. It is very important to do this so that the machine learning models have clean data in order to differentiate signal from noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputing Redirects in the Places Dictionaries\n",
    "\n",
    "In order to impute redirects in the dictionaries I need to perform the following steps:\n",
    "\n",
    "1. *Parse the dictionaries to obtain a list of all URLs*. These are either resource URLs or URLs constructed in an *ad hoc* manner from the free form text in the hope that they resolve to a genuine resource URL. The aim is to resolve as many items as possible in the fields to semantic URLs. I restrict URL selection and construction to the following impute keys: `categories`, `city`, `country` and `type` as these are fields of interest involving URLs that features may be extracted from.\n",
    "2. *Submit HTTP requests to fetch the URLs and determine their redirect URLs.* A cache is kept mapping the URLs to the redirect URLs and as a consequence a HTTP request is only made to fetch a URL if it is not found in the cache. This greatly helps with performance. Note that I reutilize the [dbpedia redirects cache](../data/raw/dbpedia-redirects.csv) created in notebook [2.0-process-physicists-raw-data.ipynb](2.0-process-physicists-raw-data.ipynb)\n",
    "3. *Replace the URLs in the dictionaries with the redirect URLs.* In fact I use just the filename since the paths are identical for every URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls_to_check = construct_resource_urls(data, PLACES_IMPUTE_KEYS)\n",
    "len(urls_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_cache_path = '../data/raw/dbpedia-redirects.csv'\n",
    "redirects = get_redirect_urls(\n",
    "    urls_to_check, url_cache_path=url_cache_path, max_workers=20,\n",
    "    timeout=30, progress_bar=progress_bar(len(urls_to_check)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(redirects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that many a few of the requested URLs were not found. This is to be expected as there is free form text in the fields that do not map to a semantic URL in DBpedia. However, my *ad hoc* approach of constructing URLs from free form texts is very successful in finding legitimate URLs in many instances.\n",
    "\n",
    "Now I sort and persist the URL cache to disk in case any new URLs are found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbpedia_redirects = pd.DataFrame(\n",
    "    sorted(list(zip(redirects.keys(), redirects.values())),\n",
    "           key=lambda x: locale.strxfrm(x[0])), columns=['url', 'redirect_url'])\n",
    "dbpedia_redirects.to_csv(url_cache_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I replace the URLs in the dictionaries with the redirect URLs making sure to just use the filename since the paths are identical for every URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_data = impute_redirect_filenames(data, PLACES_IMPUTE_KEYS, redirects)\n",
    "imputed_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Places Dataframe\n",
    "\n",
    "Now I use the dictionaries of imputed data to create a dataframe. Let's confirm that it contains the expected number of places and take a look at the first few records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places = pd.DataFrame(imputed_data)\n",
    "assert(len(places) == 1883)\n",
    "places.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute Missing Latitudes and Longitudes from Cities\n",
    "\n",
    "Let's see what percentage of places we can define as locations, namely, the number that have a value for both latitude and a longitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100 * round((\n",
    "    ~places.lat.isna() & ~places.long.isna()).sum() / len(places), 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite this being a healthy percentage, it is possible to do better as there is extra information in the `city` field that is not being utilized. The places below do not have a latitude and longitude defined but do have a city. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "places[places.lat.isna() & places.long.isna() & ~places.city.isna()][\n",
    "    ['fullName', 'city', 'lat', 'long']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for each place, I will attempt to automatically impute the missing latitude and longitude values based on their values for the associated city. For the few cities that are not present in the dataframe, I manually impute the values from the corresponding JSON file instead of making further HTTP requests to obtain the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_lat_long(places):\n",
    "    \"\"\"Impute missing latitude and longitudes from cities in the places dataframe.\n",
    "\n",
    "    Args:\n",
    "        places (pandas.DataFrame): Dataframe of places data.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Dataframe containing imputed data.\n",
    "\n",
    "        Identical to `places` except that it contains imputed values for missing\n",
    "        latitude and longitudes when the city is present.\n",
    "    \"\"\"\n",
    "    \n",
    "    imputed_places = places.copy()\n",
    "    imputed_places = places.apply(_imputes, axis=1, args=(places,))\n",
    "    \n",
    "    # manually impute values which are not in places dataframe\n",
    "    # instead of fetching the json via HTTP requests\n",
    "    imputed_places.loc[imputed_places.fullName == 'Banaras Hindu University',\n",
    "        # http://dbpedia.org/data/Varanasi.json\n",
    "        ['lat', 'long']] = [25.28000068664551, 82.95999908447266]\n",
    "    imputed_places.loc[imputed_places.fullName == 'City University of New York',\n",
    "        # http://dbpedia.org/data/New_York_City.json\n",
    "        ['lat', 'long']] = [40.71269989013672, -74.00589752197266]\n",
    "    imputed_places.loc[\n",
    "        imputed_places.fullName == 'Ghulam Ishaq Khan Institute of Engineering Sciences and Technology',\n",
    "        # http://dbpedia.org/data/Swabi_District.json\n",
    "        ['lat', 'long']] = [34.11666488647461, 72.46666717529297]\n",
    "    imputed_places.loc[\n",
    "        # http://dbpedia.org/data/Trieste.json\n",
    "        imputed_places.fullName == 'International Centre for Theoretical Physics',\n",
    "        ['lat', 'long']] = [45.63333511352539, 13.80000019073486]\n",
    "    imputed_places.loc[imputed_places.fullName == 'Kanagawa University',\n",
    "        # http://dbpedia.org/data/Kanagawa-ku,_Yokohama.json\n",
    "        ['lat', 'long']] = [35.47694396972656, 139.6294403076172]\n",
    "    imputed_places.loc[imputed_places.fullName == 'National and Kapodistrian University of Athens',\n",
    "        # https://www.latlong.net/place/athens-greece-22451.html\n",
    "        ['lat', 'long']] = [37.983810, 23.727539]\n",
    "    imputed_places.loc[imputed_places.fullName == 'University of Azad Jammu and Kashmir',\n",
    "        # http://dbpedia.org/data/Muzaffarabad.json\n",
    "        ['lat', 'long']] = [34.36100006103516, 73.46199798583984]\n",
    "    imputed_places.loc[imputed_places.fullName == 'University of Hawaii',\n",
    "        # http://dbpedia.org/data/Honolulu.json\n",
    "        ['lat', 'long']] = [21.29999923706055, -157.8166656494141]\n",
    "    imputed_places.loc[imputed_places.fullName == 'University of Lisbon',\n",
    "        # http://dbpedia.org/data/Lisbon.json\n",
    "        ['lat', 'long']] = [38.71381759643555, -9.139386177062988]\n",
    "    return imputed_places\n",
    "\n",
    "\n",
    "def _imputes(row, places):\n",
    "    update_row = row.copy()\n",
    "    \n",
    "    if np.isnan(row.lat) and np.isnan(row.long) and isinstance(row.city, str):\n",
    "        cities = row.city.split('|')\n",
    "        for city in cities:\n",
    "            place = places[places.fullName == city]\n",
    "            assert(len(place) <= 1)\n",
    "            if len(place) == 1:\n",
    "                update_row['lat'] = place['lat'].item()\n",
    "                update_row['long'] = place['long'].item()\n",
    "                return update_row  # take the first city that is found\n",
    "    return update_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_places  = impute_lat_long(places)\n",
    "imputed_places.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100 * round((\n",
    "    ~imputed_places.lat.isna() & ~imputed_places.long.isna()).sum()\n",
    "    / len(imputed_places), 2), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK I've managed to convert a further 2% of the places to locations. Not too shabby! And there are still further places with mostly well defined countries that can be utilized for feature construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_places[imputed_places.lat.isna() & imputed_places.long.isna()\n",
    "               & ~imputed_places.country.isna()][[\n",
    "    'fullName', 'country', 'lat', 'long']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now I have the dataframe, I'd like to persist it for later analysis. So I'll write out the contents to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_places.to_csv('../data/interim/places.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
