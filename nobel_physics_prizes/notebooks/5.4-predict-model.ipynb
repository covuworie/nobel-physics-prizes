{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "from src.features.features_utils import convert_categoricals_to_numerical\n",
    "from src.features.features_utils import convert_target_to_numerical\n",
    "from src.models.metrics_utils import confusion_matrix_to_dataframe\n",
    "from src.models.metrics_utils import print_matthews_corrcoef\n",
    "from src.visualization.visualization_utils import plot_logistic_regression_odds_ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in the Data\n",
    "\n",
    "First let's read in the classifier parameters and metadata that we saved so that we can reconstruct the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_params = pd.read_csv('../models/LR.csv', squeeze=True, index_col=0)\n",
    "classifier_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's read in the training, validation and test features and targets. We make sure to convert the categorical fields to a numerical form that is suitable for building machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv('../data/processed/train-features.csv')\n",
    "X_train = convert_categoricals_to_numerical(train_features)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = pd.read_csv('../data/processed/train-target.csv', index_col='full_name', squeeze=True)\n",
    "y_train = convert_target_to_numerical(train_target)\n",
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_features = pd.read_csv('../data/processed/validation-features.csv')\n",
    "X_validation = convert_categoricals_to_numerical(validation_features)\n",
    "X_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_target = pd.read_csv('../data/processed/validation-target.csv', index_col='full_name',\n",
    "                                squeeze=True)\n",
    "y_validation = convert_target_to_numerical(validation_target)\n",
    "y_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.read_csv('../data/processed/test-features.csv')\n",
    "X_test = convert_categoricals_to_numerical(test_features)\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = pd.read_csv('../data/processed/test-target.csv', index_col='full_name', squeeze=True)\n",
    "y_test = convert_target_to_numerical(test_target)\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining on the Training and Validation Data\n",
    "\n",
    "It makes sense to retrain the model on both the training and validation data so that we can obtain as good a predictive performance as possible. So let's combine the training and validation features and targets, reconstruct the classifier and retrain the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_validation = X_train.append(X_validation)\n",
    "assert(len(X_train_validation) == len(X_train) + len(X_validation))\n",
    "X_train_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_validation = y_train.append(y_validation)\n",
    "assert(len(y_train_validation) == len(y_train) + len(y_validation))\n",
    "y_train_validation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(**ast.literal_eval(classifier_params.params))\n",
    "classifier.fit(X_train_validation, y_train_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting on the Test Data\n",
    "\n",
    "Here comes the moment of truth! We will soon see just how good the model is by predicting on an unseen dataset, the test data. However, first it makes sense to look once again at the performance of our \"naive\" [baseline model](5.0-baseline-model.ipynb) on the test data. Recall that this is a model that predicts the physicist is a laureate whenever the number of workplaces is at least 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = X_train_validation.num_workplaces_at_least_2\n",
    "y_test_pred = X_test.num_workplaces_at_least_2\n",
    "mcc_train_validation = matthews_corrcoef(y_train_validation, y_train_pred)\n",
    "mcc_test = matthews_corrcoef(y_test, y_test_pred)\n",
    "name = 'Baseline Classifier'\n",
    "print_matthews_corrcoef(mcc_train_validation, name, data_label='train + validation')\n",
    "print_matthews_corrcoef(mcc_test, name, data_label='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, this classifier exhibits very poor performance on the test data. We see evidence of the covariate shift again here due to the difference in the test and train + validation MCCs. Either physicists started working in more workplaces in general, or the records of where physicists have worked are better in modern times. The confusion matrix and classification report indicate that the classifier is terrible in terms of both precision and recall when identifying laureates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confusion_matrix_to_dataframe(confusion_matrix(y_test, y_test_pred)))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK let's see how our logistic regression model does on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = (classifier.predict_proba(X_train_validation)[:, 1] > ast.literal_eval(\n",
    "    classifier_params.threshold)).astype('int64')\n",
    "y_test_pred = (classifier.predict_proba(X_test)[:, 1] > ast.literal_eval(\n",
    "    classifier_params.threshold)).astype('int64')\n",
    "mcc_train_validation = matthews_corrcoef(y_train_validation, y_train_pred)\n",
    "mcc_test = matthews_corrcoef(y_test, y_test_pred)\n",
    "print_matthews_corrcoef(mcc_train_validation, classifier_params.name, data_label='train + validation')\n",
    "print_matthews_corrcoef(mcc_test, classifier_params.name, data_label='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This classifier performs much better on the test data than the baseline classifier. Again you can see that by comparing it to the baseline classifier, I'm not actually saying whether it is a good or bad classifier. There is very little in the literature, even as a rule of thumb, saying what the expected MCC is for a \"good performing classifier\" as it is very dependent on the context and usage. As I said before, predicting Physics Nobel Laureates is a difficult task due to the many complex factors involved, so we certainly should not be expecting stellar performance from *any* classifier. This includes both machine classifiers, either machine-learning-based or rules-based, and human classifiers without inside knowledge. However, let me try and get off the fence just a little now. \n",
    "\n",
    "The MCC is a contingency matrix method of calculating the Pearson product-moment correlation coefficient and so it has the [same interpretation](https://stats.stackexchange.com/questions/118219/how-to-interpret-matthews-correlation-coefficient-mcc). If the values in the link are to be believed, then our classifier has a \"moderate positive relationship\" with the target. This [statistical guide](https://statistics.laerd.com/statistical-guides/pearson-correlation-coefficient-statistical-guide.php ) also seems to agree with this assessment. However, we can easily find examples that indicate there is a [low positive correlation](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3576830/) or a [weak uphill (positive) linear relationship](https://www.dummies.com/education/math/statistics/how-to-interpret-a-correlation-coefficient-r/) between the classifier's predictions and the target.\n",
    "\n",
    "So should we conclude that the classifier has a low or moderate performance? Asking this question is missing the purpose of this study. Instead we should ask, based on the classifier's performance, would we be willing to make recommendations to the Nobel Committee, about any biases that may be present when deciding Physics Laureates?  We can see from the confusion matrix and classification report that although this classifier has reasonable recall of laureates, it is contaminated by too many false postives. Or in other words, it is not precise enough. As a result, the answer to the question is very likely no."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(confusion_matrix_to_dataframe(confusion_matrix(y_test, y_test_pred)))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Important Features\n",
    "\n",
    "Out of interest, let's determine the features that are most important to the prediction by looking at the coefficients of the logistic regression model. Each coefficient represents the impact that the presence vs. absence of a predictor has on the log odds ratio of a physicist being a Nobel Laureate. The change in odds ratio for each predictor can can simply be computed by exponentiating its associated coefficient. The top fifteen most important features are plotted in the chart below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 15\n",
    "ax = plot_logistic_regression_odds_ratio(classifier.coef_, top_n=top_n, columns=X_train_validation.columns,\n",
    "    title='Top {} most important features in prediction of Physics Nobel Laureates'.format(top_n))\n",
    "ax.figure.set_size_inches(10, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By far the most important feature is being an experimental physicist. This matches with what we observed during the [exploratory data analysis](4.0-exploratory-data-analysis.ipynb). Next comes having at least one physics laureate doctoral student and then living for at least 65-79 years. Some of the other interesting top features we see are being a citizen of France or Switzerland, working at [Bell Labs](https://en.wikipedia.org/wiki/Bell_Labs#Discoveries_and_developments) or [The University of Cambridge](https://en.wikipedia.org/wiki/List_of_Nobel_laureates_by_university_affiliation#University_of_Cambridge_(2nd)), being an alumnus in Asia and having at least two alma mater."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction of 2018 Physics Nobel Laureates\n",
    "\n",
    "We will now use the logistic regression model to predict the 2018 Physics Nobel Laureates. A maximum of three physicists can be awarded the prize in any one year, however, to give ourselves more of a fighting chance let's try to predict the ten most likely winners. Let's start by forming the feature and target dataframes of physicists who are alive as the Nobel Prize cannot be awarded posthumously. Remember that this is the union of the validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation_test = X_validation.append(X_test)\n",
    "assert(len(X_validation_test) == len(X_validation) + len(X_test))\n",
    "X_validation_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_validation_test = y_validation.append(y_test)\n",
    "assert(len(y_validation_test) == len(y_validation) + len(y_test))\n",
    "y_validation_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that John Bardeen is the only [double laureate in Physics](https://www.nobelprize.org/prizes/facts/facts-on-the-nobel-prize-in-physics/), so although it is possible to receive the Nobel Prize in Physics multiple times, it is extremely rare. So let's drop previous Physics Laureates from the dataframe. This will make the list far more interesting as it will not be polluted by previous laureates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_eligible = X_validation_test.drop(y_validation_test[y_validation_test == 1].index)\n",
    "assert(len(X_eligible) == len(X_validation_test) - len(y_validation_test[y_validation_test == 1]))\n",
    "X_eligible.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our model, these are the ten most likely winners of 2018 Physics Nobel Prize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physicist_win_probabilites = pd.Series(\n",
    "    classifier.predict_proba(X_eligible)[:, 1], index=X_eligible.index).sort_values(ascending=False)\n",
    "physicist_win_probabilites[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list contains some great and very interesting physicists who have won numerous of the top prizes in physics. I'll leave you to check out their Wikipedia articles for some more information on them. However, a few are worth discussing now. My personal favorite and without doubt the most infamous is [Jocelyn Bell Burnell](https://en.wikipedia.org/wiki/Jocelyn_Bell_Burnell) who, as a postgraduate student, co-discovered the first radio pulsars in 1967. Her Wikipedia article says:\n",
    "\n",
    "\"The discovery was recognised by the award of the 1974 Nobel Prize in Physics, but despite the fact that she was the first to observe the pulsars, Bell was excluded from the recipients of the prize.\n",
    "\n",
    "The paper announcing the discovery of pulsars had five authors. Bell's thesis supervisor Antony Hewish was listed first, Bell second. Hewish was awarded the Nobel Prize, along with the astronomer Martin Ryle. Many prominent astronomers criticised Bell's omission, including Sir Fred Hoyle.\"\n",
    "\n",
    "You can read more about her in her Wikipedia article and further details about other [Nobel Physics Prize controversies](https://en.wikipedia.org/wiki/Nobel_Prize_controversies#Physics).\n",
    "\n",
    "[Vera Rubin](https://en.wikipedia.org/wiki/Vera_Rubin) was an American astronomer who's research provided evidence of the existence of [dark matter](https://en.wikipedia.org/wiki/Dark_matter) and according to her Wikipedia article, she \"never won the Nobel Prize, though physicists such as Lisa Randall and Emily Levesque have argued that this was an oversight.\" Unfortunately she died on 25 December 2016 and is no longer eligible for the award. Recall that the list contains some deceased physicists due to the lag in updates of dbPedia data from Wikipedia. Peter Mansfield, who is also on the list is deceased too. \n",
    "\n",
    "[Manfred Eigen](https://en.wikipedia.org/wiki/Manfred_Eigen) actually won the 1967 Nobel Prize in Chemistry for work on measuring fast chemical reactions. \n",
    "\n",
    "The actual winners of the [2018 Nobel Prize in Physics](https://www.nobelprize.org/prizes/physics/2018/summary/) were [Gérard Mourou](https://en.wikipedia.org/wiki/G%C3%A9rard_Mourou), [Arthur Ashkin](https://en.wikipedia.org/wiki/Arthur_Ashkin) and [Donna Strickland](https://en.wikipedia.org/wiki/Donna_Strickland). Our model actually had zero chance of predicting them as they were never in the original [list of physicists](../data/raw/physicists.txt) derived from Wikipedia! Obviously they have been added to Wikipedia since."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "('Gérard Mourou' in physicist_win_probabilites, \n",
    " 'Arthur Ashkin' in physicist_win_probabilites,\n",
    " 'Donna Strickland' in physicist_win_probabilites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So should we declare this part of the study as an epic failure as we were unable to identify the winners? No not quite. Closer inspection reveals many interesting characteristics of the three winners that related to the top features in our predictive model:\n",
    "\n",
    "- Gérard Mourou is an experimental physicist, a citizen of France, 74 years of age (years lived group 65-79), has at least one physics laureate doctoral student (Donna Strickland) and has 3 alma mater (which is at least 2).\n",
    "- Arthur Ashkin is an experimental physicist, worked at Bell Labs and has 2 alma mater.\n",
    "- Donna Strickland is an experimental physicist and has 2 alma mater.\n",
    "\n",
    "Maybe this is a pure coincidence, but more likely, there are patterns in the data that the model has found. Whether or not these characteristics can be attributed to biases in the Nobel Physics Prize nomination and selection process is another matter, as correlation does not necessarily imply causation.\n",
    "\n",
    "This section was a lot of fun and quite informative about the logistic regression classifier, however, it was not possible without cheating. Look closely to see if you can spot the cheating!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
