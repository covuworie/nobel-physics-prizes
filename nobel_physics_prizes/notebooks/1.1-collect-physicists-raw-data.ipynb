{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Physicists Raw Data\n",
    "\n",
    "Here I collect biographical data on the list of [physicists notable for their achievements](../data/raw/physicists.txt). Wikipedia contains this structured data in an *Infobox* on the top right side of the article for each physicist. However, similar data is available in a more machine readable, *JSON* format from [DBpedia](https://wiki.dbpedia.org/about). For an example, compare [Albert Einstein's Wikipedia infobox](https://en.wikipedia.org/wiki/Albert_Einstein) to [Albert Einstein's DBPedia JSON](http://dbpedia.org/data/Albert_Einstein.json). It is important to note that the data is similar but not identical.\n",
    "\n",
    "The shortcomings of Wikipedia infoboxes and the advantages of DBpedia datasets are explained in [DBpedia datasets (section 4.3)]. (https://wiki.dbpedia.org/services-resources/datasets/dbpedia-datasets). But basically the DBpedia data is much cleaner and better structured Wikipedia Infoboxes since it is based on hand-generated mappings of Wikipedia infoboxes/templates to a [DBpedia ontology](https://wiki.dbpedia.org/services-resources/ontology). As a result I choose DBpedia as the data source for this project.\n",
    "\n",
    "However, dBpedia does have the disadvantage that its content is roughly 6-18 months behind updates applied to Wikipedia content. This is because its data is generated from a [static dump of Wikipedia content](https://wiki.dbpedia.org/online-access/DBpediaLive) in a process that takes approximately 6 months. The fact that the data is not in sync with the latest Wikipedia content is not so significant for this project as the data is edited infrequently and when it is there are only minor changes.\n",
    "\n",
    "I will need to send HTTP requests to DBpedia to get the biographical JSON data on each of the physicists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the URLs\n",
    "\n",
    "To make the HTTP requests, I will need a list of URLs representing the resources (physicists). It's fairly easy to construct these URLs from the list of notable physicists. However, it's important to `quote` any physicist name in unicode since unicode characters are not allowed in URLs. OK let's create the list now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "from urllib import parse\n",
    "\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import progressbar as pb\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_urls(file='../data/raw/physicists.txt'):\n",
    "    \"\"\"Construct DBpedia data URLs from list in file.\n",
    "\n",
    "    Args:\n",
    "        file (str): File containing a list of url filepaths\n",
    "            with spaces replacing underscores.\n",
    "    Returns:\n",
    "        list(str): List of URLs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file, encoding='utf-8') as file:\n",
    "        names = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    urls = []\n",
    "    for name in names:\n",
    "        url = name.replace(' ', '_')\n",
    "        # some physicist names contain unicode characters\n",
    "        # which have to be quoted when in a url\n",
    "        if not all(ord(char) < 128 for char in name):\n",
    "            url = parse.quote(url)\n",
    "        url = 'http://dbpedia.org/data/' + url + '.json'\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = construct_urls()\n",
    "assert(len(urls) == 1060)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the Data\n",
    "\n",
    "Now I have the list of URLs, it's time to make the HTTP requests to acquire the data. This will take a bit of time as there are 1059 urls and I want to crawl responsibly so that I don't bombard the site. Time to get a coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_json_data(urls, progress_bar=None):\n",
    "    \"\"\"Fetch json data from DBpedia.\n",
    "\n",
    "    Args:\n",
    "        urls (list(str)): List of URLs to fetch data from.\n",
    "        progress_bar (progressbar.ProgressBar): Progress bar.\n",
    "    Returns:\n",
    "        list(json): List of JSON objects.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    json_data = []\n",
    "    for i in range(len(urls)):\n",
    "        time.sleep(1)  # delay to crawl responsibly\n",
    "        response = requests.get(urls[i])\n",
    "        response.raise_for_status()\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            json_data.append(response.json())\n",
    "        if progress_bar:\n",
    "            progress_bar.update(i)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets = [\n",
    "    'Fetching: ', pb.Counter(),\n",
    "    ' / ' + str(len(urls)) + ' urls',\n",
    "    ' ', pb.Bar(marker='â–ˆ'),\n",
    "    ' ', pb.Percentage(),\n",
    "    ' ', pb.Timer(),\n",
    "    ' ', pb.ETA()\n",
    "]\n",
    "bar = pb.ProgressBar(max_value=len(urls), widgets=widgets,\n",
    "                     redirect_stdout=True).start()\n",
    "\n",
    "json_data = fetch_json_data(urls, bar)\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that all the data was fetched and take a look at the first JSON response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(json_data) == 1060)\n",
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that every request successfully received a response. However, I see that some responses came back empty from the server. Basically, although there are Wikipedia pages for these physicists they do not have a corresponding page in DBpedia (or the page in DBpedia has a different name). Not to worry, there are only 10 and they are not so famous, so I'll just exlude these physicists from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_urls = [urls[i] for (i, url) in enumerate(json_data)\n",
    "                if not url]\n",
    "assert(len(dropped_urls) == 10)\n",
    "print(len(dropped_urls))\n",
    "dropped_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now I have the list of JSON responses, I'd like to persist them for later analysis. I'll use [Json Lines](http://jsonlines.org/) as it seems like a convenient format for storing structured data that may be processed one record at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file = '../data/raw/notable_physicists.jsonl'\n",
    "with jsonlines.open(jsonl_file, 'w') as writer:\n",
    "    writer.write_all(filter(None, json_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure the file contains the expected number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lines = []\n",
    "with jsonlines.open(jsonl_file, 'r') as reader:\n",
    "    for json_line in reader:\n",
    "        json_lines.append(json_line)\n",
    "assert(len(json_lines) == 1050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's compress the file to reduce its footprint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(jsonl_file, 'rb') as src, gzip.open(jsonl_file + '.gz', 'wb') as dest:\n",
    "    shutil.copyfileobj(src, dest)\n",
    "os.remove(jsonl_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "A few clean up steps are needed:\n",
    "\n",
    "- Convert the notebook to a HTML file with all the output.\n",
    "- Convert the notebook to another notebook with the output removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --output-dir html_output --to html 1.1-collect-physicists-raw-data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to notebook 1.1-collect-physicists-raw-data.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
