{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Physicists Raw Data\n",
    "\n",
    "Here I collect biographical data on the list of [physicists notable for their achievements](../data/raw/physicists.txt). Wikipedia contains this data in an *Infobox* on the top right side of the page for each physicist. However, similar data is available in a more structured, machine readable, *JSON* format from DBpedia. For an example, compare [Albert Einstein's Wikipedia infobox](https://en.wikipedia.org/wiki/Albert_Einstein) to [Albert Einstein's DBPedia JSON](http://dbpedia.org/data/Albert_Einstein.json). It is important to note that the data is similar but not identical. The DBpedia contains many extra fields which are not present in the Wikipedia infobox. I choose DBpedia as the data source since it has the folowing advantages over Wikipedia:\n",
    "\n",
    "- The data is structured in a machine readable JSON format\n",
    "- The data is richer \n",
    "\n",
    "I will need to send HTTP requests to get the biographical data on each of the physicists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing the URLs\n",
    "\n",
    "To make the HTTP requests, I will need a list of URLs representing the resources (physicists). It's fairly easy to construct these URLs from the list of notable physicists. However, it's important to `quote` any physicist name in unicode since unicode characters are not allowed in URLs. OK let's create the list now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import json\n",
    "import time\n",
    "from urllib import parse\n",
    "\n",
    "import pandas as pd\n",
    "import progressbar as pb\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_urls(file='../data/raw/physicists.txt'):\n",
    "    \"\"\"Construct DBpedia data URLs from list in file.\n",
    "\n",
    "    Args:\n",
    "        file (str): File containing a list of url filepaths\n",
    "            with spaces replacing underscores.\n",
    "    Returns:\n",
    "        list(str): List of URLs.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    with open(file, encoding='utf-8') as file:\n",
    "        names = [line.rstrip('\\n') for line in file]\n",
    "\n",
    "    DBPEDIA_URL = 'http://dbpedia.org/data/'\n",
    "    JSON_EXT = '.json'\n",
    "    urls = []\n",
    "    for name in names:\n",
    "        url = name.replace(' ', '_')\n",
    "        # some physicist names contain unicode characters\n",
    "        # which have to be quoted when in a url\n",
    "        if not all(ord(char) < 128 for char in name):\n",
    "            url = parse.quote(url)\n",
    "        url = DBPEDIA_URL + url + JSON_EXT\n",
    "        urls.append(url)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = construct_urls()\n",
    "assert(len(urls) == 1090)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching the Data\n",
    "\n",
    "Now I have the list of URLs, it's time to make the HTTP requests to acquire the data. This will take a bit of time as there are 1090 urls and I want to crawl responsibly so that I don't bombard the site. Time to get a coffee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_json_data(urls, progress_bar=None):\n",
    "    \"\"\"Fetch json data from DBpedia.\n",
    "\n",
    "    Args:\n",
    "        urls (list(str)): List of URLs to fetch data from.\n",
    "        progress_bar (progressbar.ProgressBar): Progress bar.\n",
    "    Returns:\n",
    "        list(json): List of JSON objects.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    json_data = []\n",
    "    for i in range(len(urls)):\n",
    "        time.sleep(1)  # delay to crawl responsibly\n",
    "        response = requests.get(urls[i])\n",
    "        response.raise_for_status()\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            json_data.append(response.json())\n",
    "        if progress_bar:\n",
    "            progress_bar.update(i)\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "widgets = [\n",
    "    'Fetching: ', pb.Counter(),\n",
    "    ' / ' + str(len(urls)) + ' urls',\n",
    "    ' ', pb.Bar(marker='â–ˆ'),\n",
    "    ' ', pb.Percentage(),\n",
    "    ' ', pb.Timer(),\n",
    "    ' ', pb.ETA()\n",
    "]\n",
    "bar = pb.ProgressBar(max_value=len(urls), widgets=widgets,\n",
    "                     redirect_stdout=True).start()\n",
    "\n",
    "json_data = fetch_json_data(urls, bar)\n",
    "bar.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that all the data was fetched and take a look at the first JSON response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(json_data) == 1090)\n",
    "json_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that every request successfully received a response. However, I see that some responses came back empty from the server. Basically, although there are Wikipedia pages for these physicists they do not have a corresponding page in DBpedia. Not to worry, there are only 14 and they are not so famous, so I'll just exlude these physicists from the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_urls = [urls[i] for (i, url) in enumerate(json_data)\n",
    "                if not url]\n",
    "assert(len(dropped_urls) == 14)\n",
    "print(len(dropped_urls))\n",
    "dropped_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Persisting the Data\n",
    "\n",
    "Now I have the list of JSON responses, I'd like to persist them for later analysis. [Json Lines](http://jsonlines.org/) seems like a convenient format for storing structured data that may be processed one record at a time. So I'll use that format and also compress the file to save some space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_json_lines(lines, file, mode='wt', encoding='utf-8',\n",
    "                     compress=False):\n",
    "    \"\"\"Write a json lines file to disk.\n",
    "\n",
    "    Args:\n",
    "        lines(list(json)): List of JSON objects.\n",
    "        file (str): A text or byte string giving the name (and the path\n",
    "            if the file isn't in the current working directory) of the\n",
    "            file to be opened or an integer file descriptor of the file\n",
    "            to be wrapped. See `open()` and `gzip.open() methods in the\n",
    "            standard library for more details.\n",
    "        list_to_write (list): The list of items.\n",
    "        mode (str): Specifies the mode in which the file is opened. See\n",
    "            `open()` and `gzip.open() methods in the standard library for\n",
    "            more details.\n",
    "        encoding (str): The name of the encoding used to decode or\n",
    "            encode the file. This should only be used in text mode. See\n",
    "            the codecs module for the list of supported encodings.\n",
    "        compress (bool): True to compress the file using gzip, otherwise\n",
    "            write the file as is.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if compress:\n",
    "        file = gzip.open(filename=file, mode=mode, encoding=encoding)\n",
    "    else:\n",
    "        file = open(file=file, mode=mode, encoding=encoding)\n",
    "    for datum in filter(None, lines):\n",
    "        json.dump(datum, file, ensure_ascii=False)\n",
    "        file.write('\\n')\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json_lines(json_data, '../data/raw/notable_physicists.jsonl.gz',\n",
    "                 compress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure the file contains the expected number of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonlines = []\n",
    "with gzip.open(filename='../data/raw/notable_physicists.jsonl.gz',\n",
    "               mode='rt', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        jsonlines.append(json.loads(line))\n",
    "assert(len(jsonlines) == 1076)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Up\n",
    "\n",
    "A few clean up steps are needed:\n",
    "\n",
    "- Convert the notebook to a HTML file with all the output.\n",
    "- Convert the notebook to another notebook with the output removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=False --output-dir html_output --to html 1.1-collect-physicists-raw-data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --ClearOutputPreprocessor.enabled=True --to notebook 1.1-collect-physicists-raw-data.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
